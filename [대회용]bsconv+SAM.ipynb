{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[대회용]_bsconv_+_SAM.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "503e012abca548cf8d08395594d86e19": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_b50fcebd177b463abe158e1e16b98c1b",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_a9f3b2f92be0487ca1846ea054da5152",
              "IPY_MODEL_de0ae9ad2a6549c69f87450dcebaf978"
            ]
          }
        },
        "b50fcebd177b463abe158e1e16b98c1b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a9f3b2f92be0487ca1846ea054da5152": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_4167c443210d4035a22c2dd063b339f4",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_39a389e7a0a8467b85f7a2f7cfc4dde4"
          }
        },
        "de0ae9ad2a6549c69f87450dcebaf978": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_b867c8190b7e441c9524a1789917d6e7",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 169009152/? [00:20&lt;00:00, 94184477.49it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5eaee457dc894f90b4298e0d348dcbd9"
          }
        },
        "4167c443210d4035a22c2dd063b339f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "39a389e7a0a8467b85f7a2f7cfc4dde4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b867c8190b7e441c9524a1789917d6e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5eaee457dc894f90b4298e0d348dcbd9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "237d4385557648849031c6800ddbad2a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_5d4d56d8b8ed42f89ed1048288de67e9",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_a3f5f5ae1da6468d8f1a9d6ff9fac92d",
              "IPY_MODEL_d8c78c2e900b4b8f82dbb34e89119e16"
            ]
          }
        },
        "5d4d56d8b8ed42f89ed1048288de67e9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a3f5f5ae1da6468d8f1a9d6ff9fac92d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_0c8c892120874f279b009768e93889d5",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_24309690548c400581142321a27e62bb"
          }
        },
        "d8c78c2e900b4b8f82dbb34e89119e16": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_32a581040ed54c2db79fbdda935540c7",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 169009152/? [00:04&lt;00:00, 36858404.46it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2b6a6acfbad243a8acbffac97b7d3f1d"
          }
        },
        "0c8c892120874f279b009768e93889d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "24309690548c400581142321a27e62bb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "32a581040ed54c2db79fbdda935540c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2b6a6acfbad243a8acbffac97b7d3f1d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "HC1Cll7oVOwj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6btQfRz2Jj7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4006e057-b729-4e63-fca9-50f635a6bf23"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wed Feb 24 07:53:25 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.39       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   65C    P8    10W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ldOZQU1w3kiG"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.utils import save_image\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import os\n",
        "import glob\n",
        "import PIL\n",
        "import math\n",
        "from PIL import Image\n",
        "from torch.utils import data as D\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import random\n",
        "import torchsummary\n",
        "\n",
        "import argparse\n",
        "import os\n",
        "import shutil\n",
        "import time\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim\n",
        "import torch.utils.data\n",
        "import torch.utils.data.distributed\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.models as models\n",
        "import numpy as np\n",
        "\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "model_names = sorted(name for name in models.__dict__\n",
        "                     if name.islower() and not name.startswith(\"__\")\n",
        "                     and callable(models.__dict__[name]))\n",
        "import argparse\n",
        "import os\n",
        "import shutil\n",
        "import time\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim\n",
        "import torch.utils.data\n",
        "import torch.utils.data.distributed\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.models as models\n",
        "import numpy as np\n",
        "\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "model_names = sorted(name for name in models.__dict__\n",
        "                     if name.islower() and not name.startswith(\"__\")\n",
        "                     and callable(models.__dict__[name]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R7lfw9Mw3l8n",
        "outputId": "bdce5d67-dafa-4e9f-d830-a541cfb9c89a"
      },
      "source": [
        "!pip install bsconv\n",
        "import bsconv.pytorch\n",
        "replacer = bsconv.pytorch.BSConvU_Replacer()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting bsconv\n",
            "  Downloading https://files.pythonhosted.org/packages/ab/2b/c1e85d364217884e13809649150e3e790f0a79fdb576bdbbae6745c34e58/bsconv-0.4.0-py3-none-any.whl\n",
            "Installing collected packages: bsconv\n",
            "Successfully installed bsconv-0.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "INXafvIH3n9c"
      },
      "source": [
        "import easydict \n",
        "args = easydict.EasyDict({'net_type':'resnet', 'workers':4,'epochs':290,'batch_size':64, 'lr':0.1,\n",
        "                                          'momentum':0.9, 'weight_decay':1e-4, 'print_freq' : 1, 'depth':20, \n",
        "                                          'bottleneck':True, 'dataset':'cifar100', 'verbose':True, 'alpha':300,\n",
        "                                          'expname': 'TEST', 'cutmix_prob':0.5, 'beta':1.0})\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDD71D_q3pby"
      },
      "source": [
        "import torch\n",
        "import random\n",
        "\n",
        "__all__ = [\"Compose\", \"Lighting\", \"ColorJitter\"]\n",
        "\n",
        "\n",
        "class Compose(object):\n",
        "    \"\"\"Composes several transforms together.\n",
        "    Args:\n",
        "        transforms (list of ``Transform`` objects): list of transforms to compose.\n",
        "    Example:\n",
        "        >>> transforms.Compose([\n",
        "        >>>     transforms.CenterCrop(10),\n",
        "        >>>     transforms.ToTensor(),\n",
        "        >>> ])\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, transforms):\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __call__(self, img):\n",
        "        for t in self.transforms:\n",
        "            img = t(img)\n",
        "        return img\n",
        "\n",
        "    def __repr__(self):\n",
        "        format_string = self.__class__.__name__ + '('\n",
        "        for t in self.transforms:\n",
        "            format_string += '\\n'\n",
        "            format_string += '    {0}'.format(t)\n",
        "        format_string += '\\n)'\n",
        "        return format_string\n",
        "\n",
        "\n",
        "class Lighting(object):\n",
        "    \"\"\"Lighting noise(AlexNet - style PCA - based noise)\"\"\"\n",
        "\n",
        "    def __init__(self, alphastd, eigval, eigvec):\n",
        "        self.alphastd = alphastd\n",
        "        self.eigval = torch.Tensor(eigval)\n",
        "        self.eigvec = torch.Tensor(eigvec)\n",
        "\n",
        "    def __call__(self, img):\n",
        "        if self.alphastd == 0:\n",
        "            return img\n",
        "\n",
        "        alpha = img.new().resize_(3).normal_(0, self.alphastd)\n",
        "        rgb = self.eigvec.type_as(img).clone() \\\n",
        "            .mul(alpha.view(1, 3).expand(3, 3)) \\\n",
        "            .mul(self.eigval.view(1, 3).expand(3, 3)) \\\n",
        "            .sum(1).squeeze()\n",
        "\n",
        "        return img.add(rgb.view(3, 1, 1).expand_as(img))\n",
        "\n",
        "\n",
        "class Grayscale(object):\n",
        "\n",
        "    def __call__(self, img):\n",
        "        gs = img.clone()\n",
        "        gs[0].mul_(0.299).add_(0.587, gs[1]).add_(0.114, gs[2])\n",
        "        gs[1].copy_(gs[0])\n",
        "        gs[2].copy_(gs[0])\n",
        "        return gs\n",
        "\n",
        "\n",
        "class Saturation(object):\n",
        "\n",
        "    def __init__(self, var):\n",
        "        self.var = var\n",
        "\n",
        "    def __call__(self, img):\n",
        "        gs = Grayscale()(img)\n",
        "        alpha = random.uniform(-self.var, self.var)\n",
        "        return img.lerp(gs, alpha)\n",
        "\n",
        "\n",
        "class Brightness(object):\n",
        "\n",
        "    def __init__(self, var):\n",
        "        self.var = var\n",
        "\n",
        "    def __call__(self, img):\n",
        "        gs = img.new().resize_as_(img).zero_()\n",
        "        alpha = random.uniform(-self.var, self.var)\n",
        "        return img.lerp(gs, alpha)\n",
        "\n",
        "\n",
        "class Contrast(object):\n",
        "\n",
        "    def __init__(self, var):\n",
        "        self.var = var\n",
        "\n",
        "    def __call__(self, img):\n",
        "        gs = Grayscale()(img)\n",
        "        gs.fill_(gs.mean())\n",
        "        alpha = random.uniform(-self.var, self.var)\n",
        "        return img.lerp(gs, alpha)\n",
        "\n",
        "\n",
        "class ColorJitter(object):\n",
        "\n",
        "    def __init__(self, brightness=0.4, contrast=0.4, saturation=0.4):\n",
        "        self.brightness = brightness\n",
        "        self.contrast = contrast\n",
        "        self.saturation = saturation\n",
        "\n",
        "    def __call__(self, img):\n",
        "        self.transforms = []\n",
        "        if self.brightness != 0:\n",
        "            self.transforms.append(Brightness(self.brightness))\n",
        "        if self.contrast != 0:\n",
        "            self.transforms.append(Contrast(self.contrast))\n",
        "        if self.saturation != 0:\n",
        "            self.transforms.append(Saturation(self.saturation))\n",
        "\n",
        "        random.shuffle(self.transforms)\n",
        "        transform = Compose(self.transforms)\n",
        "        # print(transform)\n",
        "        return transform(img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rAV-V4HF3qSh"
      },
      "source": [
        "from typing import Iterable\n",
        "\n",
        "import torch\n",
        "from torch.optim._multi_tensor import SGD\n",
        "\n",
        "__all__ = [\"SAMSGD\"]\n",
        "\n",
        "\n",
        "class SAMSGD(SGD):\n",
        "    \"\"\" SGD wrapped with Sharp-Aware Minimization\n",
        "    Args:\n",
        "        params: tensors to be optimized\n",
        "        lr: learning rate\n",
        "        momentum: momentum factor\n",
        "        dampening: damping factor\n",
        "        weight_decay: weight decay factor\n",
        "        nesterov: enables Nesterov momentum\n",
        "        rho: neighborhood size\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 params: Iterable[torch.Tensor],\n",
        "                 lr: float,\n",
        "                 momentum: float = 0,\n",
        "                 dampening: float = 0,\n",
        "                 weight_decay: float = 0,\n",
        "                 nesterov: bool = False,\n",
        "                 rho: float = 0.05,\n",
        "                 ):\n",
        "        if rho <= 0:\n",
        "            raise ValueError(f\"Invalid neighborhood size: {rho}\")\n",
        "        super().__init__(params, lr, momentum, dampening, weight_decay, nesterov)\n",
        "        # todo: generalize this\n",
        "        if len(self.param_groups) > 1:\n",
        "            raise ValueError(\"Not supported\")\n",
        "        self.param_groups[0][\"rho\"] = rho\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def step(self,\n",
        "             closure\n",
        "             ) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            closure: A closure that reevaluates the model and returns the loss.\n",
        "        Returns: the loss value evaluated on the original point\n",
        "        \"\"\"\n",
        "        closure = torch.enable_grad()(closure)\n",
        "        loss = closure().detach()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            grads = []\n",
        "            params_with_grads = []\n",
        "\n",
        "            rho = group['rho']\n",
        "            # update internal_optim's learning rate\n",
        "\n",
        "            for p in group['params']:\n",
        "                if p.grad is not None:\n",
        "                    # without clone().detach(), p.grad will be zeroed by closure()\n",
        "                    grads.append(p.grad.clone().detach())\n",
        "                    params_with_grads.append(p)\n",
        "            device = grads[0].device\n",
        "\n",
        "            # compute \\hat{\\epsilon}=\\rho/\\norm{g}\\|g\\|\n",
        "            grad_norm = torch.stack([g.detach().norm(2).to(device) for g in grads]).norm(2)\n",
        "            epsilon = grads  # alias for readability\n",
        "            torch._foreach_mul_(epsilon, rho / grad_norm)\n",
        "\n",
        "            # virtual step toward \\epsilon\n",
        "            torch._foreach_add_(params_with_grads, epsilon)\n",
        "            # compute g=\\nabla_w L_B(w)|_{w+\\hat{\\epsilon}}\n",
        "            closure()\n",
        "            # virtual step back to the original point\n",
        "            torch._foreach_sub_(params_with_grads, epsilon)\n",
        "\n",
        "        super().step()\n",
        "        return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MNjDXBGdQic5"
      },
      "source": [
        "torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr = 0.1, total_steps=None, epochs=300, steps_per_epoch=None, pct_start=0.3, anneal_strategy='cos', \n",
        "                                    cycle_momentum=True, base_momentum=0.85, max_momentum=0.95, div_factor=25.0, final_div_factor=10000.0, last_epoch=-1, verbose=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2F9KwR-B6IF"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m3F2q9wXZTqY"
      },
      "source": [
        "### 1-cycle policy 적용해보기\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "503e012abca548cf8d08395594d86e19",
            "b50fcebd177b463abe158e1e16b98c1b",
            "a9f3b2f92be0487ca1846ea054da5152",
            "de0ae9ad2a6549c69f87450dcebaf978",
            "4167c443210d4035a22c2dd063b339f4",
            "39a389e7a0a8467b85f7a2f7cfc4dde4",
            "b867c8190b7e441c9524a1789917d6e7",
            "5eaee457dc894f90b4298e0d348dcbd9"
          ]
        },
        "id": "Llzvp9hnZXE7",
        "outputId": "a83097c0-c1f6-4e5a-93bc-448aab1ad67c"
      },
      "source": [
        "#parser.set_defaults(bottleneck=True)\n",
        "#parser.set_defaults(verbose=True)\n",
        "\n",
        "best_err1 = 100\n",
        "best_err5 = 100\n",
        "\n",
        "\n",
        "def main():\n",
        "    global args, best_err1, best_err5\n",
        "    \n",
        "\n",
        "    if args.dataset.startswith('cifar'):\n",
        "        normalize = transforms.Normalize(mean=[x / 255.0 for x in [125.3, 123.0, 113.9]],\n",
        "                                         std=[x / 255.0 for x in [63.0, 62.1, 66.7]])\n",
        "\n",
        "        transform_train = transforms.Compose([\n",
        "            transforms.RandomCrop(32, padding=4),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.ToTensor(),\n",
        "            normalize,\n",
        "        ])\n",
        "\n",
        "        transform_test = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            normalize\n",
        "        ])\n",
        "\n",
        "        if args.dataset == 'cifar100':\n",
        "            train_loader = torch.utils.data.DataLoader(\n",
        "                datasets.CIFAR100('../data', train=True, download=True, transform=transform_train),\n",
        "                batch_size=args.batch_size, shuffle=True, num_workers=args.workers, pin_memory=True)\n",
        "            val_loader = torch.utils.data.DataLoader(\n",
        "                datasets.CIFAR100('../data', train=False, transform=transform_test),\n",
        "                batch_size=args.batch_size, shuffle=True, num_workers=args.workers, pin_memory=True)\n",
        "            numberofclass = 100\n",
        "        elif args.dataset == 'cifar10':\n",
        "            train_loader = torch.utils.data.DataLoader(\n",
        "                datasets.CIFAR10('../data', train=True, download=True, transform=transform_train),\n",
        "                batch_size=args.batch_size, shuffle=True, num_workers=args.workers, pin_memory=True)\n",
        "            val_loader = torch.utils.data.DataLoader(\n",
        "                datasets.CIFAR10('../data', train=False, transform=transform_test),\n",
        "                batch_size=args.batch_size, shuffle=True, num_workers=args.workers, pin_memory=True)\n",
        "            numberofclass = 10\n",
        "        else:\n",
        "            raise Exception('unknown dataset: {}'.format(args.dataset))\n",
        "\n",
        "    else:\n",
        "        raise Exception('unknown dataset: {}'.format(args.dataset))\n",
        "\n",
        "    print(\"=> creating model '{}'\".format(args.net_type))\n",
        "    if args.net_type == 'resnet':\n",
        "        model = bsconv.pytorch.get_model('cifar_wrn28_4_bsconvs_p1d8', num_classes=100)\n",
        "    else:\n",
        "        raise Exception('unknown network architecture: {}'.format(args.net_type))\n",
        "\n",
        "    model = torch.nn.DataParallel(model).cuda()\n",
        "\n",
        "    print(model)\n",
        "    print('the number of model parameters: {}'.format(sum([p.data.nelement() for p in model.parameters()])))\n",
        "\n",
        "    # define loss function (criterion) and optimizer\n",
        "    criterion = nn.CrossEntropyLoss().cuda()\n",
        "\n",
        "    optimizer = SAMSGD(model.parameters(), lr=args.lr, momentum=args.momentum,\n",
        "                       weight_decay=args.weight_decay, nesterov=True, rho=0.05)\n",
        "\n",
        "    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.1, steps_per_epoch=10,\n",
        "                                                    base_momentum=0.85, max_momentum=0.95, epochs=290)\n",
        "\n",
        "#          optimizer = torch.optim.SGD(model.parameters(), args.lr,\n",
        "#                                      momentum=args.momentum,\n",
        "#                                      weight_decay=args.weight_decay, nesterov=True)\n",
        "\n",
        "    cudnn.benchmark = True\n",
        "    if os.path.exists(f'/content/drive/MyDrive/ajoudeep/contest/bsconv_sam/checkpoint.pth.tar'):\n",
        "        checkpoint = torch.load(f'/content/drive/MyDrive/ajoudeep/contest/bsconv_sam/checkpoint.pth.tar')\n",
        "        model.load_state_dict(checkpoint['state_dict'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "        start_epoch = checkpoint['epoch']\n",
        "        best_err1 = checkpoint['best_err1']\n",
        "        best_err5 = checkpoint['best_err5']\n",
        "        \n",
        "    else:\n",
        "        start_epoch = 0\n",
        "\n",
        "\n",
        "    for epoch in range(start_epoch, args.epochs):\n",
        "\n",
        "       # adjust_learning_rate(optimizer, epoch)\n",
        "        scheduler.step()\n",
        "        # train for one epoch\n",
        "        train_loss = train(train_loader, model, criterion, optimizer, epoch)\n",
        "\n",
        "        # evaluate on validation set\n",
        "        err1, err5, val_loss = validate(val_loader, model, criterion, epoch)\n",
        "\n",
        "        # remember best prec@1 and save checkpoint\n",
        "        is_best = err1 <= best_err1\n",
        "        best_err1 = min(err1, best_err1)\n",
        "        if is_best:\n",
        "            best_err5 = err5\n",
        "\n",
        "        print('Current best accuracy (top-1 and 5 error):', best_err1, best_err5)\n",
        "        save_checkpoint({\n",
        "            'epoch': epoch,\n",
        "            'arch': args.net_type,\n",
        "            'state_dict': model.state_dict(),\n",
        "            'best_err1': best_err1,\n",
        "            'best_err5': best_err5,\n",
        "            'optimizer': optimizer.state_dict(),\n",
        "        }, is_best)\n",
        "\n",
        "    print('Best accuracy (top-1 and 5 error):', best_err1, best_err5)\n",
        "\n",
        "\n",
        "def train(train_loader, model, criterion, optimizer, epoch):\n",
        "    batch_time = AverageMeter()\n",
        "    data_time = AverageMeter()\n",
        "    losses = AverageMeter()\n",
        "    top1 = AverageMeter()\n",
        "    top5 = AverageMeter()\n",
        "\n",
        "    # switch to train mode\n",
        "    model.train()\n",
        "\n",
        "    end = time.time()\n",
        "    current_LR = get_learning_rate(optimizer)[0]\n",
        "    for i, (input, target) in enumerate(train_loader):\n",
        "        # measure data loading time\n",
        "        data_time.update(time.time() - end)\n",
        "\n",
        "        input = input.cuda()\n",
        "        target = target.cuda()\n",
        "\n",
        "        r = np.random.rand(1)\n",
        "        \n",
        "        def closure():\n",
        "            optimizer.zero_grad()\n",
        "            if args.beta > 0 and r < args.cutmix_prob:\n",
        "            # generate mixed sample\n",
        "                lam = np.random.beta(args.beta, args.beta)\n",
        "                rand_index = torch.randperm(input.size()[0]).cuda()\n",
        "                target_a = target\n",
        "                target_b = target[rand_index]\n",
        "                bbx1, bby1, bbx2, bby2 = rand_bbox(input.size(), lam)\n",
        "                input[:, :, bbx1:bbx2, bby1:bby2] = input[rand_index, :, bbx1:bbx2, bby1:bby2]\n",
        "            # adjust lambda to exactly match pixel ratio\n",
        "                lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (input.size()[-1] * input.size()[-2]))\n",
        "            # compute output\n",
        "                output = model(input)\n",
        "                loss = criterion(output, target_a) * lam + criterion(output, target_b) * (1. - lam)\n",
        "            else:\n",
        "            # compute output\n",
        "                output = model(input)\n",
        "                loss = criterion(output, target)\n",
        "\n",
        "                    # measure accuracy and record loss\n",
        "            err1, err5 = accuracy(output.data, target, topk=(1, 5))\n",
        "\n",
        "            losses.update(loss.item(), input.size(0))\n",
        "            top1.update(err1.item(), input.size(0))\n",
        "            top5.update(err5.item(), input.size(0))\n",
        "            \n",
        "            loss.backward()\n",
        "            return loss\n",
        "\n",
        "\n",
        "\n",
        "        # compute gradient and do SGD step\n",
        "        #optimizer.zero_grad()\n",
        "        #loss.backward()\n",
        "        #optimizer.step()\n",
        "\n",
        "        loss = optimizer.step(closure)\n",
        "\n",
        "\n",
        "        # measure elapsed time\n",
        "        batch_time.update(time.time() - end)\n",
        "        end = time.time()\n",
        "\n",
        "        show_period = 100\n",
        "\n",
        "        if i % show_period == show_period-1:        \n",
        "#        if i % args.print_freq == 0 and args.verbose == True:\n",
        "            print('Epoch: [{0}/{1}][{2}/{3}]\\t'\n",
        "                  'LR: {LR:.6f}\\t'\n",
        "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
        "                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
        "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
        "                  'Top 1-err {top1.val:.4f} ({top1.avg:.4f})\\t'\n",
        "                  'Top 5-err {top5.val:.4f} ({top5.avg:.4f})'.format(\n",
        "                epoch, args.epochs, i, len(train_loader), LR=current_LR, batch_time=batch_time,\n",
        "                data_time=data_time, loss=losses, top1=top1, top5=top5))\n",
        "\n",
        "    print('* Epoch: [{0}/{1}]\\t Top 1-err {top1.avg:.3f}  Top 5-err {top5.avg:.3f}\\t Train Loss {loss.avg:.3f}'.format(\n",
        "        epoch, args.epochs, top1=top1, top5=top5, loss=losses))\n",
        "\n",
        "    return losses.avg\n",
        "\n",
        "\n",
        "def rand_bbox(size, lam):\n",
        "    W = size[2]\n",
        "    H = size[3]\n",
        "    cut_rat = np.sqrt(1. - lam)\n",
        "    cut_w = np.int(W * cut_rat)\n",
        "    cut_h = np.int(H * cut_rat)\n",
        "\n",
        "    # uniform\n",
        "    cx = np.random.randint(W)\n",
        "    cy = np.random.randint(H)\n",
        "\n",
        "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
        "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
        "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
        "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
        "\n",
        "    return bbx1, bby1, bbx2, bby2\n",
        "\n",
        "\n",
        "def validate(val_loader, model, criterion, epoch):\n",
        "    batch_time = AverageMeter()\n",
        "    losses = AverageMeter()\n",
        "    top1 = AverageMeter()\n",
        "    top5 = AverageMeter()\n",
        "\n",
        "    # switch to evaluate mode\n",
        "    model.eval()\n",
        "\n",
        "    end = time.time()\n",
        "    for i, (input, target) in enumerate(val_loader):\n",
        "        target = target.cuda()\n",
        "\n",
        "        output = model(input)\n",
        "        loss = criterion(output, target)\n",
        "\n",
        "        # measure accuracy and record loss\n",
        "        err1, err5 = accuracy(output.data, target, topk=(1, 5))\n",
        "\n",
        "        losses.update(loss.item(), input.size(0))\n",
        "\n",
        "        top1.update(err1.item(), input.size(0))\n",
        "        top5.update(err5.item(), input.size(0))\n",
        "\n",
        "        # measure elapsed time\n",
        "        batch_time.update(time.time() - end)\n",
        "        end = time.time()\n",
        "\n",
        "    print('* Epoch: [{0}/{1}]\\t Top 1-err {top1.avg:.3f}  Top 5-err {top5.avg:.3f}\\t Test Loss {loss.avg:.3f}'.format(\n",
        "        epoch, args.epochs, top1=top1, top5=top5, loss=losses))\n",
        "    return top1.avg, top5.avg, losses.avg\n",
        "\n",
        "\n",
        "def save_checkpoint(state, is_best, filename='checkpoint.pth.tar'):\n",
        "    directory = os.path.join(\"/content/drive/MyDrive/ajoudeep/contest/\",\"bsconv_sam/\")\n",
        "    if not os.path.exists(directory):\n",
        "        os.makedirs(directory)\n",
        "    filename = os.path.join(directory, filename)\n",
        "    torch.save(state, filename)\n",
        "    if is_best:\n",
        "        shutil.copyfile(filename, os.path.join('/content/drive/MyDrive/ajoudeep/contest/bsconv_sam/','model_best.pth.tar'))\n",
        "\n",
        "\n",
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "\n",
        "def adjust_learning_rate(optimizer, epoch):\n",
        "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\"\n",
        "    lr = args.lr * (0.1 ** (epoch // (args.epochs * 0.3))) * (0.1 ** (epoch // (args.epochs * 0.75)))\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "\n",
        "def get_learning_rate(optimizer):\n",
        "    lr = []\n",
        "    for param_group in optimizer.param_groups:\n",
        "        lr += [param_group['lr']]\n",
        "    return lr\n",
        "\n",
        "def accuracy(output, target, topk=(1,)):\n",
        "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
        "    maxk = max(topk)\n",
        "    batch_size = target.size(0)\n",
        "\n",
        "    _, pred = output.topk(maxk, 1, True, True)\n",
        "    pred = pred.t()\n",
        "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "\n",
        "    res = []\n",
        "    for k in topk:\n",
        "        correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
        "        wrong_k = batch_size - correct_k\n",
        "        res.append(wrong_k.mul_(100.0 / batch_size))\n",
        "\n",
        "    return res\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ../data/cifar-100-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "503e012abca548cf8d08395594d86e19",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ../data/cifar-100-python.tar.gz to ../data\n",
            "=> creating model 'resnet'\n",
            "DataParallel(\n",
            "  (module): ResNet(\n",
            "    (backbone): Sequential(\n",
            "      (data_bn): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (init_unit): InitUnitSmall(\n",
            "        (conv): ConvBlock(\n",
            "          (conv): BSConvS(\n",
            "            (pw1): Conv2d(3, 3, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (bn1): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (pw2): Conv2d(3, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (dw): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (stage1): Sequential(\n",
            "        (unit1): PreactUnit(\n",
            "          (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "          (conv1): ConvBlock(\n",
            "            (conv): BSConvS(\n",
            "              (pw1): Conv2d(16, 4, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (pw2): Conv2d(4, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (dw): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
            "            )\n",
            "            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (activation): ReLU(inplace=True)\n",
            "          )\n",
            "          (conv2): ConvBlock(\n",
            "            (conv): BSConvS(\n",
            "              (pw1): Conv2d(64, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (pw2): Conv2d(8, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (dw): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
            "            )\n",
            "          )\n",
            "          (projection): ConvBlock(\n",
            "            (conv): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          )\n",
            "        )\n",
            "        (unit2): PreactUnit(\n",
            "          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "          (conv1): ConvBlock(\n",
            "            (conv): BSConvS(\n",
            "              (pw1): Conv2d(64, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (pw2): Conv2d(8, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (dw): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
            "            )\n",
            "            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (activation): ReLU(inplace=True)\n",
            "          )\n",
            "          (conv2): ConvBlock(\n",
            "            (conv): BSConvS(\n",
            "              (pw1): Conv2d(64, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (pw2): Conv2d(8, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (dw): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (unit3): PreactUnit(\n",
            "          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "          (conv1): ConvBlock(\n",
            "            (conv): BSConvS(\n",
            "              (pw1): Conv2d(64, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (pw2): Conv2d(8, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (dw): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
            "            )\n",
            "            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (activation): ReLU(inplace=True)\n",
            "          )\n",
            "          (conv2): ConvBlock(\n",
            "            (conv): BSConvS(\n",
            "              (pw1): Conv2d(64, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (pw2): Conv2d(8, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (dw): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (unit4): PreactUnit(\n",
            "          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "          (conv1): ConvBlock(\n",
            "            (conv): BSConvS(\n",
            "              (pw1): Conv2d(64, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (pw2): Conv2d(8, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (dw): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
            "            )\n",
            "            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (activation): ReLU(inplace=True)\n",
            "          )\n",
            "          (conv2): ConvBlock(\n",
            "            (conv): BSConvS(\n",
            "              (pw1): Conv2d(64, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (pw2): Conv2d(8, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (dw): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (stage2): Sequential(\n",
            "        (unit1): PreactUnit(\n",
            "          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "          (conv1): ConvBlock(\n",
            "            (conv): BSConvS(\n",
            "              (pw1): Conv2d(64, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (pw2): Conv2d(8, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (dw): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)\n",
            "            )\n",
            "            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (activation): ReLU(inplace=True)\n",
            "          )\n",
            "          (conv2): ConvBlock(\n",
            "            (conv): BSConvS(\n",
            "              (pw1): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (pw2): Conv2d(16, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (dw): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
            "            )\n",
            "          )\n",
            "          (projection): ConvBlock(\n",
            "            (conv): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          )\n",
            "        )\n",
            "        (unit2): PreactUnit(\n",
            "          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "          (conv1): ConvBlock(\n",
            "            (conv): BSConvS(\n",
            "              (pw1): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (pw2): Conv2d(16, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (dw): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
            "            )\n",
            "            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (activation): ReLU(inplace=True)\n",
            "          )\n",
            "          (conv2): ConvBlock(\n",
            "            (conv): BSConvS(\n",
            "              (pw1): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (pw2): Conv2d(16, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (dw): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (unit3): PreactUnit(\n",
            "          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "          (conv1): ConvBlock(\n",
            "            (conv): BSConvS(\n",
            "              (pw1): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (pw2): Conv2d(16, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (dw): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
            "            )\n",
            "            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (activation): ReLU(inplace=True)\n",
            "          )\n",
            "          (conv2): ConvBlock(\n",
            "            (conv): BSConvS(\n",
            "              (pw1): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (pw2): Conv2d(16, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (dw): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (unit4): PreactUnit(\n",
            "          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "          (conv1): ConvBlock(\n",
            "            (conv): BSConvS(\n",
            "              (pw1): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (pw2): Conv2d(16, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (dw): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
            "            )\n",
            "            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (activation): ReLU(inplace=True)\n",
            "          )\n",
            "          (conv2): ConvBlock(\n",
            "            (conv): BSConvS(\n",
            "              (pw1): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (pw2): Conv2d(16, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (dw): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (stage3): Sequential(\n",
            "        (unit1): PreactUnit(\n",
            "          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "          (conv1): ConvBlock(\n",
            "            (conv): BSConvS(\n",
            "              (pw1): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (pw2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (dw): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)\n",
            "            )\n",
            "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (activation): ReLU(inplace=True)\n",
            "          )\n",
            "          (conv2): ConvBlock(\n",
            "            (conv): BSConvS(\n",
            "              (pw1): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (pw2): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (dw): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
            "            )\n",
            "          )\n",
            "          (projection): ConvBlock(\n",
            "            (conv): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          )\n",
            "        )\n",
            "        (unit2): PreactUnit(\n",
            "          (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "          (conv1): ConvBlock(\n",
            "            (conv): BSConvS(\n",
            "              (pw1): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (pw2): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (dw): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
            "            )\n",
            "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (activation): ReLU(inplace=True)\n",
            "          )\n",
            "          (conv2): ConvBlock(\n",
            "            (conv): BSConvS(\n",
            "              (pw1): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (pw2): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (dw): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (unit3): PreactUnit(\n",
            "          (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "          (conv1): ConvBlock(\n",
            "            (conv): BSConvS(\n",
            "              (pw1): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (pw2): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (dw): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
            "            )\n",
            "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (activation): ReLU(inplace=True)\n",
            "          )\n",
            "          (conv2): ConvBlock(\n",
            "            (conv): BSConvS(\n",
            "              (pw1): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (pw2): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (dw): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (unit4): PreactUnit(\n",
            "          (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "          (conv1): ConvBlock(\n",
            "            (conv): BSConvS(\n",
            "              (pw1): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (pw2): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (dw): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
            "            )\n",
            "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (activation): ReLU(inplace=True)\n",
            "          )\n",
            "          (conv2): ConvBlock(\n",
            "            (conv): BSConvS(\n",
            "              (pw1): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (pw2): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (dw): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (final_activation): PostActivation(\n",
            "        (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (global_pool): AdaptiveAvgPool2d(output_size=1)\n",
            "    )\n",
            "    (classifier): Classifier(\n",
            "      (conv): Conv2d(256, 100, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "  )\n",
            ")\n",
            "the number of model parameters: 273921\n",
            "Epoch: [0/290][99/782]\tLR: 0.004000\tTime 0.154 (0.156)\tData 0.000 (0.003)\tLoss 4.5383 (4.6129)\tTop 1-err 95.3125 (98.4766)\tTop 5-err 90.6250 (92.7422)\n",
            "Epoch: [0/290][199/782]\tLR: 0.004000\tTime 0.146 (0.152)\tData 0.000 (0.002)\tLoss 4.5576 (4.5349)\tTop 1-err 98.4375 (97.4648)\tTop 5-err 90.6250 (89.7461)\n",
            "Epoch: [0/290][299/782]\tLR: 0.004000\tTime 0.147 (0.151)\tData 0.000 (0.001)\tLoss 4.5317 (4.4767)\tTop 1-err 98.4375 (96.7943)\tTop 5-err 90.6250 (87.4505)\n",
            "Epoch: [0/290][399/782]\tLR: 0.004000\tTime 0.147 (0.151)\tData 0.000 (0.001)\tLoss 4.1840 (4.4236)\tTop 1-err 93.7500 (96.2227)\tTop 5-err 81.2500 (85.6348)\n",
            "Epoch: [0/290][499/782]\tLR: 0.004000\tTime 0.157 (0.151)\tData 0.000 (0.001)\tLoss 4.0692 (4.3797)\tTop 1-err 92.1875 (95.6875)\tTop 5-err 81.2500 (84.0891)\n",
            "Epoch: [0/290][599/782]\tLR: 0.004000\tTime 0.143 (0.150)\tData 0.000 (0.001)\tLoss 3.9792 (4.3476)\tTop 1-err 93.7500 (95.2786)\tTop 5-err 67.1875 (83.0443)\n",
            "Epoch: [0/290][699/782]\tLR: 0.004000\tTime 0.148 (0.150)\tData 0.000 (0.001)\tLoss 3.8654 (4.3200)\tTop 1-err 84.3750 (94.9062)\tTop 5-err 67.1875 (82.1350)\n",
            "* Epoch: [0/290]\t Top 1-err 94.560  Top 5-err 81.347\t Train Loss 4.297\n",
            "* Epoch: [0/290]\t Top 1-err 88.630  Top 5-err 67.250\t Test Loss 3.808\n",
            "Current best accuracy (top-1 and 5 error): 88.63 67.25\n",
            "Epoch: [1/290][99/782]\tLR: 0.004001\tTime 0.146 (0.149)\tData 0.000 (0.002)\tLoss 4.2689 (4.0255)\tTop 1-err 92.1875 (91.0234)\tTop 5-err 82.8125 (72.7734)\n",
            "Epoch: [1/290][199/782]\tLR: 0.004001\tTime 0.146 (0.149)\tData 0.000 (0.001)\tLoss 4.5564 (4.0179)\tTop 1-err 100.0000 (90.9141)\tTop 5-err 100.0000 (72.1250)\n",
            "Epoch: [1/290][299/782]\tLR: 0.004001\tTime 0.150 (0.149)\tData 0.000 (0.001)\tLoss 4.2073 (4.0025)\tTop 1-err 93.7500 (90.6562)\tTop 5-err 79.6875 (71.7135)\n",
            "Epoch: [1/290][399/782]\tLR: 0.004001\tTime 0.144 (0.149)\tData 0.000 (0.001)\tLoss 4.5134 (3.9865)\tTop 1-err 92.1875 (90.4512)\tTop 5-err 85.9375 (71.2109)\n",
            "Epoch: [1/290][499/782]\tLR: 0.004001\tTime 0.153 (0.148)\tData 0.000 (0.001)\tLoss 4.0645 (3.9842)\tTop 1-err 90.6250 (90.4125)\tTop 5-err 70.3125 (71.0609)\n",
            "Epoch: [1/290][599/782]\tLR: 0.004001\tTime 0.149 (0.149)\tData 0.000 (0.000)\tLoss 4.3625 (3.9678)\tTop 1-err 93.7500 (90.0924)\tTop 5-err 84.3750 (70.4453)\n",
            "Epoch: [1/290][699/782]\tLR: 0.004001\tTime 0.146 (0.148)\tData 0.000 (0.000)\tLoss 3.8732 (3.9700)\tTop 1-err 85.9375 (90.0592)\tTop 5-err 65.6250 (70.5379)\n",
            "* Epoch: [1/290]\t Top 1-err 89.901  Top 5-err 70.223\t Train Loss 3.961\n",
            "* Epoch: [1/290]\t Top 1-err 84.230  Top 5-err 58.540\t Test Loss 3.528\n",
            "Current best accuracy (top-1 and 5 error): 84.23 58.54\n",
            "Epoch: [2/290][99/782]\tLR: 0.004003\tTime 0.145 (0.151)\tData 0.000 (0.002)\tLoss 3.5350 (3.8334)\tTop 1-err 87.5000 (87.4609)\tTop 5-err 60.9375 (65.6953)\n",
            "Epoch: [2/290][199/782]\tLR: 0.004003\tTime 0.153 (0.149)\tData 0.000 (0.001)\tLoss 3.4798 (3.8333)\tTop 1-err 79.6875 (87.8125)\tTop 5-err 54.6875 (65.6953)\n",
            "Epoch: [2/290][299/782]\tLR: 0.004003\tTime 0.151 (0.149)\tData 0.000 (0.001)\tLoss 4.3975 (3.8521)\tTop 1-err 96.8750 (87.9870)\tTop 5-err 75.0000 (66.2760)\n",
            "Epoch: [2/290][399/782]\tLR: 0.004003\tTime 0.146 (0.148)\tData 0.000 (0.001)\tLoss 3.3029 (3.8244)\tTop 1-err 81.2500 (87.5586)\tTop 5-err 51.5625 (65.6172)\n",
            "Epoch: [2/290][499/782]\tLR: 0.004003\tTime 0.145 (0.149)\tData 0.000 (0.000)\tLoss 4.3944 (3.8004)\tTop 1-err 98.4375 (87.1422)\tTop 5-err 81.2500 (64.9938)\n",
            "Epoch: [2/290][599/782]\tLR: 0.004003\tTime 0.147 (0.149)\tData 0.000 (0.000)\tLoss 4.2341 (3.7807)\tTop 1-err 93.7500 (86.6914)\tTop 5-err 85.9375 (64.3359)\n",
            "Epoch: [2/290][699/782]\tLR: 0.004003\tTime 0.148 (0.149)\tData 0.000 (0.000)\tLoss 3.2272 (3.7665)\tTop 1-err 79.6875 (86.5569)\tTop 5-err 51.5625 (63.9877)\n",
            "* Epoch: [2/290]\t Top 1-err 86.352  Top 5-err 63.774\t Train Loss 3.761\n",
            "* Epoch: [2/290]\t Top 1-err 81.250  Top 5-err 52.630\t Test Loss 3.305\n",
            "Current best accuracy (top-1 and 5 error): 81.25 52.63\n",
            "Epoch: [3/290][99/782]\tLR: 0.004005\tTime 0.144 (0.151)\tData 0.000 (0.002)\tLoss 3.3140 (3.6521)\tTop 1-err 84.3750 (84.3359)\tTop 5-err 53.1250 (60.0156)\n",
            "Epoch: [3/290][199/782]\tLR: 0.004005\tTime 0.145 (0.150)\tData 0.000 (0.001)\tLoss 3.0357 (3.6202)\tTop 1-err 68.7500 (83.5195)\tTop 5-err 42.1875 (59.1016)\n",
            "Epoch: [3/290][299/782]\tLR: 0.004005\tTime 0.146 (0.149)\tData 0.000 (0.001)\tLoss 4.4365 (3.6493)\tTop 1-err 93.7500 (83.8620)\tTop 5-err 79.6875 (59.7786)\n",
            "Epoch: [3/290][399/782]\tLR: 0.004005\tTime 0.148 (0.149)\tData 0.000 (0.001)\tLoss 3.3348 (3.6424)\tTop 1-err 78.1250 (83.8477)\tTop 5-err 53.1250 (59.4238)\n",
            "Epoch: [3/290][499/782]\tLR: 0.004005\tTime 0.156 (0.149)\tData 0.000 (0.001)\tLoss 3.4109 (3.6308)\tTop 1-err 84.3750 (83.7250)\tTop 5-err 51.5625 (59.3250)\n",
            "Epoch: [3/290][599/782]\tLR: 0.004005\tTime 0.144 (0.148)\tData 0.000 (0.000)\tLoss 3.0995 (3.6217)\tTop 1-err 79.6875 (83.5469)\tTop 5-err 45.3125 (59.0326)\n",
            "Epoch: [3/290][699/782]\tLR: 0.004005\tTime 0.144 (0.148)\tData 0.000 (0.000)\tLoss 3.4642 (3.6062)\tTop 1-err 84.3750 (83.3225)\tTop 5-err 54.6875 (58.6708)\n",
            "* Epoch: [3/290]\t Top 1-err 83.235  Top 5-err 58.516\t Train Loss 3.598\n",
            "* Epoch: [3/290]\t Top 1-err 75.960  Top 5-err 45.780\t Test Loss 3.037\n",
            "Current best accuracy (top-1 and 5 error): 75.96 45.78\n",
            "Epoch: [4/290][99/782]\tLR: 0.004008\tTime 0.157 (0.150)\tData 0.000 (0.002)\tLoss 3.0564 (3.5012)\tTop 1-err 73.4375 (81.8359)\tTop 5-err 45.3125 (55.7500)\n",
            "Epoch: [4/290][199/782]\tLR: 0.004008\tTime 0.157 (0.150)\tData 0.000 (0.001)\tLoss 3.5222 (3.5162)\tTop 1-err 84.3750 (81.7930)\tTop 5-err 59.3750 (56.1602)\n",
            "Epoch: [4/290][299/782]\tLR: 0.004008\tTime 0.154 (0.149)\tData 0.000 (0.001)\tLoss 3.0391 (3.4888)\tTop 1-err 76.5625 (81.3906)\tTop 5-err 39.0625 (55.5547)\n",
            "Epoch: [4/290][399/782]\tLR: 0.004008\tTime 0.146 (0.149)\tData 0.000 (0.001)\tLoss 4.1608 (3.4752)\tTop 1-err 90.6250 (81.1113)\tTop 5-err 65.6250 (55.1426)\n",
            "Epoch: [4/290][499/782]\tLR: 0.004008\tTime 0.144 (0.149)\tData 0.000 (0.001)\tLoss 3.4492 (3.4697)\tTop 1-err 84.3750 (80.9266)\tTop 5-err 54.6875 (54.8563)\n",
            "Epoch: [4/290][599/782]\tLR: 0.004008\tTime 0.148 (0.148)\tData 0.000 (0.001)\tLoss 4.2846 (3.4705)\tTop 1-err 89.0625 (80.9531)\tTop 5-err 75.0000 (55.0430)\n",
            "Epoch: [4/290][699/782]\tLR: 0.004008\tTime 0.121 (0.148)\tData 0.000 (0.000)\tLoss 2.8781 (3.4637)\tTop 1-err 71.8750 (80.8259)\tTop 5-err 40.6250 (54.8839)\n",
            "* Epoch: [4/290]\t Top 1-err 80.625  Top 5-err 54.581\t Train Loss 3.448\n",
            "* Epoch: [4/290]\t Top 1-err 71.990  Top 5-err 40.340\t Test Loss 2.849\n",
            "Current best accuracy (top-1 and 5 error): 71.99 40.34\n",
            "Epoch: [5/290][99/782]\tLR: 0.004011\tTime 0.144 (0.150)\tData 0.000 (0.002)\tLoss 2.9678 (3.4191)\tTop 1-err 76.5625 (78.9688)\tTop 5-err 42.1875 (53.1797)\n",
            "Epoch: [5/290][199/782]\tLR: 0.004011\tTime 0.149 (0.149)\tData 0.000 (0.001)\tLoss 4.1884 (3.3990)\tTop 1-err 98.4375 (79.3750)\tTop 5-err 79.6875 (53.3789)\n",
            "Epoch: [5/290][299/782]\tLR: 0.004011\tTime 0.153 (0.148)\tData 0.000 (0.001)\tLoss 4.9387 (3.3950)\tTop 1-err 100.0000 (79.2500)\tTop 5-err 89.0625 (53.2292)\n",
            "Epoch: [5/290][399/782]\tLR: 0.004011\tTime 0.146 (0.148)\tData 0.000 (0.001)\tLoss 4.6897 (3.3823)\tTop 1-err 98.4375 (79.0762)\tTop 5-err 96.8750 (52.7480)\n",
            "Epoch: [5/290][499/782]\tLR: 0.004011\tTime 0.145 (0.148)\tData 0.000 (0.001)\tLoss 4.0951 (3.3684)\tTop 1-err 89.0625 (78.6688)\tTop 5-err 75.0000 (52.3703)\n",
            "Epoch: [5/290][599/782]\tLR: 0.004011\tTime 0.152 (0.148)\tData 0.000 (0.000)\tLoss 2.8590 (3.3682)\tTop 1-err 81.2500 (78.7266)\tTop 5-err 40.6250 (52.4115)\n",
            "Epoch: [5/290][699/782]\tLR: 0.004011\tTime 0.144 (0.148)\tData 0.000 (0.000)\tLoss 3.1175 (3.3506)\tTop 1-err 70.3125 (78.4487)\tTop 5-err 45.3125 (51.8839)\n",
            "* Epoch: [5/290]\t Top 1-err 78.159  Top 5-err 51.405\t Train Loss 3.332\n",
            "* Epoch: [5/290]\t Top 1-err 70.220  Top 5-err 38.320\t Test Loss 2.737\n",
            "Current best accuracy (top-1 and 5 error): 70.22 38.32\n",
            "Epoch: [6/290][99/782]\tLR: 0.004015\tTime 0.147 (0.150)\tData 0.000 (0.002)\tLoss 2.6866 (3.2773)\tTop 1-err 68.7500 (76.1328)\tTop 5-err 39.0625 (49.2891)\n",
            "Epoch: [6/290][199/782]\tLR: 0.004015\tTime 0.144 (0.149)\tData 0.000 (0.001)\tLoss 2.7444 (3.2603)\tTop 1-err 71.8750 (76.3281)\tTop 5-err 42.1875 (48.8906)\n",
            "Epoch: [6/290][299/782]\tLR: 0.004015\tTime 0.145 (0.149)\tData 0.000 (0.001)\tLoss 2.8589 (3.2416)\tTop 1-err 79.6875 (76.0391)\tTop 5-err 35.9375 (48.5156)\n",
            "Epoch: [6/290][399/782]\tLR: 0.004015\tTime 0.146 (0.149)\tData 0.000 (0.001)\tLoss 2.6130 (3.2025)\tTop 1-err 70.3125 (75.6172)\tTop 5-err 39.0625 (47.5000)\n",
            "Epoch: [6/290][499/782]\tLR: 0.004015\tTime 0.146 (0.149)\tData 0.000 (0.001)\tLoss 2.7745 (3.1964)\tTop 1-err 71.8750 (75.4531)\tTop 5-err 46.8750 (47.4594)\n",
            "Epoch: [6/290][599/782]\tLR: 0.004015\tTime 0.150 (0.148)\tData 0.000 (0.000)\tLoss 2.8732 (3.1987)\tTop 1-err 73.4375 (75.4284)\tTop 5-err 45.3125 (47.3633)\n",
            "Epoch: [6/290][699/782]\tLR: 0.004015\tTime 0.151 (0.148)\tData 0.000 (0.000)\tLoss 4.5576 (3.2076)\tTop 1-err 98.4375 (75.5569)\tTop 5-err 85.9375 (47.6362)\n",
            "* Epoch: [6/290]\t Top 1-err 75.493  Top 5-err 47.691\t Train Loss 3.207\n",
            "* Epoch: [6/290]\t Top 1-err 65.730  Top 5-err 31.860\t Test Loss 2.507\n",
            "Current best accuracy (top-1 and 5 error): 65.73 31.86\n",
            "Epoch: [7/290][99/782]\tLR: 0.004020\tTime 0.145 (0.151)\tData 0.000 (0.002)\tLoss 2.7245 (3.1684)\tTop 1-err 78.1250 (75.0625)\tTop 5-err 34.3750 (47.1016)\n",
            "Epoch: [7/290][199/782]\tLR: 0.004020\tTime 0.149 (0.150)\tData 0.000 (0.001)\tLoss 2.9373 (3.1274)\tTop 1-err 76.5625 (74.2383)\tTop 5-err 43.7500 (45.9805)\n",
            "Epoch: [7/290][299/782]\tLR: 0.004020\tTime 0.150 (0.150)\tData 0.000 (0.001)\tLoss 2.7975 (3.1091)\tTop 1-err 68.7500 (73.8385)\tTop 5-err 43.7500 (45.2005)\n",
            "Epoch: [7/290][399/782]\tLR: 0.004020\tTime 0.149 (0.149)\tData 0.000 (0.001)\tLoss 3.7528 (3.1015)\tTop 1-err 82.8125 (73.6797)\tTop 5-err 43.7500 (44.6992)\n",
            "Epoch: [7/290][499/782]\tLR: 0.004020\tTime 0.144 (0.149)\tData 0.000 (0.000)\tLoss 2.6010 (3.1066)\tTop 1-err 70.3125 (73.7438)\tTop 5-err 29.6875 (45.0719)\n",
            "Epoch: [7/290][599/782]\tLR: 0.004020\tTime 0.150 (0.149)\tData 0.000 (0.000)\tLoss 4.6411 (3.1230)\tTop 1-err 92.1875 (73.9870)\tTop 5-err 78.1250 (45.5521)\n",
            "Epoch: [7/290][699/782]\tLR: 0.004020\tTime 0.144 (0.149)\tData 0.000 (0.000)\tLoss 3.9978 (3.1234)\tTop 1-err 95.3125 (73.8538)\tTop 5-err 79.6875 (45.5926)\n",
            "* Epoch: [7/290]\t Top 1-err 73.665  Top 5-err 45.370\t Train Loss 3.117\n",
            "* Epoch: [7/290]\t Top 1-err 63.010  Top 5-err 30.730\t Test Loss 2.434\n",
            "Current best accuracy (top-1 and 5 error): 63.01 30.73\n",
            "Epoch: [8/290][99/782]\tLR: 0.004025\tTime 0.144 (0.150)\tData 0.000 (0.002)\tLoss 2.3996 (3.0108)\tTop 1-err 59.3750 (71.3672)\tTop 5-err 29.6875 (42.5469)\n",
            "Epoch: [8/290][199/782]\tLR: 0.004025\tTime 0.145 (0.150)\tData 0.000 (0.001)\tLoss 5.3206 (3.0438)\tTop 1-err 98.4375 (72.3398)\tTop 5-err 92.1875 (43.7891)\n",
            "Epoch: [8/290][299/782]\tLR: 0.004025\tTime 0.145 (0.149)\tData 0.000 (0.001)\tLoss 2.7420 (3.0113)\tTop 1-err 60.9375 (71.9245)\tTop 5-err 43.7500 (43.1719)\n",
            "Epoch: [8/290][399/782]\tLR: 0.004025\tTime 0.144 (0.149)\tData 0.000 (0.001)\tLoss 2.3170 (3.0090)\tTop 1-err 54.6875 (71.7832)\tTop 5-err 26.5625 (42.8613)\n",
            "Epoch: [8/290][499/782]\tLR: 0.004025\tTime 0.145 (0.149)\tData 0.000 (0.000)\tLoss 3.9365 (2.9912)\tTop 1-err 93.7500 (71.4953)\tTop 5-err 78.1250 (42.4578)\n",
            "Epoch: [8/290][599/782]\tLR: 0.004025\tTime 0.153 (0.149)\tData 0.000 (0.000)\tLoss 3.9504 (2.9913)\tTop 1-err 85.9375 (71.4297)\tTop 5-err 67.1875 (42.4297)\n",
            "Epoch: [8/290][699/782]\tLR: 0.004025\tTime 0.145 (0.149)\tData 0.000 (0.000)\tLoss 2.5846 (2.9983)\tTop 1-err 65.6250 (71.4375)\tTop 5-err 32.8125 (42.6138)\n",
            "* Epoch: [8/290]\t Top 1-err 71.451  Top 5-err 42.661\t Train Loss 3.002\n",
            "* Epoch: [8/290]\t Top 1-err 59.810  Top 5-err 27.300\t Test Loss 2.268\n",
            "Current best accuracy (top-1 and 5 error): 59.81 27.3\n",
            "Epoch: [9/290][99/782]\tLR: 0.004031\tTime 0.148 (0.150)\tData 0.000 (0.002)\tLoss 2.8684 (3.0655)\tTop 1-err 71.8750 (71.8984)\tTop 5-err 32.8125 (44.2500)\n",
            "Epoch: [9/290][199/782]\tLR: 0.004031\tTime 0.144 (0.149)\tData 0.000 (0.001)\tLoss 2.3578 (3.0050)\tTop 1-err 60.9375 (70.6523)\tTop 5-err 28.1250 (43.3008)\n",
            "Epoch: [9/290][299/782]\tLR: 0.004031\tTime 0.145 (0.149)\tData 0.000 (0.001)\tLoss 4.0377 (2.9908)\tTop 1-err 96.8750 (70.3568)\tTop 5-err 87.5000 (42.7786)\n",
            "Epoch: [9/290][399/782]\tLR: 0.004031\tTime 0.144 (0.148)\tData 0.000 (0.001)\tLoss 2.4089 (3.0043)\tTop 1-err 54.6875 (70.4023)\tTop 5-err 25.0000 (42.8242)\n",
            "Epoch: [9/290][499/782]\tLR: 0.004031\tTime 0.144 (0.148)\tData 0.000 (0.000)\tLoss 2.0200 (2.9958)\tTop 1-err 51.5625 (70.3391)\tTop 5-err 18.7500 (42.3391)\n",
            "Epoch: [9/290][599/782]\tLR: 0.004031\tTime 0.148 (0.148)\tData 0.000 (0.000)\tLoss 2.1775 (2.9889)\tTop 1-err 57.8125 (70.3646)\tTop 5-err 23.4375 (42.1393)\n",
            "Epoch: [9/290][699/782]\tLR: 0.004031\tTime 0.153 (0.148)\tData 0.000 (0.000)\tLoss 2.3541 (2.9887)\tTop 1-err 56.2500 (70.2902)\tTop 5-err 25.0000 (42.0859)\n",
            "* Epoch: [9/290]\t Top 1-err 70.449  Top 5-err 42.358\t Train Loss 2.997\n",
            "* Epoch: [9/290]\t Top 1-err 58.300  Top 5-err 25.580\t Test Loss 2.214\n",
            "Current best accuracy (top-1 and 5 error): 58.3 25.58\n",
            "Epoch: [10/290][99/782]\tLR: 0.004038\tTime 0.145 (0.149)\tData 0.000 (0.002)\tLoss 2.6881 (3.0079)\tTop 1-err 70.3125 (70.5859)\tTop 5-err 39.0625 (43.5469)\n",
            "Epoch: [10/290][199/782]\tLR: 0.004038\tTime 0.144 (0.148)\tData 0.000 (0.001)\tLoss 2.4381 (2.9271)\tTop 1-err 57.8125 (69.3398)\tTop 5-err 29.6875 (41.1172)\n",
            "Epoch: [10/290][299/782]\tLR: 0.004038\tTime 0.145 (0.149)\tData 0.000 (0.001)\tLoss 3.8797 (2.9370)\tTop 1-err 92.1875 (69.3672)\tTop 5-err 62.5000 (41.0547)\n",
            "Epoch: [10/290][399/782]\tLR: 0.004038\tTime 0.152 (0.148)\tData 0.000 (0.001)\tLoss 2.3806 (2.9320)\tTop 1-err 57.8125 (69.1719)\tTop 5-err 32.8125 (41.1309)\n",
            "Epoch: [10/290][499/782]\tLR: 0.004038\tTime 0.144 (0.148)\tData 0.000 (0.001)\tLoss 4.0033 (2.9308)\tTop 1-err 98.4375 (69.2188)\tTop 5-err 87.5000 (41.2078)\n",
            "Epoch: [10/290][599/782]\tLR: 0.004038\tTime 0.154 (0.148)\tData 0.000 (0.000)\tLoss 3.8289 (2.9234)\tTop 1-err 84.3750 (68.9661)\tTop 5-err 59.3750 (40.9531)\n",
            "Epoch: [10/290][699/782]\tLR: 0.004038\tTime 0.147 (0.148)\tData 0.000 (0.000)\tLoss 3.8323 (2.9191)\tTop 1-err 87.5000 (69.0781)\tTop 5-err 54.6875 (40.8114)\n",
            "* Epoch: [10/290]\t Top 1-err 69.012  Top 5-err 40.629\t Train Loss 2.913\n",
            "* Epoch: [10/290]\t Top 1-err 56.820  Top 5-err 24.360\t Test Loss 2.134\n",
            "Current best accuracy (top-1 and 5 error): 56.82 24.36\n",
            "Epoch: [11/290][99/782]\tLR: 0.004045\tTime 0.146 (0.150)\tData 0.000 (0.002)\tLoss 5.0264 (2.9605)\tTop 1-err 95.3125 (69.1250)\tTop 5-err 90.6250 (42.0234)\n",
            "Epoch: [11/290][199/782]\tLR: 0.004045\tTime 0.154 (0.148)\tData 0.000 (0.001)\tLoss 3.6456 (2.8668)\tTop 1-err 89.0625 (67.8750)\tTop 5-err 43.7500 (39.8086)\n",
            "Epoch: [11/290][299/782]\tLR: 0.004045\tTime 0.144 (0.148)\tData 0.000 (0.001)\tLoss 2.1587 (2.8145)\tTop 1-err 59.3750 (66.9271)\tTop 5-err 18.7500 (38.4401)\n",
            "Epoch: [11/290][399/782]\tLR: 0.004045\tTime 0.152 (0.148)\tData 0.000 (0.001)\tLoss 2.1647 (2.8291)\tTop 1-err 57.8125 (67.2285)\tTop 5-err 23.4375 (38.5020)\n",
            "Epoch: [11/290][499/782]\tLR: 0.004045\tTime 0.152 (0.148)\tData 0.000 (0.000)\tLoss 4.1278 (2.8351)\tTop 1-err 95.3125 (67.2484)\tTop 5-err 81.2500 (38.5625)\n",
            "Epoch: [11/290][599/782]\tLR: 0.004045\tTime 0.147 (0.148)\tData 0.000 (0.000)\tLoss 3.4388 (2.8417)\tTop 1-err 71.8750 (67.3919)\tTop 5-err 53.1250 (38.6263)\n",
            "Epoch: [11/290][699/782]\tLR: 0.004045\tTime 0.150 (0.148)\tData 0.000 (0.000)\tLoss 2.5851 (2.8556)\tTop 1-err 65.6250 (67.5826)\tTop 5-err 35.9375 (39.0033)\n",
            "* Epoch: [11/290]\t Top 1-err 67.723  Top 5-err 39.303\t Train Loss 2.867\n",
            "* Epoch: [11/290]\t Top 1-err 55.760  Top 5-err 23.890\t Test Loss 2.090\n",
            "Current best accuracy (top-1 and 5 error): 55.76 23.89\n",
            "Epoch: [12/290][99/782]\tLR: 0.004053\tTime 0.154 (0.151)\tData 0.000 (0.002)\tLoss 2.4526 (2.6938)\tTop 1-err 65.6250 (64.3672)\tTop 5-err 29.6875 (34.4141)\n",
            "Epoch: [12/290][199/782]\tLR: 0.004053\tTime 0.147 (0.149)\tData 0.000 (0.001)\tLoss 4.2695 (2.7975)\tTop 1-err 92.1875 (66.2305)\tTop 5-err 78.1250 (37.0469)\n",
            "Epoch: [12/290][299/782]\tLR: 0.004053\tTime 0.145 (0.149)\tData 0.000 (0.001)\tLoss 2.0811 (2.8097)\tTop 1-err 59.3750 (66.4609)\tTop 5-err 18.7500 (37.5703)\n",
            "Epoch: [12/290][399/782]\tLR: 0.004053\tTime 0.153 (0.149)\tData 0.000 (0.001)\tLoss 2.1507 (2.7932)\tTop 1-err 59.3750 (66.1602)\tTop 5-err 25.0000 (37.2539)\n",
            "Epoch: [12/290][499/782]\tLR: 0.004053\tTime 0.145 (0.149)\tData 0.000 (0.001)\tLoss 4.1511 (2.7774)\tTop 1-err 85.9375 (65.9094)\tTop 5-err 64.0625 (36.9344)\n",
            "Epoch: [12/290][599/782]\tLR: 0.004053\tTime 0.145 (0.148)\tData 0.000 (0.000)\tLoss 1.9178 (2.7760)\tTop 1-err 53.1250 (65.8451)\tTop 5-err 23.4375 (36.8112)\n",
            "Epoch: [12/290][699/782]\tLR: 0.004053\tTime 0.152 (0.148)\tData 0.000 (0.000)\tLoss 1.8537 (2.7642)\tTop 1-err 53.1250 (65.5491)\tTop 5-err 20.3125 (36.3906)\n",
            "* Epoch: [12/290]\t Top 1-err 65.457  Top 5-err 36.358\t Train Loss 2.762\n",
            "* Epoch: [12/290]\t Top 1-err 53.550  Top 5-err 22.980\t Test Loss 2.006\n",
            "Current best accuracy (top-1 and 5 error): 53.55 22.98\n",
            "Epoch: [13/290][99/782]\tLR: 0.004061\tTime 0.145 (0.150)\tData 0.000 (0.002)\tLoss 2.1146 (2.6679)\tTop 1-err 51.5625 (64.4844)\tTop 5-err 26.5625 (35.0469)\n",
            "Epoch: [13/290][199/782]\tLR: 0.004061\tTime 0.152 (0.148)\tData 0.000 (0.001)\tLoss 2.0687 (2.7014)\tTop 1-err 56.2500 (64.6602)\tTop 5-err 20.3125 (35.1367)\n",
            "Epoch: [13/290][299/782]\tLR: 0.004061\tTime 0.153 (0.149)\tData 0.000 (0.001)\tLoss 4.0922 (2.7181)\tTop 1-err 92.1875 (64.5729)\tTop 5-err 82.8125 (35.4245)\n",
            "Epoch: [13/290][399/782]\tLR: 0.004061\tTime 0.145 (0.148)\tData 0.000 (0.001)\tLoss 2.7746 (2.7362)\tTop 1-err 56.2500 (65.0332)\tTop 5-err 31.2500 (36.3789)\n",
            "Epoch: [13/290][499/782]\tLR: 0.004061\tTime 0.148 (0.148)\tData 0.000 (0.001)\tLoss 4.2327 (2.7557)\tTop 1-err 89.0625 (65.5938)\tTop 5-err 81.2500 (36.9328)\n",
            "Epoch: [13/290][599/782]\tLR: 0.004061\tTime 0.153 (0.148)\tData 0.000 (0.000)\tLoss 1.7133 (2.7501)\tTop 1-err 43.7500 (65.3372)\tTop 5-err 17.1875 (36.6641)\n",
            "Epoch: [13/290][699/782]\tLR: 0.004061\tTime 0.147 (0.148)\tData 0.000 (0.000)\tLoss 2.1177 (2.7548)\tTop 1-err 59.3750 (65.2444)\tTop 5-err 23.4375 (36.6105)\n",
            "* Epoch: [13/290]\t Top 1-err 65.086  Top 5-err 36.456\t Train Loss 2.750\n",
            "* Epoch: [13/290]\t Top 1-err 51.710  Top 5-err 20.730\t Test Loss 1.910\n",
            "Current best accuracy (top-1 and 5 error): 51.71 20.73\n",
            "Epoch: [14/290][99/782]\tLR: 0.004071\tTime 0.154 (0.149)\tData 0.000 (0.002)\tLoss 5.7247 (2.7004)\tTop 1-err 98.4375 (63.6719)\tTop 5-err 92.1875 (35.4297)\n",
            "Epoch: [14/290][199/782]\tLR: 0.004071\tTime 0.144 (0.148)\tData 0.000 (0.001)\tLoss 1.7777 (2.7262)\tTop 1-err 50.0000 (64.2031)\tTop 5-err 21.8750 (35.4805)\n",
            "Epoch: [14/290][299/782]\tLR: 0.004071\tTime 0.152 (0.148)\tData 0.000 (0.001)\tLoss 1.8977 (2.6793)\tTop 1-err 45.3125 (63.5286)\tTop 5-err 23.4375 (34.8177)\n",
            "Epoch: [14/290][399/782]\tLR: 0.004071\tTime 0.146 (0.148)\tData 0.000 (0.001)\tLoss 4.3030 (2.6710)\tTop 1-err 95.3125 (63.3184)\tTop 5-err 70.3125 (34.6523)\n",
            "Epoch: [14/290][499/782]\tLR: 0.004071\tTime 0.145 (0.148)\tData 0.000 (0.001)\tLoss 2.1633 (2.6665)\tTop 1-err 59.3750 (63.3078)\tTop 5-err 21.8750 (34.5063)\n",
            "Epoch: [14/290][599/782]\tLR: 0.004071\tTime 0.154 (0.148)\tData 0.000 (0.000)\tLoss 3.9949 (2.6928)\tTop 1-err 95.3125 (63.6940)\tTop 5-err 79.6875 (35.2760)\n",
            "Epoch: [14/290][699/782]\tLR: 0.004071\tTime 0.151 (0.148)\tData 0.000 (0.000)\tLoss 1.8102 (2.6848)\tTop 1-err 37.5000 (63.5033)\tTop 5-err 15.6250 (35.0268)\n",
            "* Epoch: [14/290]\t Top 1-err 63.701  Top 5-err 35.244\t Train Loss 2.699\n",
            "* Epoch: [14/290]\t Top 1-err 51.790  Top 5-err 21.040\t Test Loss 1.944\n",
            "Current best accuracy (top-1 and 5 error): 51.71 20.73\n",
            "Epoch: [15/290][99/782]\tLR: 0.004080\tTime 0.142 (0.150)\tData 0.000 (0.002)\tLoss 1.8856 (2.6474)\tTop 1-err 51.5625 (63.0625)\tTop 5-err 15.6250 (36.0234)\n",
            "Epoch: [15/290][199/782]\tLR: 0.004080\tTime 0.146 (0.148)\tData 0.000 (0.001)\tLoss 4.2447 (2.6984)\tTop 1-err 82.8125 (64.0039)\tTop 5-err 65.6250 (36.4062)\n",
            "Epoch: [15/290][299/782]\tLR: 0.004080\tTime 0.157 (0.148)\tData 0.000 (0.001)\tLoss 3.4178 (2.7158)\tTop 1-err 75.0000 (64.3646)\tTop 5-err 48.4375 (36.7344)\n",
            "Epoch: [15/290][399/782]\tLR: 0.004080\tTime 0.146 (0.148)\tData 0.000 (0.001)\tLoss 1.8193 (2.6936)\tTop 1-err 50.0000 (63.8477)\tTop 5-err 15.6250 (36.0020)\n",
            "Epoch: [15/290][499/782]\tLR: 0.004080\tTime 0.145 (0.148)\tData 0.000 (0.001)\tLoss 3.9767 (2.6976)\tTop 1-err 90.6250 (63.9344)\tTop 5-err 71.8750 (36.2859)\n",
            "Epoch: [15/290][599/782]\tLR: 0.004080\tTime 0.145 (0.148)\tData 0.000 (0.000)\tLoss 3.7712 (2.6891)\tTop 1-err 84.3750 (63.8971)\tTop 5-err 64.0625 (36.2357)\n",
            "Epoch: [15/290][699/782]\tLR: 0.004080\tTime 0.144 (0.147)\tData 0.000 (0.000)\tLoss 2.8680 (2.6700)\tTop 1-err 68.7500 (63.5257)\tTop 5-err 42.1875 (35.7076)\n",
            "* Epoch: [15/290]\t Top 1-err 63.635  Top 5-err 35.716\t Train Loss 2.676\n",
            "* Epoch: [15/290]\t Top 1-err 49.320  Top 5-err 19.340\t Test Loss 1.820\n",
            "Current best accuracy (top-1 and 5 error): 49.32 19.34\n",
            "Epoch: [16/290][99/782]\tLR: 0.004091\tTime 0.148 (0.151)\tData 0.000 (0.002)\tLoss 1.9624 (2.7224)\tTop 1-err 54.6875 (63.1562)\tTop 5-err 20.3125 (36.1406)\n",
            "Epoch: [16/290][199/782]\tLR: 0.004091\tTime 0.151 (0.149)\tData 0.000 (0.001)\tLoss 2.0751 (2.7483)\tTop 1-err 50.0000 (63.8281)\tTop 5-err 21.8750 (36.4297)\n",
            "Epoch: [16/290][299/782]\tLR: 0.004091\tTime 0.149 (0.149)\tData 0.000 (0.001)\tLoss 2.1446 (2.7571)\tTop 1-err 57.8125 (64.2135)\tTop 5-err 26.5625 (36.9844)\n",
            "Epoch: [16/290][399/782]\tLR: 0.004091\tTime 0.144 (0.148)\tData 0.000 (0.001)\tLoss 1.9482 (2.7424)\tTop 1-err 56.2500 (64.0781)\tTop 5-err 17.1875 (36.4160)\n",
            "Epoch: [16/290][499/782]\tLR: 0.004091\tTime 0.145 (0.148)\tData 0.000 (0.000)\tLoss 1.8548 (2.7161)\tTop 1-err 53.1250 (63.6531)\tTop 5-err 9.3750 (35.7156)\n",
            "Epoch: [16/290][599/782]\tLR: 0.004091\tTime 0.153 (0.148)\tData 0.000 (0.000)\tLoss 3.9869 (2.6942)\tTop 1-err 90.6250 (63.3581)\tTop 5-err 68.7500 (35.4479)\n",
            "Epoch: [16/290][699/782]\tLR: 0.004091\tTime 0.149 (0.148)\tData 0.000 (0.000)\tLoss 1.9927 (2.6547)\tTop 1-err 51.5625 (62.6150)\tTop 5-err 18.7500 (34.6987)\n",
            "* Epoch: [16/290]\t Top 1-err 62.941  Top 5-err 35.081\t Train Loss 2.670\n",
            "* Epoch: [16/290]\t Top 1-err 50.310  Top 5-err 20.280\t Test Loss 1.880\n",
            "Current best accuracy (top-1 and 5 error): 49.32 19.34\n",
            "Epoch: [17/290][99/782]\tLR: 0.004102\tTime 0.146 (0.151)\tData 0.000 (0.002)\tLoss 3.4789 (2.5552)\tTop 1-err 70.3125 (61.3594)\tTop 5-err 42.1875 (33.2891)\n",
            "Epoch: [17/290][199/782]\tLR: 0.004102\tTime 0.155 (0.150)\tData 0.000 (0.001)\tLoss 3.5343 (2.6558)\tTop 1-err 90.6250 (62.8477)\tTop 5-err 84.3750 (34.7539)\n",
            "Epoch: [17/290][299/782]\tLR: 0.004102\tTime 0.148 (0.150)\tData 0.000 (0.001)\tLoss 1.8108 (2.7091)\tTop 1-err 51.5625 (63.8594)\tTop 5-err 12.5000 (35.9062)\n",
            "Epoch: [17/290][399/782]\tLR: 0.004102\tTime 0.155 (0.150)\tData 0.000 (0.001)\tLoss 2.8599 (2.7038)\tTop 1-err 64.0625 (63.6055)\tTop 5-err 26.5625 (35.8262)\n",
            "Epoch: [17/290][499/782]\tLR: 0.004102\tTime 0.150 (0.150)\tData 0.000 (0.001)\tLoss 3.9272 (2.7082)\tTop 1-err 79.6875 (63.5391)\tTop 5-err 57.8125 (35.7469)\n",
            "Epoch: [17/290][599/782]\tLR: 0.004102\tTime 0.149 (0.149)\tData 0.000 (0.000)\tLoss 1.7292 (2.7089)\tTop 1-err 46.8750 (63.4401)\tTop 5-err 14.0625 (35.4349)\n",
            "Epoch: [17/290][699/782]\tLR: 0.004102\tTime 0.157 (0.149)\tData 0.000 (0.000)\tLoss 4.1069 (2.6722)\tTop 1-err 93.7500 (62.7243)\tTop 5-err 75.0000 (34.6384)\n",
            "* Epoch: [17/290]\t Top 1-err 62.546  Top 5-err 34.395\t Train Loss 2.663\n",
            "* Epoch: [17/290]\t Top 1-err 48.350  Top 5-err 18.910\t Test Loss 1.786\n",
            "Current best accuracy (top-1 and 5 error): 48.35 18.91\n",
            "Epoch: [18/290][99/782]\tLR: 0.004113\tTime 0.148 (0.151)\tData 0.000 (0.002)\tLoss 1.8431 (2.6370)\tTop 1-err 46.8750 (61.9688)\tTop 5-err 20.3125 (33.7500)\n",
            "Epoch: [18/290][199/782]\tLR: 0.004113\tTime 0.145 (0.150)\tData 0.000 (0.001)\tLoss 1.7065 (2.5475)\tTop 1-err 43.7500 (60.7500)\tTop 5-err 18.7500 (32.8672)\n",
            "Epoch: [18/290][299/782]\tLR: 0.004113\tTime 0.144 (0.149)\tData 0.000 (0.001)\tLoss 3.1075 (2.5855)\tTop 1-err 68.7500 (61.3099)\tTop 5-err 32.8125 (33.4505)\n",
            "Epoch: [18/290][399/782]\tLR: 0.004113\tTime 0.144 (0.149)\tData 0.000 (0.001)\tLoss 1.7235 (2.6016)\tTop 1-err 51.5625 (61.6816)\tTop 5-err 12.5000 (33.7910)\n",
            "Epoch: [18/290][499/782]\tLR: 0.004113\tTime 0.145 (0.149)\tData 0.000 (0.000)\tLoss 5.3406 (2.5744)\tTop 1-err 98.4375 (61.0969)\tTop 5-err 93.7500 (33.2031)\n",
            "Epoch: [18/290][599/782]\tLR: 0.004113\tTime 0.151 (0.149)\tData 0.000 (0.000)\tLoss 1.6156 (2.5327)\tTop 1-err 46.8750 (60.4375)\tTop 5-err 14.0625 (32.2656)\n",
            "Epoch: [18/290][699/782]\tLR: 0.004113\tTime 0.144 (0.148)\tData 0.000 (0.000)\tLoss 1.8506 (2.5379)\tTop 1-err 53.1250 (60.3694)\tTop 5-err 18.7500 (32.3281)\n",
            "* Epoch: [18/290]\t Top 1-err 60.682  Top 5-err 32.788\t Train Loss 2.559\n",
            "* Epoch: [18/290]\t Top 1-err 47.840  Top 5-err 17.790\t Test Loss 1.761\n",
            "Current best accuracy (top-1 and 5 error): 47.84 17.79\n",
            "Epoch: [19/290][99/782]\tLR: 0.004125\tTime 0.149 (0.150)\tData 0.000 (0.002)\tLoss 1.6843 (2.5240)\tTop 1-err 48.4375 (59.8594)\tTop 5-err 10.9375 (32.2109)\n",
            "Epoch: [19/290][199/782]\tLR: 0.004125\tTime 0.205 (0.149)\tData 0.000 (0.001)\tLoss 2.0202 (2.5882)\tTop 1-err 57.8125 (60.8555)\tTop 5-err 18.7500 (33.0312)\n",
            "Epoch: [19/290][299/782]\tLR: 0.004125\tTime 0.145 (0.149)\tData 0.000 (0.001)\tLoss 1.7745 (2.5709)\tTop 1-err 54.6875 (60.5000)\tTop 5-err 17.1875 (32.5286)\n",
            "Epoch: [19/290][399/782]\tLR: 0.004125\tTime 0.146 (0.149)\tData 0.000 (0.001)\tLoss 1.7977 (2.5869)\tTop 1-err 51.5625 (60.7012)\tTop 5-err 15.6250 (32.7988)\n",
            "Epoch: [19/290][499/782]\tLR: 0.004125\tTime 0.153 (0.149)\tData 0.000 (0.000)\tLoss 2.5987 (2.5450)\tTop 1-err 54.6875 (59.9000)\tTop 5-err 25.0000 (31.7469)\n",
            "Epoch: [19/290][599/782]\tLR: 0.004125\tTime 0.145 (0.149)\tData 0.000 (0.000)\tLoss 3.9545 (2.5377)\tTop 1-err 87.5000 (59.9193)\tTop 5-err 68.7500 (31.7279)\n",
            "Epoch: [19/290][699/782]\tLR: 0.004125\tTime 0.145 (0.149)\tData 0.000 (0.000)\tLoss 1.8353 (2.5294)\tTop 1-err 50.0000 (59.8036)\tTop 5-err 18.7500 (31.6194)\n",
            "* Epoch: [19/290]\t Top 1-err 60.037  Top 5-err 31.851\t Train Loss 2.540\n",
            "* Epoch: [19/290]\t Top 1-err 46.590  Top 5-err 17.200\t Test Loss 1.699\n",
            "Current best accuracy (top-1 and 5 error): 46.59 17.2\n",
            "Epoch: [20/290][99/782]\tLR: 0.004138\tTime 0.144 (0.150)\tData 0.000 (0.002)\tLoss 1.7546 (2.5426)\tTop 1-err 40.6250 (59.1250)\tTop 5-err 17.1875 (31.9219)\n",
            "Epoch: [20/290][199/782]\tLR: 0.004138\tTime 0.146 (0.148)\tData 0.000 (0.001)\tLoss 3.5939 (2.5449)\tTop 1-err 78.1250 (59.5234)\tTop 5-err 50.0000 (31.6797)\n",
            "Epoch: [20/290][299/782]\tLR: 0.004138\tTime 0.144 (0.148)\tData 0.000 (0.001)\tLoss 2.0106 (2.5013)\tTop 1-err 51.5625 (58.8490)\tTop 5-err 23.4375 (30.9141)\n",
            "Epoch: [20/290][399/782]\tLR: 0.004138\tTime 0.146 (0.148)\tData 0.000 (0.001)\tLoss 3.4211 (2.4713)\tTop 1-err 95.3125 (58.3926)\tTop 5-err 78.1250 (30.3887)\n",
            "Epoch: [20/290][499/782]\tLR: 0.004138\tTime 0.146 (0.148)\tData 0.000 (0.001)\tLoss 1.7195 (2.4797)\tTop 1-err 45.3125 (58.3406)\tTop 5-err 17.1875 (30.5422)\n",
            "Epoch: [20/290][599/782]\tLR: 0.004138\tTime 0.145 (0.148)\tData 0.000 (0.000)\tLoss 2.1515 (2.5017)\tTop 1-err 51.5625 (58.8698)\tTop 5-err 28.1250 (31.0000)\n",
            "Epoch: [20/290][699/782]\tLR: 0.004138\tTime 0.156 (0.148)\tData 0.000 (0.000)\tLoss 3.9041 (2.5232)\tTop 1-err 84.3750 (59.2801)\tTop 5-err 53.1250 (31.5067)\n",
            "* Epoch: [20/290]\t Top 1-err 59.312  Top 5-err 31.514\t Train Loss 2.528\n",
            "* Epoch: [20/290]\t Top 1-err 47.340  Top 5-err 17.850\t Test Loss 1.720\n",
            "Current best accuracy (top-1 and 5 error): 46.59 17.2\n",
            "Epoch: [21/290][99/782]\tLR: 0.004152\tTime 0.147 (0.150)\tData 0.000 (0.002)\tLoss 1.6142 (2.4822)\tTop 1-err 42.1875 (58.7188)\tTop 5-err 9.3750 (30.4297)\n",
            "Epoch: [21/290][199/782]\tLR: 0.004152\tTime 0.153 (0.149)\tData 0.000 (0.001)\tLoss 1.7695 (2.4457)\tTop 1-err 50.0000 (57.8633)\tTop 5-err 15.6250 (30.3125)\n",
            "Epoch: [21/290][299/782]\tLR: 0.004152\tTime 0.154 (0.149)\tData 0.000 (0.001)\tLoss 2.0682 (2.4930)\tTop 1-err 51.5625 (58.7396)\tTop 5-err 32.8125 (31.5391)\n",
            "Epoch: [21/290][399/782]\tLR: 0.004152\tTime 0.144 (0.148)\tData 0.000 (0.001)\tLoss 1.8125 (2.5191)\tTop 1-err 37.5000 (58.9785)\tTop 5-err 26.5625 (31.6973)\n",
            "Epoch: [21/290][499/782]\tLR: 0.004152\tTime 0.145 (0.148)\tData 0.000 (0.000)\tLoss 3.5566 (2.4931)\tTop 1-err 73.4375 (58.5109)\tTop 5-err 45.3125 (31.1406)\n",
            "Epoch: [21/290][599/782]\tLR: 0.004152\tTime 0.150 (0.148)\tData 0.000 (0.000)\tLoss 3.3687 (2.5042)\tTop 1-err 71.8750 (58.7878)\tTop 5-err 35.9375 (31.3828)\n",
            "Epoch: [21/290][699/782]\tLR: 0.004152\tTime 0.144 (0.148)\tData 0.000 (0.000)\tLoss 1.7919 (2.4927)\tTop 1-err 43.7500 (58.6551)\tTop 5-err 18.7500 (31.0938)\n",
            "* Epoch: [21/290]\t Top 1-err 58.798  Top 5-err 31.096\t Train Loss 2.505\n",
            "* Epoch: [21/290]\t Top 1-err 46.540  Top 5-err 17.270\t Test Loss 1.720\n",
            "Current best accuracy (top-1 and 5 error): 46.54 17.27\n",
            "Epoch: [22/290][99/782]\tLR: 0.004166\tTime 0.146 (0.151)\tData 0.000 (0.003)\tLoss 4.1521 (2.5017)\tTop 1-err 100.0000 (59.0391)\tTop 5-err 92.1875 (31.5312)\n",
            "Epoch: [22/290][199/782]\tLR: 0.004166\tTime 0.150 (0.150)\tData 0.000 (0.001)\tLoss 2.3991 (2.4905)\tTop 1-err 53.1250 (58.2188)\tTop 5-err 21.8750 (31.1523)\n",
            "Epoch: [22/290][299/782]\tLR: 0.004166\tTime 0.149 (0.150)\tData 0.000 (0.001)\tLoss 3.3713 (2.4846)\tTop 1-err 67.1875 (58.2266)\tTop 5-err 37.5000 (30.9297)\n",
            "Epoch: [22/290][399/782]\tLR: 0.004166\tTime 0.144 (0.149)\tData 0.000 (0.001)\tLoss 1.9943 (2.4946)\tTop 1-err 50.0000 (58.4121)\tTop 5-err 20.3125 (31.0488)\n",
            "Epoch: [22/290][499/782]\tLR: 0.004166\tTime 0.145 (0.149)\tData 0.000 (0.001)\tLoss 2.4117 (2.4539)\tTop 1-err 57.8125 (57.6000)\tTop 5-err 34.3750 (30.2203)\n",
            "Epoch: [22/290][599/782]\tLR: 0.004166\tTime 0.151 (0.148)\tData 0.000 (0.001)\tLoss 1.9899 (2.4383)\tTop 1-err 51.5625 (57.3763)\tTop 5-err 25.0000 (29.9766)\n",
            "Epoch: [22/290][699/782]\tLR: 0.004166\tTime 0.151 (0.148)\tData 0.000 (0.000)\tLoss 1.8447 (2.4258)\tTop 1-err 56.2500 (57.1931)\tTop 5-err 14.0625 (29.5179)\n",
            "* Epoch: [22/290]\t Top 1-err 57.345  Top 5-err 29.653\t Train Loss 2.438\n",
            "* Epoch: [22/290]\t Top 1-err 44.770  Top 5-err 15.650\t Test Loss 1.613\n",
            "Current best accuracy (top-1 and 5 error): 44.77 15.65\n",
            "Epoch: [23/290][99/782]\tLR: 0.004181\tTime 0.149 (0.151)\tData 0.000 (0.002)\tLoss 1.8157 (2.5060)\tTop 1-err 42.1875 (59.1484)\tTop 5-err 23.4375 (33.0547)\n",
            "Epoch: [23/290][199/782]\tLR: 0.004181\tTime 0.143 (0.150)\tData 0.000 (0.001)\tLoss 3.8449 (2.5070)\tTop 1-err 82.8125 (59.0508)\tTop 5-err 59.3750 (32.2734)\n",
            "Epoch: [23/290][299/782]\tLR: 0.004181\tTime 0.143 (0.149)\tData 0.000 (0.001)\tLoss 3.3404 (2.4605)\tTop 1-err 96.8750 (58.1562)\tTop 5-err 92.1875 (31.1615)\n",
            "Epoch: [23/290][399/782]\tLR: 0.004181\tTime 0.147 (0.148)\tData 0.000 (0.001)\tLoss 3.2041 (2.5141)\tTop 1-err 68.7500 (58.9355)\tTop 5-err 34.3750 (31.7617)\n",
            "Epoch: [23/290][499/782]\tLR: 0.004181\tTime 0.149 (0.148)\tData 0.000 (0.000)\tLoss 1.7235 (2.4918)\tTop 1-err 51.5625 (58.5797)\tTop 5-err 14.0625 (31.3938)\n",
            "Epoch: [23/290][599/782]\tLR: 0.004181\tTime 0.149 (0.148)\tData 0.000 (0.000)\tLoss 1.8492 (2.4759)\tTop 1-err 50.0000 (58.3047)\tTop 5-err 18.7500 (30.8555)\n",
            "Epoch: [23/290][699/782]\tLR: 0.004181\tTime 0.147 (0.148)\tData 0.000 (0.000)\tLoss 4.0670 (2.4819)\tTop 1-err 90.6250 (58.3237)\tTop 5-err 71.8750 (30.9621)\n",
            "* Epoch: [23/290]\t Top 1-err 58.426  Top 5-err 31.062\t Train Loss 2.488\n",
            "* Epoch: [23/290]\t Top 1-err 47.620  Top 5-err 18.280\t Test Loss 1.758\n",
            "Current best accuracy (top-1 and 5 error): 44.77 15.65\n",
            "Epoch: [24/290][99/782]\tLR: 0.004196\tTime 0.148 (0.150)\tData 0.000 (0.002)\tLoss 2.4863 (2.3527)\tTop 1-err 60.9375 (55.6797)\tTop 5-err 23.4375 (27.7344)\n",
            "Epoch: [24/290][199/782]\tLR: 0.004196\tTime 0.145 (0.149)\tData 0.000 (0.001)\tLoss 1.6155 (2.4490)\tTop 1-err 42.1875 (57.1523)\tTop 5-err 14.0625 (29.8984)\n",
            "Epoch: [24/290][299/782]\tLR: 0.004196\tTime 0.144 (0.148)\tData 0.000 (0.001)\tLoss 1.5885 (2.4387)\tTop 1-err 43.7500 (57.2578)\tTop 5-err 12.5000 (30.1589)\n",
            "Epoch: [24/290][399/782]\tLR: 0.004196\tTime 0.144 (0.148)\tData 0.000 (0.000)\tLoss 3.7503 (2.4331)\tTop 1-err 81.2500 (57.2031)\tTop 5-err 46.8750 (30.0488)\n",
            "Epoch: [24/290][499/782]\tLR: 0.004196\tTime 0.151 (0.148)\tData 0.000 (0.000)\tLoss 1.9054 (2.4386)\tTop 1-err 37.5000 (57.2172)\tTop 5-err 12.5000 (29.8844)\n",
            "Epoch: [24/290][599/782]\tLR: 0.004196\tTime 0.145 (0.148)\tData 0.000 (0.000)\tLoss 1.3449 (2.4408)\tTop 1-err 35.9375 (57.0703)\tTop 5-err 12.5000 (29.7240)\n",
            "Epoch: [24/290][699/782]\tLR: 0.004196\tTime 0.151 (0.148)\tData 0.000 (0.000)\tLoss 1.4799 (2.4258)\tTop 1-err 42.1875 (56.7277)\tTop 5-err 6.2500 (29.2433)\n",
            "* Epoch: [24/290]\t Top 1-err 56.694  Top 5-err 29.181\t Train Loss 2.422\n",
            "* Epoch: [24/290]\t Top 1-err 44.080  Top 5-err 15.410\t Test Loss 1.583\n",
            "Current best accuracy (top-1 and 5 error): 44.08 15.41\n",
            "Epoch: [25/290][99/782]\tLR: 0.004212\tTime 0.144 (0.150)\tData 0.000 (0.002)\tLoss 1.4457 (2.5523)\tTop 1-err 40.6250 (59.4375)\tTop 5-err 10.9375 (32.6719)\n",
            "Epoch: [25/290][199/782]\tLR: 0.004212\tTime 0.151 (0.149)\tData 0.000 (0.001)\tLoss 1.7114 (2.4391)\tTop 1-err 45.3125 (57.1172)\tTop 5-err 18.7500 (29.8945)\n",
            "Epoch: [25/290][299/782]\tLR: 0.004212\tTime 0.145 (0.149)\tData 0.000 (0.001)\tLoss 3.9172 (2.4024)\tTop 1-err 85.9375 (56.3698)\tTop 5-err 70.3125 (28.8802)\n",
            "Epoch: [25/290][399/782]\tLR: 0.004212\tTime 0.145 (0.148)\tData 0.000 (0.001)\tLoss 4.0402 (2.3975)\tTop 1-err 85.9375 (56.1738)\tTop 5-err 57.8125 (28.6855)\n",
            "Epoch: [25/290][499/782]\tLR: 0.004212\tTime 0.149 (0.148)\tData 0.000 (0.001)\tLoss 1.6235 (2.4182)\tTop 1-err 45.3125 (56.4250)\tTop 5-err 15.6250 (29.1625)\n",
            "Epoch: [25/290][599/782]\tLR: 0.004212\tTime 0.144 (0.148)\tData 0.000 (0.000)\tLoss 1.3904 (2.3989)\tTop 1-err 39.0625 (56.0625)\tTop 5-err 12.5000 (28.7135)\n",
            "Epoch: [25/290][699/782]\tLR: 0.004212\tTime 0.151 (0.148)\tData 0.000 (0.000)\tLoss 1.7601 (2.4181)\tTop 1-err 50.0000 (56.3627)\tTop 5-err 17.1875 (29.0971)\n",
            "* Epoch: [25/290]\t Top 1-err 56.333  Top 5-err 28.997\t Train Loss 2.416\n",
            "* Epoch: [25/290]\t Top 1-err 43.470  Top 5-err 15.130\t Test Loss 1.574\n",
            "Current best accuracy (top-1 and 5 error): 43.47 15.13\n",
            "Epoch: [26/290][99/782]\tLR: 0.004228\tTime 0.144 (0.150)\tData 0.000 (0.002)\tLoss 1.7701 (2.3714)\tTop 1-err 53.1250 (55.6328)\tTop 5-err 12.5000 (28.4453)\n",
            "Epoch: [26/290][199/782]\tLR: 0.004228\tTime 0.146 (0.148)\tData 0.000 (0.001)\tLoss 2.9278 (2.4349)\tTop 1-err 64.0625 (56.7773)\tTop 5-err 40.6250 (29.5469)\n",
            "Epoch: [26/290][299/782]\tLR: 0.004228\tTime 0.145 (0.148)\tData 0.000 (0.001)\tLoss 1.6933 (2.4646)\tTop 1-err 43.7500 (57.9036)\tTop 5-err 17.1875 (31.2943)\n",
            "Epoch: [26/290][399/782]\tLR: 0.004228\tTime 0.145 (0.148)\tData 0.000 (0.001)\tLoss 3.1152 (2.4539)\tTop 1-err 70.3125 (57.7812)\tTop 5-err 35.9375 (30.9648)\n",
            "Epoch: [26/290][499/782]\tLR: 0.004228\tTime 0.145 (0.148)\tData 0.000 (0.000)\tLoss 1.9378 (2.4567)\tTop 1-err 54.6875 (57.7219)\tTop 5-err 18.7500 (30.7922)\n",
            "Epoch: [26/290][599/782]\tLR: 0.004228\tTime 0.160 (0.148)\tData 0.000 (0.000)\tLoss 1.6568 (2.4565)\tTop 1-err 45.3125 (57.7630)\tTop 5-err 14.0625 (30.8854)\n",
            "Epoch: [26/290][699/782]\tLR: 0.004228\tTime 0.153 (0.148)\tData 0.000 (0.000)\tLoss 1.5638 (2.4457)\tTop 1-err 42.1875 (57.5212)\tTop 5-err 15.6250 (30.5335)\n",
            "* Epoch: [26/290]\t Top 1-err 57.452  Top 5-err 30.483\t Train Loss 2.446\n",
            "* Epoch: [26/290]\t Top 1-err 42.950  Top 5-err 14.710\t Test Loss 1.545\n",
            "Current best accuracy (top-1 and 5 error): 42.95 14.71\n",
            "Epoch: [27/290][99/782]\tLR: 0.004246\tTime 0.153 (0.151)\tData 0.000 (0.002)\tLoss 1.5399 (2.3719)\tTop 1-err 39.0625 (55.0078)\tTop 5-err 15.6250 (28.7266)\n",
            "Epoch: [27/290][199/782]\tLR: 0.004246\tTime 0.151 (0.149)\tData 0.000 (0.001)\tLoss 2.7488 (2.3494)\tTop 1-err 57.8125 (54.4102)\tTop 5-err 23.4375 (27.1289)\n",
            "Epoch: [27/290][299/782]\tLR: 0.004246\tTime 0.145 (0.149)\tData 0.000 (0.001)\tLoss 3.0602 (2.3663)\tTop 1-err 75.0000 (54.9844)\tTop 5-err 45.3125 (27.7161)\n",
            "Epoch: [27/290][399/782]\tLR: 0.004246\tTime 0.152 (0.148)\tData 0.000 (0.001)\tLoss 1.6678 (2.4063)\tTop 1-err 45.3125 (55.8750)\tTop 5-err 12.5000 (28.5742)\n",
            "Epoch: [27/290][499/782]\tLR: 0.004246\tTime 0.146 (0.148)\tData 0.000 (0.001)\tLoss 4.6680 (2.3913)\tTop 1-err 96.8750 (55.5828)\tTop 5-err 85.9375 (28.5031)\n",
            "Epoch: [27/290][599/782]\tLR: 0.004246\tTime 0.145 (0.148)\tData 0.000 (0.000)\tLoss 1.3911 (2.3990)\tTop 1-err 37.5000 (55.8711)\tTop 5-err 14.0625 (28.8568)\n",
            "Epoch: [27/290][699/782]\tLR: 0.004246\tTime 0.144 (0.148)\tData 0.000 (0.000)\tLoss 1.8965 (2.3878)\tTop 1-err 46.8750 (55.6752)\tTop 5-err 23.4375 (28.6116)\n",
            "* Epoch: [27/290]\t Top 1-err 56.115  Top 5-err 29.070\t Train Loss 2.410\n",
            "* Epoch: [27/290]\t Top 1-err 43.220  Top 5-err 15.190\t Test Loss 1.582\n",
            "Current best accuracy (top-1 and 5 error): 42.95 14.71\n",
            "Epoch: [28/290][99/782]\tLR: 0.004264\tTime 0.145 (0.150)\tData 0.000 (0.002)\tLoss 3.3928 (2.4176)\tTop 1-err 79.6875 (55.4922)\tTop 5-err 45.3125 (28.8438)\n",
            "Epoch: [28/290][199/782]\tLR: 0.004264\tTime 0.141 (0.149)\tData 0.000 (0.001)\tLoss 1.5666 (2.3513)\tTop 1-err 43.7500 (54.8789)\tTop 5-err 15.6250 (27.8555)\n",
            "Epoch: [28/290][299/782]\tLR: 0.004264\tTime 0.154 (0.148)\tData 0.000 (0.001)\tLoss 3.8562 (2.3654)\tTop 1-err 79.6875 (55.3672)\tTop 5-err 68.7500 (28.2839)\n",
            "Epoch: [28/290][399/782]\tLR: 0.004264\tTime 0.152 (0.148)\tData 0.000 (0.001)\tLoss 1.6428 (2.3877)\tTop 1-err 46.8750 (55.9199)\tTop 5-err 15.6250 (29.1250)\n",
            "Epoch: [28/290][499/782]\tLR: 0.004264\tTime 0.154 (0.148)\tData 0.000 (0.001)\tLoss 3.7039 (2.3732)\tTop 1-err 68.7500 (55.6063)\tTop 5-err 40.6250 (28.8031)\n",
            "Epoch: [28/290][599/782]\tLR: 0.004264\tTime 0.147 (0.149)\tData 0.000 (0.000)\tLoss 1.5333 (2.3557)\tTop 1-err 45.3125 (55.2435)\tTop 5-err 15.6250 (28.3190)\n",
            "Epoch: [28/290][699/782]\tLR: 0.004264\tTime 0.152 (0.149)\tData 0.000 (0.000)\tLoss 1.7588 (2.3467)\tTop 1-err 48.4375 (55.0391)\tTop 5-err 15.6250 (28.1518)\n",
            "* Epoch: [28/290]\t Top 1-err 55.108  Top 5-err 28.136\t Train Loss 2.353\n",
            "* Epoch: [28/290]\t Top 1-err 43.770  Top 5-err 15.150\t Test Loss 1.598\n",
            "Current best accuracy (top-1 and 5 error): 42.95 14.71\n",
            "Epoch: [29/290][99/782]\tLR: 0.004282\tTime 0.144 (0.149)\tData 0.000 (0.002)\tLoss 1.4148 (2.2729)\tTop 1-err 31.2500 (53.1875)\tTop 5-err 10.9375 (26.7422)\n",
            "Epoch: [29/290][199/782]\tLR: 0.004282\tTime 0.150 (0.149)\tData 0.000 (0.001)\tLoss 3.8398 (2.3017)\tTop 1-err 87.5000 (53.7617)\tTop 5-err 62.5000 (26.8320)\n",
            "Epoch: [29/290][299/782]\tLR: 0.004282\tTime 0.151 (0.149)\tData 0.000 (0.001)\tLoss 1.7720 (2.3571)\tTop 1-err 57.8125 (54.7135)\tTop 5-err 15.6250 (27.6615)\n",
            "Epoch: [29/290][399/782]\tLR: 0.004282\tTime 0.148 (0.149)\tData 0.000 (0.001)\tLoss 2.3362 (2.3611)\tTop 1-err 46.8750 (54.7363)\tTop 5-err 23.4375 (27.7383)\n",
            "Epoch: [29/290][499/782]\tLR: 0.004282\tTime 0.156 (0.149)\tData 0.000 (0.001)\tLoss 4.5991 (2.3669)\tTop 1-err 95.3125 (55.1094)\tTop 5-err 84.3750 (28.1562)\n",
            "Epoch: [29/290][599/782]\tLR: 0.004282\tTime 0.146 (0.149)\tData 0.000 (0.000)\tLoss 3.8835 (2.3629)\tTop 1-err 82.8125 (55.1172)\tTop 5-err 65.6250 (28.2292)\n",
            "Epoch: [29/290][699/782]\tLR: 0.004282\tTime 0.146 (0.149)\tData 0.000 (0.000)\tLoss 3.7876 (2.3493)\tTop 1-err 92.1875 (54.9330)\tTop 5-err 62.5000 (28.0357)\n",
            "* Epoch: [29/290]\t Top 1-err 54.880  Top 5-err 27.884\t Train Loss 2.345\n",
            "* Epoch: [29/290]\t Top 1-err 42.570  Top 5-err 14.330\t Test Loss 1.519\n",
            "Current best accuracy (top-1 and 5 error): 42.57 14.33\n",
            "Epoch: [30/290][99/782]\tLR: 0.004301\tTime 0.157 (0.151)\tData 0.000 (0.002)\tLoss 1.4972 (2.3230)\tTop 1-err 46.8750 (54.1250)\tTop 5-err 10.9375 (26.9531)\n",
            "Epoch: [30/290][199/782]\tLR: 0.004301\tTime 0.146 (0.150)\tData 0.000 (0.001)\tLoss 3.0622 (2.2719)\tTop 1-err 53.1250 (53.1523)\tTop 5-err 17.1875 (25.6758)\n",
            "Epoch: [30/290][299/782]\tLR: 0.004301\tTime 0.153 (0.149)\tData 0.000 (0.001)\tLoss 3.2955 (2.2668)\tTop 1-err 75.0000 (53.0391)\tTop 5-err 43.7500 (25.7214)\n",
            "Epoch: [30/290][399/782]\tLR: 0.004301\tTime 0.146 (0.149)\tData 0.000 (0.001)\tLoss 2.8762 (2.2577)\tTop 1-err 48.4375 (53.2031)\tTop 5-err 21.8750 (25.9258)\n",
            "Epoch: [30/290][499/782]\tLR: 0.004301\tTime 0.145 (0.149)\tData 0.000 (0.000)\tLoss 3.5641 (2.2971)\tTop 1-err 81.2500 (53.8750)\tTop 5-err 45.3125 (26.7953)\n",
            "Epoch: [30/290][599/782]\tLR: 0.004301\tTime 0.149 (0.149)\tData 0.000 (0.000)\tLoss 3.1002 (2.2940)\tTop 1-err 73.4375 (53.9583)\tTop 5-err 28.1250 (27.0286)\n",
            "Epoch: [30/290][699/782]\tLR: 0.004301\tTime 0.150 (0.149)\tData 0.000 (0.000)\tLoss 2.0332 (2.3011)\tTop 1-err 59.3750 (54.0960)\tTop 5-err 25.0000 (27.3538)\n",
            "* Epoch: [30/290]\t Top 1-err 53.996  Top 5-err 27.279\t Train Loss 2.305\n",
            "* Epoch: [30/290]\t Top 1-err 42.370  Top 5-err 14.880\t Test Loss 1.561\n",
            "Current best accuracy (top-1 and 5 error): 42.37 14.88\n",
            "Epoch: [31/290][99/782]\tLR: 0.004321\tTime 0.145 (0.150)\tData 0.000 (0.002)\tLoss 1.5110 (2.2889)\tTop 1-err 39.0625 (53.6562)\tTop 5-err 15.6250 (27.1406)\n",
            "Epoch: [31/290][199/782]\tLR: 0.004321\tTime 0.148 (0.149)\tData 0.000 (0.001)\tLoss 1.5346 (2.3464)\tTop 1-err 43.7500 (54.8086)\tTop 5-err 14.0625 (28.2031)\n",
            "Epoch: [31/290][299/782]\tLR: 0.004321\tTime 0.149 (0.149)\tData 0.000 (0.001)\tLoss 3.2558 (2.2862)\tTop 1-err 68.7500 (53.4714)\tTop 5-err 29.6875 (26.5339)\n",
            "Epoch: [31/290][399/782]\tLR: 0.004321\tTime 0.148 (0.149)\tData 0.000 (0.000)\tLoss 1.9056 (2.2858)\tTop 1-err 43.7500 (53.6074)\tTop 5-err 18.7500 (26.7188)\n",
            "Epoch: [31/290][499/782]\tLR: 0.004321\tTime 0.153 (0.149)\tData 0.000 (0.000)\tLoss 3.5406 (2.3344)\tTop 1-err 59.3750 (54.4953)\tTop 5-err 28.1250 (27.6719)\n",
            "Epoch: [31/290][599/782]\tLR: 0.004321\tTime 0.146 (0.149)\tData 0.000 (0.000)\tLoss 4.3555 (2.3378)\tTop 1-err 100.0000 (54.6849)\tTop 5-err 87.5000 (27.8255)\n",
            "Epoch: [31/290][699/782]\tLR: 0.004321\tTime 0.145 (0.149)\tData 0.000 (0.000)\tLoss 3.0529 (2.3446)\tTop 1-err 67.1875 (54.8996)\tTop 5-err 35.9375 (28.0324)\n",
            "* Epoch: [31/290]\t Top 1-err 54.939  Top 5-err 28.064\t Train Loss 2.350\n",
            "* Epoch: [31/290]\t Top 1-err 40.010  Top 5-err 13.120\t Test Loss 1.446\n",
            "Current best accuracy (top-1 and 5 error): 40.01 13.12\n",
            "Epoch: [32/290][99/782]\tLR: 0.004341\tTime 0.145 (0.151)\tData 0.000 (0.002)\tLoss 3.2289 (2.4158)\tTop 1-err 67.1875 (55.5781)\tTop 5-err 37.5000 (28.9062)\n",
            "Epoch: [32/290][199/782]\tLR: 0.004341\tTime 0.141 (0.150)\tData 0.000 (0.001)\tLoss 1.4857 (2.3954)\tTop 1-err 40.6250 (55.1602)\tTop 5-err 12.5000 (28.4570)\n",
            "Epoch: [32/290][299/782]\tLR: 0.004341\tTime 0.146 (0.149)\tData 0.000 (0.001)\tLoss 4.8303 (2.3409)\tTop 1-err 95.3125 (54.5677)\tTop 5-err 89.0625 (27.6224)\n",
            "Epoch: [32/290][399/782]\tLR: 0.004341\tTime 0.161 (0.149)\tData 0.000 (0.001)\tLoss 2.6477 (2.3058)\tTop 1-err 51.5625 (54.0078)\tTop 5-err 17.1875 (27.1133)\n",
            "Epoch: [32/290][499/782]\tLR: 0.004341\tTime 0.145 (0.148)\tData 0.000 (0.000)\tLoss 1.4493 (2.3087)\tTop 1-err 34.3750 (53.7516)\tTop 5-err 4.6875 (26.9141)\n",
            "Epoch: [32/290][599/782]\tLR: 0.004341\tTime 0.145 (0.148)\tData 0.000 (0.000)\tLoss 1.6656 (2.3502)\tTop 1-err 53.1250 (54.6419)\tTop 5-err 14.0625 (27.7604)\n",
            "Epoch: [32/290][699/782]\tLR: 0.004341\tTime 0.154 (0.148)\tData 0.000 (0.000)\tLoss 1.6706 (2.3563)\tTop 1-err 45.3125 (54.8549)\tTop 5-err 18.7500 (28.0011)\n",
            "* Epoch: [32/290]\t Top 1-err 54.611  Top 5-err 27.791\t Train Loss 2.345\n",
            "* Epoch: [32/290]\t Top 1-err 41.230  Top 5-err 13.850\t Test Loss 1.504\n",
            "Current best accuracy (top-1 and 5 error): 40.01 13.12\n",
            "Epoch: [33/290][99/782]\tLR: 0.004362\tTime 0.145 (0.151)\tData 0.000 (0.002)\tLoss 2.0005 (2.2326)\tTop 1-err 37.5000 (52.6094)\tTop 5-err 12.5000 (25.7188)\n",
            "Epoch: [33/290][199/782]\tLR: 0.004362\tTime 0.145 (0.150)\tData 0.000 (0.001)\tLoss 3.2192 (2.3024)\tTop 1-err 62.5000 (53.8086)\tTop 5-err 31.2500 (26.4297)\n",
            "Epoch: [33/290][299/782]\tLR: 0.004362\tTime 0.143 (0.150)\tData 0.000 (0.001)\tLoss 1.3518 (2.2873)\tTop 1-err 29.6875 (53.2188)\tTop 5-err 6.2500 (26.0156)\n",
            "Epoch: [33/290][399/782]\tLR: 0.004362\tTime 0.146 (0.149)\tData 0.000 (0.001)\tLoss 3.4727 (2.3304)\tTop 1-err 79.6875 (54.1680)\tTop 5-err 50.0000 (27.0566)\n",
            "Epoch: [33/290][499/782]\tLR: 0.004362\tTime 0.147 (0.149)\tData 0.000 (0.001)\tLoss 3.5553 (2.3317)\tTop 1-err 65.6250 (54.0594)\tTop 5-err 26.5625 (26.8797)\n",
            "Epoch: [33/290][599/782]\tLR: 0.004362\tTime 0.153 (0.149)\tData 0.000 (0.000)\tLoss 2.7085 (2.3364)\tTop 1-err 62.5000 (54.2266)\tTop 5-err 28.1250 (27.0742)\n",
            "Epoch: [33/290][699/782]\tLR: 0.004362\tTime 0.144 (0.149)\tData 0.000 (0.000)\tLoss 1.2188 (2.3490)\tTop 1-err 35.9375 (54.5100)\tTop 5-err 4.6875 (27.5167)\n",
            "* Epoch: [33/290]\t Top 1-err 54.606  Top 5-err 27.753\t Train Loss 2.345\n",
            "* Epoch: [33/290]\t Top 1-err 41.560  Top 5-err 14.080\t Test Loss 1.531\n",
            "Current best accuracy (top-1 and 5 error): 40.01 13.12\n",
            "Epoch: [34/290][99/782]\tLR: 0.004384\tTime 0.145 (0.150)\tData 0.000 (0.002)\tLoss 2.7125 (2.2696)\tTop 1-err 54.6875 (52.7266)\tTop 5-err 21.8750 (26.1250)\n",
            "Epoch: [34/290][199/782]\tLR: 0.004384\tTime 0.144 (0.150)\tData 0.000 (0.001)\tLoss 1.4713 (2.2328)\tTop 1-err 35.9375 (51.7109)\tTop 5-err 14.0625 (25.0117)\n",
            "Epoch: [34/290][299/782]\tLR: 0.004384\tTime 0.146 (0.149)\tData 0.000 (0.001)\tLoss 3.5137 (2.2713)\tTop 1-err 79.6875 (52.5365)\tTop 5-err 51.5625 (25.8255)\n",
            "Epoch: [34/290][399/782]\tLR: 0.004384\tTime 0.149 (0.148)\tData 0.000 (0.001)\tLoss 1.3614 (2.2609)\tTop 1-err 46.8750 (52.7344)\tTop 5-err 12.5000 (25.9688)\n",
            "Epoch: [34/290][499/782]\tLR: 0.004384\tTime 0.152 (0.148)\tData 0.000 (0.001)\tLoss 3.9425 (2.3095)\tTop 1-err 85.9375 (53.8391)\tTop 5-err 64.0625 (27.2812)\n",
            "Epoch: [34/290][599/782]\tLR: 0.004384\tTime 0.146 (0.148)\tData 0.000 (0.000)\tLoss 3.6623 (2.3033)\tTop 1-err 95.3125 (53.6992)\tTop 5-err 81.2500 (27.0247)\n",
            "Epoch: [34/290][699/782]\tLR: 0.004384\tTime 0.145 (0.148)\tData 0.000 (0.000)\tLoss 3.3324 (2.2900)\tTop 1-err 71.8750 (53.4408)\tTop 5-err 43.7500 (26.7355)\n",
            "* Epoch: [34/290]\t Top 1-err 53.359  Top 5-err 26.539\t Train Loss 2.285\n",
            "* Epoch: [34/290]\t Top 1-err 41.080  Top 5-err 13.450\t Test Loss 1.464\n",
            "Current best accuracy (top-1 and 5 error): 40.01 13.12\n",
            "Epoch: [35/290][99/782]\tLR: 0.004406\tTime 0.145 (0.152)\tData 0.000 (0.002)\tLoss 1.3893 (2.0774)\tTop 1-err 34.3750 (49.0547)\tTop 5-err 14.0625 (22.8516)\n",
            "Epoch: [35/290][199/782]\tLR: 0.004406\tTime 0.150 (0.150)\tData 0.000 (0.001)\tLoss 1.0274 (2.2697)\tTop 1-err 34.3750 (52.4375)\tTop 5-err 9.3750 (26.0234)\n",
            "Epoch: [35/290][299/782]\tLR: 0.004406\tTime 0.145 (0.149)\tData 0.000 (0.001)\tLoss 4.5832 (2.2317)\tTop 1-err 96.8750 (52.1250)\tTop 5-err 81.2500 (25.6406)\n",
            "Epoch: [35/290][399/782]\tLR: 0.004406\tTime 0.153 (0.149)\tData 0.000 (0.001)\tLoss 1.5240 (2.2628)\tTop 1-err 39.0625 (52.7559)\tTop 5-err 15.6250 (26.4902)\n",
            "Epoch: [35/290][499/782]\tLR: 0.004406\tTime 0.145 (0.148)\tData 0.000 (0.001)\tLoss 1.4563 (2.2713)\tTop 1-err 34.3750 (52.9016)\tTop 5-err 6.2500 (26.4672)\n",
            "Epoch: [35/290][599/782]\tLR: 0.004406\tTime 0.152 (0.148)\tData 0.000 (0.000)\tLoss 1.6134 (2.2768)\tTop 1-err 37.5000 (53.0729)\tTop 5-err 20.3125 (26.5938)\n",
            "Epoch: [35/290][699/782]\tLR: 0.004406\tTime 0.144 (0.148)\tData 0.000 (0.000)\tLoss 1.9066 (2.2900)\tTop 1-err 50.0000 (53.3527)\tTop 5-err 18.7500 (26.9018)\n",
            "* Epoch: [35/290]\t Top 1-err 53.450  Top 5-err 27.027\t Train Loss 2.298\n",
            "* Epoch: [35/290]\t Top 1-err 41.620  Top 5-err 14.340\t Test Loss 1.536\n",
            "Current best accuracy (top-1 and 5 error): 40.01 13.12\n",
            "Epoch: [36/290][99/782]\tLR: 0.004429\tTime 0.147 (0.151)\tData 0.000 (0.002)\tLoss 3.7058 (2.3621)\tTop 1-err 85.9375 (54.1094)\tTop 5-err 51.5625 (27.9844)\n",
            "Epoch: [36/290][199/782]\tLR: 0.004429\tTime 0.149 (0.149)\tData 0.000 (0.001)\tLoss 1.3218 (2.2904)\tTop 1-err 37.5000 (52.8438)\tTop 5-err 7.8125 (26.7031)\n",
            "Epoch: [36/290][299/782]\tLR: 0.004429\tTime 0.144 (0.149)\tData 0.000 (0.001)\tLoss 4.2720 (2.2737)\tTop 1-err 87.5000 (52.8932)\tTop 5-err 64.0625 (26.7812)\n",
            "Epoch: [36/290][399/782]\tLR: 0.004429\tTime 0.143 (0.149)\tData 0.000 (0.001)\tLoss 1.4587 (2.2841)\tTop 1-err 35.9375 (53.0762)\tTop 5-err 17.1875 (27.0820)\n",
            "Epoch: [36/290][499/782]\tLR: 0.004429\tTime 0.144 (0.148)\tData 0.000 (0.001)\tLoss 1.4471 (2.2463)\tTop 1-err 45.3125 (52.3813)\tTop 5-err 10.9375 (26.1687)\n",
            "Epoch: [36/290][599/782]\tLR: 0.004429\tTime 0.149 (0.148)\tData 0.000 (0.000)\tLoss 1.9742 (2.2096)\tTop 1-err 56.2500 (51.7396)\tTop 5-err 21.8750 (25.4922)\n",
            "Epoch: [36/290][699/782]\tLR: 0.004429\tTime 0.149 (0.148)\tData 0.000 (0.000)\tLoss 4.1931 (2.1966)\tTop 1-err 84.3750 (51.3136)\tTop 5-err 64.0625 (24.9933)\n",
            "* Epoch: [36/290]\t Top 1-err 51.526  Top 5-err 25.299\t Train Loss 2.203\n",
            "* Epoch: [36/290]\t Top 1-err 39.840  Top 5-err 12.970\t Test Loss 1.417\n",
            "Current best accuracy (top-1 and 5 error): 39.84 12.97\n",
            "Epoch: [37/290][99/782]\tLR: 0.004452\tTime 0.157 (0.151)\tData 0.000 (0.002)\tLoss 1.5658 (2.2992)\tTop 1-err 50.0000 (53.3906)\tTop 5-err 10.9375 (27.9922)\n",
            "Epoch: [37/290][199/782]\tLR: 0.004452\tTime 0.150 (0.150)\tData 0.000 (0.001)\tLoss 1.6146 (2.2728)\tTop 1-err 45.3125 (52.7188)\tTop 5-err 17.1875 (26.2891)\n",
            "Epoch: [37/290][299/782]\tLR: 0.004452\tTime 0.145 (0.149)\tData 0.000 (0.001)\tLoss 3.3717 (2.2146)\tTop 1-err 95.3125 (51.3880)\tTop 5-err 84.3750 (25.3333)\n",
            "Epoch: [37/290][399/782]\tLR: 0.004452\tTime 0.145 (0.148)\tData 0.000 (0.001)\tLoss 1.3892 (2.1906)\tTop 1-err 32.8125 (50.9355)\tTop 5-err 12.5000 (24.9219)\n",
            "Epoch: [37/290][499/782]\tLR: 0.004452\tTime 0.152 (0.148)\tData 0.000 (0.001)\tLoss 1.5792 (2.2044)\tTop 1-err 42.1875 (51.1984)\tTop 5-err 12.5000 (25.1313)\n",
            "Epoch: [37/290][599/782]\tLR: 0.004452\tTime 0.143 (0.148)\tData 0.000 (0.000)\tLoss 1.5414 (2.2027)\tTop 1-err 39.0625 (51.2721)\tTop 5-err 20.3125 (25.2487)\n",
            "Epoch: [37/290][699/782]\tLR: 0.004452\tTime 0.151 (0.148)\tData 0.000 (0.000)\tLoss 4.3393 (2.2087)\tTop 1-err 92.1875 (51.5234)\tTop 5-err 75.0000 (25.2857)\n",
            "* Epoch: [37/290]\t Top 1-err 51.933  Top 5-err 25.662\t Train Loss 2.223\n",
            "* Epoch: [37/290]\t Top 1-err 40.420  Top 5-err 13.030\t Test Loss 1.466\n",
            "Current best accuracy (top-1 and 5 error): 39.84 12.97\n",
            "Epoch: [38/290][99/782]\tLR: 0.004476\tTime 0.145 (0.151)\tData 0.000 (0.002)\tLoss 1.3836 (2.3264)\tTop 1-err 43.7500 (53.0625)\tTop 5-err 12.5000 (26.5547)\n",
            "Epoch: [38/290][199/782]\tLR: 0.004476\tTime 0.148 (0.150)\tData 0.000 (0.001)\tLoss 3.0344 (2.2947)\tTop 1-err 81.2500 (52.9922)\tTop 5-err 43.7500 (26.9844)\n",
            "Epoch: [38/290][299/782]\tLR: 0.004476\tTime 0.152 (0.149)\tData 0.000 (0.001)\tLoss 2.8127 (2.2431)\tTop 1-err 95.3125 (52.0234)\tTop 5-err 87.5000 (26.0208)\n",
            "Epoch: [38/290][399/782]\tLR: 0.004476\tTime 0.152 (0.149)\tData 0.000 (0.001)\tLoss 1.3085 (2.1960)\tTop 1-err 35.9375 (51.3711)\tTop 5-err 9.3750 (25.3594)\n",
            "Epoch: [38/290][499/782]\tLR: 0.004476\tTime 0.144 (0.148)\tData 0.000 (0.001)\tLoss 3.3501 (2.2266)\tTop 1-err 76.5625 (51.8172)\tTop 5-err 37.5000 (25.6469)\n",
            "Epoch: [38/290][599/782]\tLR: 0.004476\tTime 0.165 (0.148)\tData 0.000 (0.000)\tLoss 1.5421 (2.2297)\tTop 1-err 39.0625 (51.8789)\tTop 5-err 12.5000 (25.6445)\n",
            "Epoch: [38/290][699/782]\tLR: 0.004476\tTime 0.154 (0.148)\tData 0.000 (0.000)\tLoss 4.6265 (2.2225)\tTop 1-err 96.8750 (51.6819)\tTop 5-err 84.3750 (25.4710)\n",
            "* Epoch: [38/290]\t Top 1-err 51.884  Top 5-err 25.586\t Train Loss 2.228\n",
            "* Epoch: [38/290]\t Top 1-err 39.430  Top 5-err 12.640\t Test Loss 1.416\n",
            "Current best accuracy (top-1 and 5 error): 39.43 12.64\n",
            "Epoch: [39/290][99/782]\tLR: 0.004501\tTime 0.144 (0.151)\tData 0.000 (0.002)\tLoss 3.8503 (2.2098)\tTop 1-err 78.1250 (51.9609)\tTop 5-err 57.8125 (26.1094)\n",
            "Epoch: [39/290][199/782]\tLR: 0.004501\tTime 0.146 (0.150)\tData 0.000 (0.001)\tLoss 1.3172 (2.1971)\tTop 1-err 35.9375 (51.8438)\tTop 5-err 12.5000 (25.9844)\n",
            "Epoch: [39/290][299/782]\tLR: 0.004501\tTime 0.145 (0.150)\tData 0.000 (0.001)\tLoss 1.4996 (2.1609)\tTop 1-err 43.7500 (51.0104)\tTop 5-err 9.3750 (24.8568)\n",
            "Epoch: [39/290][399/782]\tLR: 0.004501\tTime 0.145 (0.149)\tData 0.000 (0.001)\tLoss 1.9931 (2.1505)\tTop 1-err 51.5625 (50.9160)\tTop 5-err 18.7500 (24.5938)\n",
            "Epoch: [39/290][499/782]\tLR: 0.004501\tTime 0.146 (0.149)\tData 0.000 (0.001)\tLoss 3.7443 (2.1855)\tTop 1-err 87.5000 (51.4578)\tTop 5-err 53.1250 (25.1078)\n",
            "Epoch: [39/290][599/782]\tLR: 0.004501\tTime 0.145 (0.149)\tData 0.000 (0.000)\tLoss 3.8998 (2.2092)\tTop 1-err 85.9375 (51.8490)\tTop 5-err 71.8750 (25.5859)\n",
            "Epoch: [39/290][699/782]\tLR: 0.004501\tTime 0.146 (0.149)\tData 0.000 (0.000)\tLoss 3.3493 (2.2483)\tTop 1-err 93.7500 (52.5167)\tTop 5-err 78.1250 (26.2288)\n",
            "* Epoch: [39/290]\t Top 1-err 52.640  Top 5-err 26.277\t Train Loss 2.260\n",
            "* Epoch: [39/290]\t Top 1-err 38.850  Top 5-err 12.600\t Test Loss 1.402\n",
            "Current best accuracy (top-1 and 5 error): 38.85 12.6\n",
            "Epoch: [40/290][99/782]\tLR: 0.004526\tTime 0.146 (0.150)\tData 0.000 (0.002)\tLoss 3.4425 (2.3065)\tTop 1-err 67.1875 (52.7734)\tTop 5-err 37.5000 (26.9141)\n",
            "Epoch: [40/290][199/782]\tLR: 0.004526\tTime 0.149 (0.149)\tData 0.000 (0.001)\tLoss 3.8296 (2.2158)\tTop 1-err 93.7500 (51.1680)\tTop 5-err 78.1250 (25.2695)\n",
            "Epoch: [40/290][299/782]\tLR: 0.004526\tTime 0.143 (0.148)\tData 0.000 (0.001)\tLoss 1.2840 (2.2234)\tTop 1-err 34.3750 (51.1328)\tTop 5-err 9.3750 (25.2552)\n",
            "Epoch: [40/290][399/782]\tLR: 0.004526\tTime 0.143 (0.148)\tData 0.000 (0.001)\tLoss 1.2664 (2.2434)\tTop 1-err 35.9375 (51.7246)\tTop 5-err 10.9375 (26.0020)\n",
            "Epoch: [40/290][499/782]\tLR: 0.004526\tTime 0.152 (0.148)\tData 0.000 (0.000)\tLoss 1.3376 (2.2153)\tTop 1-err 34.3750 (51.4391)\tTop 5-err 15.6250 (25.7156)\n",
            "Epoch: [40/290][599/782]\tLR: 0.004526\tTime 0.154 (0.148)\tData 0.000 (0.000)\tLoss 3.9748 (2.2114)\tTop 1-err 85.9375 (51.2669)\tTop 5-err 71.8750 (25.2708)\n",
            "Epoch: [40/290][699/782]\tLR: 0.004526\tTime 0.145 (0.148)\tData 0.000 (0.000)\tLoss 1.3101 (2.2297)\tTop 1-err 37.5000 (51.6317)\tTop 5-err 14.0625 (25.6094)\n",
            "* Epoch: [40/290]\t Top 1-err 51.366  Top 5-err 25.247\t Train Loss 2.215\n",
            "* Epoch: [40/290]\t Top 1-err 38.750  Top 5-err 12.330\t Test Loss 1.383\n",
            "Current best accuracy (top-1 and 5 error): 38.75 12.33\n",
            "Epoch: [41/290][99/782]\tLR: 0.004552\tTime 0.145 (0.150)\tData 0.000 (0.002)\tLoss 1.5139 (2.1247)\tTop 1-err 42.1875 (48.6562)\tTop 5-err 14.0625 (22.9609)\n",
            "Epoch: [41/290][199/782]\tLR: 0.004552\tTime 0.144 (0.149)\tData 0.000 (0.001)\tLoss 1.2821 (2.1707)\tTop 1-err 37.5000 (49.7578)\tTop 5-err 9.3750 (24.2695)\n",
            "Epoch: [41/290][299/782]\tLR: 0.004552\tTime 0.145 (0.148)\tData 0.000 (0.001)\tLoss 2.4425 (2.2143)\tTop 1-err 43.7500 (50.3073)\tTop 5-err 18.7500 (24.6589)\n",
            "Epoch: [41/290][399/782]\tLR: 0.004552\tTime 0.144 (0.148)\tData 0.000 (0.001)\tLoss 1.4497 (2.1940)\tTop 1-err 37.5000 (50.0176)\tTop 5-err 14.0625 (24.3379)\n",
            "Epoch: [41/290][499/782]\tLR: 0.004552\tTime 0.145 (0.148)\tData 0.000 (0.000)\tLoss 1.3441 (2.1636)\tTop 1-err 32.8125 (49.4828)\tTop 5-err 9.3750 (23.6719)\n",
            "Epoch: [41/290][599/782]\tLR: 0.004552\tTime 0.152 (0.148)\tData 0.000 (0.000)\tLoss 1.3373 (2.1669)\tTop 1-err 37.5000 (49.6419)\tTop 5-err 12.5000 (23.7409)\n",
            "Epoch: [41/290][699/782]\tLR: 0.004552\tTime 0.147 (0.148)\tData 0.000 (0.000)\tLoss 3.4776 (2.1658)\tTop 1-err 67.1875 (49.9208)\tTop 5-err 42.1875 (23.9420)\n",
            "* Epoch: [41/290]\t Top 1-err 50.389  Top 5-err 24.380\t Train Loss 2.190\n",
            "* Epoch: [41/290]\t Top 1-err 40.030  Top 5-err 13.480\t Test Loss 1.458\n",
            "Current best accuracy (top-1 and 5 error): 38.75 12.33\n",
            "Epoch: [42/290][99/782]\tLR: 0.004579\tTime 0.145 (0.149)\tData 0.000 (0.002)\tLoss 2.7344 (2.0072)\tTop 1-err 53.1250 (47.5391)\tTop 5-err 18.7500 (21.9141)\n",
            "Epoch: [42/290][199/782]\tLR: 0.004579\tTime 0.146 (0.148)\tData 0.000 (0.001)\tLoss 3.2739 (2.1210)\tTop 1-err 68.7500 (49.9531)\tTop 5-err 40.6250 (24.3398)\n",
            "Epoch: [42/290][299/782]\tLR: 0.004579\tTime 0.147 (0.148)\tData 0.000 (0.001)\tLoss 1.2917 (2.1042)\tTop 1-err 37.5000 (49.3672)\tTop 5-err 7.8125 (23.9870)\n",
            "Epoch: [42/290][399/782]\tLR: 0.004579\tTime 0.145 (0.148)\tData 0.000 (0.001)\tLoss 2.9696 (2.1160)\tTop 1-err 62.5000 (49.2676)\tTop 5-err 26.5625 (23.8672)\n",
            "Epoch: [42/290][499/782]\tLR: 0.004579\tTime 0.145 (0.148)\tData 0.000 (0.001)\tLoss 3.4923 (2.1331)\tTop 1-err 98.4375 (49.4641)\tTop 5-err 85.9375 (23.8906)\n",
            "Epoch: [42/290][599/782]\tLR: 0.004579\tTime 0.148 (0.148)\tData 0.000 (0.000)\tLoss 3.6766 (2.1192)\tTop 1-err 82.8125 (49.1979)\tTop 5-err 42.1875 (23.5990)\n",
            "Epoch: [42/290][699/782]\tLR: 0.004579\tTime 0.151 (0.148)\tData 0.000 (0.000)\tLoss 1.7963 (2.1177)\tTop 1-err 45.3125 (49.1842)\tTop 5-err 21.8750 (23.4208)\n",
            "* Epoch: [42/290]\t Top 1-err 49.257  Top 5-err 23.586\t Train Loss 2.117\n",
            "* Epoch: [42/290]\t Top 1-err 38.470  Top 5-err 11.850\t Test Loss 1.355\n",
            "Current best accuracy (top-1 and 5 error): 38.47 11.85\n",
            "Epoch: [43/290][99/782]\tLR: 0.004606\tTime 0.153 (0.150)\tData 0.000 (0.002)\tLoss 1.1038 (2.1402)\tTop 1-err 28.1250 (49.4766)\tTop 5-err 7.8125 (24.6406)\n",
            "Epoch: [43/290][199/782]\tLR: 0.004606\tTime 0.152 (0.149)\tData 0.000 (0.001)\tLoss 2.3307 (2.1826)\tTop 1-err 40.6250 (50.8047)\tTop 5-err 12.5000 (24.8984)\n",
            "Epoch: [43/290][299/782]\tLR: 0.004606\tTime 0.143 (0.148)\tData 0.000 (0.001)\tLoss 1.1020 (2.1756)\tTop 1-err 31.2500 (50.8073)\tTop 5-err 6.2500 (24.9401)\n",
            "Epoch: [43/290][399/782]\tLR: 0.004606\tTime 0.147 (0.148)\tData 0.000 (0.001)\tLoss 4.6795 (2.1384)\tTop 1-err 81.2500 (49.7227)\tTop 5-err 60.9375 (24.0156)\n",
            "Epoch: [43/290][499/782]\tLR: 0.004606\tTime 0.145 (0.148)\tData 0.000 (0.000)\tLoss 1.2927 (2.1767)\tTop 1-err 39.0625 (50.2734)\tTop 5-err 6.2500 (24.4016)\n",
            "Epoch: [43/290][599/782]\tLR: 0.004606\tTime 0.144 (0.149)\tData 0.000 (0.000)\tLoss 2.9728 (2.1961)\tTop 1-err 60.9375 (50.8112)\tTop 5-err 37.5000 (24.8529)\n",
            "Epoch: [43/290][699/782]\tLR: 0.004606\tTime 0.145 (0.148)\tData 0.000 (0.000)\tLoss 2.3418 (2.1711)\tTop 1-err 45.3125 (50.3850)\tTop 5-err 18.7500 (24.5234)\n",
            "* Epoch: [43/290]\t Top 1-err 50.326  Top 5-err 24.513\t Train Loss 2.171\n",
            "* Epoch: [43/290]\t Top 1-err 39.470  Top 5-err 12.960\t Test Loss 1.430\n",
            "Current best accuracy (top-1 and 5 error): 38.47 11.85\n",
            "Epoch: [44/290][99/782]\tLR: 0.004634\tTime 0.152 (0.150)\tData 0.000 (0.002)\tLoss 3.4699 (2.1297)\tTop 1-err 64.0625 (48.7734)\tTop 5-err 35.9375 (22.1875)\n",
            "Epoch: [44/290][199/782]\tLR: 0.004634\tTime 0.146 (0.149)\tData 0.000 (0.001)\tLoss 3.9128 (2.0960)\tTop 1-err 85.9375 (48.3906)\tTop 5-err 64.0625 (22.7773)\n",
            "Epoch: [44/290][299/782]\tLR: 0.004634\tTime 0.145 (0.148)\tData 0.000 (0.001)\tLoss 1.1848 (2.1063)\tTop 1-err 31.2500 (48.5521)\tTop 5-err 10.9375 (22.8151)\n",
            "Epoch: [44/290][399/782]\tLR: 0.004634\tTime 0.145 (0.148)\tData 0.000 (0.000)\tLoss 3.1163 (2.1524)\tTop 1-err 71.8750 (49.3418)\tTop 5-err 46.8750 (23.9922)\n",
            "Epoch: [44/290][499/782]\tLR: 0.004634\tTime 0.151 (0.148)\tData 0.000 (0.000)\tLoss 1.5415 (2.1648)\tTop 1-err 45.3125 (49.7281)\tTop 5-err 15.6250 (24.3344)\n",
            "Epoch: [44/290][599/782]\tLR: 0.004634\tTime 0.145 (0.148)\tData 0.000 (0.000)\tLoss 3.5810 (2.1962)\tTop 1-err 81.2500 (50.2773)\tTop 5-err 53.1250 (24.8594)\n",
            "Epoch: [44/290][699/782]\tLR: 0.004634\tTime 0.149 (0.148)\tData 0.000 (0.000)\tLoss 3.5822 (2.2016)\tTop 1-err 87.5000 (50.4375)\tTop 5-err 62.5000 (24.9911)\n",
            "* Epoch: [44/290]\t Top 1-err 50.472  Top 5-err 24.909\t Train Loss 2.197\n",
            "* Epoch: [44/290]\t Top 1-err 37.740  Top 5-err 11.580\t Test Loss 1.334\n",
            "Current best accuracy (top-1 and 5 error): 37.74 11.58\n",
            "Epoch: [45/290][99/782]\tLR: 0.004662\tTime 0.145 (0.151)\tData 0.000 (0.002)\tLoss 3.8958 (2.1567)\tTop 1-err 92.1875 (49.3047)\tTop 5-err 60.9375 (24.3125)\n",
            "Epoch: [45/290][199/782]\tLR: 0.004662\tTime 0.146 (0.149)\tData 0.000 (0.001)\tLoss 3.3591 (2.1304)\tTop 1-err 73.4375 (49.0195)\tTop 5-err 42.1875 (24.1328)\n",
            "Epoch: [45/290][299/782]\tLR: 0.004662\tTime 0.145 (0.148)\tData 0.000 (0.001)\tLoss 3.6678 (2.1419)\tTop 1-err 65.6250 (49.3438)\tTop 5-err 31.2500 (24.0339)\n",
            "Epoch: [45/290][399/782]\tLR: 0.004662\tTime 0.145 (0.148)\tData 0.000 (0.001)\tLoss 3.4668 (2.1550)\tTop 1-err 73.4375 (49.7031)\tTop 5-err 50.0000 (24.1406)\n",
            "Epoch: [45/290][499/782]\tLR: 0.004662\tTime 0.144 (0.148)\tData 0.000 (0.000)\tLoss 1.1763 (2.1166)\tTop 1-err 40.6250 (48.9125)\tTop 5-err 14.0625 (23.5609)\n",
            "Epoch: [45/290][599/782]\tLR: 0.004662\tTime 0.160 (0.148)\tData 0.000 (0.000)\tLoss 5.5031 (2.1490)\tTop 1-err 96.8750 (49.5221)\tTop 5-err 89.0625 (24.2344)\n",
            "Epoch: [45/290][699/782]\tLR: 0.004662\tTime 0.145 (0.148)\tData 0.000 (0.000)\tLoss 1.1680 (2.1375)\tTop 1-err 34.3750 (49.3516)\tTop 5-err 9.3750 (24.0033)\n",
            "* Epoch: [45/290]\t Top 1-err 49.309  Top 5-err 23.952\t Train Loss 2.133\n",
            "* Epoch: [45/290]\t Top 1-err 37.880  Top 5-err 11.610\t Test Loss 1.339\n",
            "Current best accuracy (top-1 and 5 error): 37.74 11.58\n",
            "Epoch: [46/290][99/782]\tLR: 0.004691\tTime 0.151 (0.150)\tData 0.000 (0.002)\tLoss 3.4276 (2.0884)\tTop 1-err 67.1875 (48.9844)\tTop 5-err 43.7500 (23.7188)\n",
            "Epoch: [46/290][199/782]\tLR: 0.004691\tTime 0.148 (0.149)\tData 0.000 (0.001)\tLoss 3.6646 (2.1145)\tTop 1-err 84.3750 (49.1641)\tTop 5-err 60.9375 (23.3438)\n",
            "Epoch: [46/290][299/782]\tLR: 0.004691\tTime 0.156 (0.149)\tData 0.000 (0.001)\tLoss 1.5571 (2.1686)\tTop 1-err 46.8750 (49.8620)\tTop 5-err 14.0625 (24.2578)\n",
            "Epoch: [46/290][399/782]\tLR: 0.004691\tTime 0.152 (0.149)\tData 0.000 (0.001)\tLoss 1.2460 (2.1793)\tTop 1-err 31.2500 (50.1445)\tTop 5-err 9.3750 (24.5449)\n",
            "Epoch: [46/290][499/782]\tLR: 0.004691\tTime 0.151 (0.149)\tData 0.000 (0.000)\tLoss 3.6930 (2.2243)\tTop 1-err 62.5000 (50.9531)\tTop 5-err 45.3125 (25.3391)\n",
            "Epoch: [46/290][599/782]\tLR: 0.004691\tTime 0.145 (0.148)\tData 0.000 (0.000)\tLoss 1.5951 (2.2372)\tTop 1-err 48.4375 (51.2917)\tTop 5-err 21.8750 (25.4596)\n",
            "Epoch: [46/290][699/782]\tLR: 0.004691\tTime 0.146 (0.148)\tData 0.000 (0.000)\tLoss 3.3452 (2.2222)\tTop 1-err 59.3750 (51.0714)\tTop 5-err 29.6875 (25.0692)\n",
            "* Epoch: [46/290]\t Top 1-err 50.689  Top 5-err 24.674\t Train Loss 2.201\n",
            "* Epoch: [46/290]\t Top 1-err 36.860  Top 5-err 11.110\t Test Loss 1.299\n",
            "Current best accuracy (top-1 and 5 error): 36.86 11.11\n",
            "Epoch: [47/290][99/782]\tLR: 0.004721\tTime 0.149 (0.150)\tData 0.000 (0.002)\tLoss 1.3675 (1.9970)\tTop 1-err 39.0625 (46.2031)\tTop 5-err 12.5000 (21.0000)\n",
            "Epoch: [47/290][199/782]\tLR: 0.004721\tTime 0.144 (0.148)\tData 0.000 (0.001)\tLoss 1.6234 (2.0278)\tTop 1-err 48.4375 (47.2930)\tTop 5-err 15.6250 (22.1172)\n",
            "Epoch: [47/290][299/782]\tLR: 0.004721\tTime 0.154 (0.148)\tData 0.000 (0.001)\tLoss 1.3895 (2.0489)\tTop 1-err 46.8750 (47.6667)\tTop 5-err 9.3750 (22.5885)\n",
            "Epoch: [47/290][399/782]\tLR: 0.004721\tTime 0.148 (0.148)\tData 0.000 (0.001)\tLoss 4.3657 (2.0616)\tTop 1-err 90.6250 (47.5742)\tTop 5-err 65.6250 (22.3301)\n",
            "Epoch: [47/290][499/782]\tLR: 0.004721\tTime 0.147 (0.148)\tData 0.000 (0.000)\tLoss 4.3311 (2.0625)\tTop 1-err 85.9375 (47.8375)\tTop 5-err 64.0625 (22.6125)\n",
            "Epoch: [47/290][599/782]\tLR: 0.004721\tTime 0.145 (0.148)\tData 0.000 (0.000)\tLoss 2.3457 (2.0849)\tTop 1-err 50.0000 (48.4635)\tTop 5-err 20.3125 (23.1185)\n",
            "Epoch: [47/290][699/782]\tLR: 0.004721\tTime 0.144 (0.148)\tData 0.000 (0.000)\tLoss 1.3498 (2.0897)\tTop 1-err 37.5000 (48.6272)\tTop 5-err 10.9375 (23.3315)\n",
            "* Epoch: [47/290]\t Top 1-err 48.776  Top 5-err 23.477\t Train Loss 2.094\n",
            "* Epoch: [47/290]\t Top 1-err 36.350  Top 5-err 10.950\t Test Loss 1.293\n",
            "Current best accuracy (top-1 and 5 error): 36.35 10.95\n",
            "Epoch: [48/290][99/782]\tLR: 0.004751\tTime 0.152 (0.151)\tData 0.000 (0.002)\tLoss 1.0423 (2.0000)\tTop 1-err 32.8125 (46.6250)\tTop 5-err 4.6875 (21.6016)\n",
            "Epoch: [48/290][199/782]\tLR: 0.004751\tTime 0.145 (0.149)\tData 0.000 (0.001)\tLoss 2.6034 (2.1208)\tTop 1-err 54.6875 (48.4688)\tTop 5-err 26.5625 (23.2344)\n",
            "Epoch: [48/290][299/782]\tLR: 0.004751\tTime 0.155 (0.149)\tData 0.000 (0.001)\tLoss 4.5173 (2.1262)\tTop 1-err 96.8750 (48.3229)\tTop 5-err 76.5625 (22.8646)\n",
            "Epoch: [48/290][399/782]\tLR: 0.004751\tTime 0.144 (0.149)\tData 0.000 (0.001)\tLoss 1.8804 (2.1173)\tTop 1-err 54.6875 (48.0664)\tTop 5-err 18.7500 (22.5508)\n",
            "Epoch: [48/290][499/782]\tLR: 0.004751\tTime 0.154 (0.148)\tData 0.000 (0.000)\tLoss 3.2486 (2.1297)\tTop 1-err 65.6250 (48.5031)\tTop 5-err 32.8125 (22.9953)\n",
            "Epoch: [48/290][599/782]\tLR: 0.004751\tTime 0.152 (0.148)\tData 0.000 (0.000)\tLoss 1.5477 (2.1239)\tTop 1-err 43.7500 (48.5352)\tTop 5-err 17.1875 (22.9076)\n",
            "Epoch: [48/290][699/782]\tLR: 0.004751\tTime 0.145 (0.148)\tData 0.000 (0.000)\tLoss 2.7393 (2.1394)\tTop 1-err 50.0000 (48.8906)\tTop 5-err 18.7500 (23.2232)\n",
            "* Epoch: [48/290]\t Top 1-err 49.171  Top 5-err 23.502\t Train Loss 2.155\n",
            "* Epoch: [48/290]\t Top 1-err 37.010  Top 5-err 11.680\t Test Loss 1.339\n",
            "Current best accuracy (top-1 and 5 error): 36.35 10.95\n",
            "Epoch: [49/290][99/782]\tLR: 0.004782\tTime 0.145 (0.149)\tData 0.000 (0.002)\tLoss 1.3261 (2.1069)\tTop 1-err 40.6250 (48.4141)\tTop 5-err 14.0625 (23.2109)\n",
            "Epoch: [49/290][199/782]\tLR: 0.004782\tTime 0.144 (0.148)\tData 0.000 (0.001)\tLoss 1.0599 (2.0543)\tTop 1-err 29.6875 (47.4570)\tTop 5-err 3.1250 (22.7188)\n",
            "Epoch: [49/290][299/782]\tLR: 0.004782\tTime 0.153 (0.148)\tData 0.000 (0.001)\tLoss 1.1676 (2.0479)\tTop 1-err 32.8125 (47.4792)\tTop 5-err 9.3750 (22.6406)\n",
            "Epoch: [49/290][399/782]\tLR: 0.004782\tTime 0.146 (0.148)\tData 0.000 (0.001)\tLoss 4.0125 (2.0680)\tTop 1-err 87.5000 (47.6445)\tTop 5-err 65.6250 (22.4883)\n",
            "Epoch: [49/290][499/782]\tLR: 0.004782\tTime 0.146 (0.148)\tData 0.000 (0.001)\tLoss 3.6578 (2.0927)\tTop 1-err 67.1875 (48.2109)\tTop 5-err 46.8750 (23.0469)\n",
            "Epoch: [49/290][599/782]\tLR: 0.004782\tTime 0.146 (0.148)\tData 0.000 (0.000)\tLoss 2.8094 (2.1338)\tTop 1-err 56.2500 (48.9948)\tTop 5-err 26.5625 (23.8646)\n",
            "Epoch: [49/290][699/782]\tLR: 0.004782\tTime 0.144 (0.148)\tData 0.000 (0.000)\tLoss 2.9647 (2.1434)\tTop 1-err 54.6875 (49.1830)\tTop 5-err 32.8125 (24.0089)\n",
            "* Epoch: [49/290]\t Top 1-err 49.288  Top 5-err 24.101\t Train Loss 2.146\n",
            "* Epoch: [49/290]\t Top 1-err 37.990  Top 5-err 11.770\t Test Loss 1.350\n",
            "Current best accuracy (top-1 and 5 error): 36.35 10.95\n",
            "Epoch: [50/290][99/782]\tLR: 0.004814\tTime 0.146 (0.150)\tData 0.000 (0.002)\tLoss 4.2011 (2.2664)\tTop 1-err 96.8750 (51.1797)\tTop 5-err 78.1250 (25.1797)\n",
            "Epoch: [50/290][199/782]\tLR: 0.004814\tTime 0.153 (0.149)\tData 0.000 (0.001)\tLoss 3.5309 (2.1669)\tTop 1-err 76.5625 (49.0742)\tTop 5-err 60.9375 (23.5469)\n",
            "Epoch: [50/290][299/782]\tLR: 0.004814\tTime 0.150 (0.149)\tData 0.000 (0.001)\tLoss 1.2507 (2.1752)\tTop 1-err 37.5000 (49.5573)\tTop 5-err 10.9375 (23.8698)\n",
            "Epoch: [50/290][399/782]\tLR: 0.004814\tTime 0.145 (0.148)\tData 0.000 (0.001)\tLoss 1.1558 (2.1827)\tTop 1-err 37.5000 (49.9570)\tTop 5-err 10.9375 (24.3008)\n",
            "Epoch: [50/290][499/782]\tLR: 0.004814\tTime 0.153 (0.148)\tData 0.000 (0.000)\tLoss 3.3342 (2.1517)\tTop 1-err 96.8750 (49.4375)\tTop 5-err 84.3750 (23.6594)\n",
            "Epoch: [50/290][599/782]\tLR: 0.004814\tTime 0.144 (0.148)\tData 0.000 (0.000)\tLoss 1.0872 (2.1693)\tTop 1-err 31.2500 (49.7617)\tTop 5-err 9.3750 (24.0677)\n",
            "Epoch: [50/290][699/782]\tLR: 0.004814\tTime 0.154 (0.148)\tData 0.000 (0.000)\tLoss 3.8765 (2.1926)\tTop 1-err 85.9375 (50.2154)\tTop 5-err 68.7500 (24.5223)\n",
            "* Epoch: [50/290]\t Top 1-err 49.834  Top 5-err 24.187\t Train Loss 2.172\n",
            "* Epoch: [50/290]\t Top 1-err 35.310  Top 5-err 10.580\t Test Loss 1.262\n",
            "Current best accuracy (top-1 and 5 error): 35.31 10.58\n",
            "Epoch: [51/290][99/782]\tLR: 0.004846\tTime 0.149 (0.151)\tData 0.000 (0.002)\tLoss 3.9461 (2.2325)\tTop 1-err 90.6250 (50.3516)\tTop 5-err 70.3125 (24.3047)\n",
            "Epoch: [51/290][199/782]\tLR: 0.004846\tTime 0.148 (0.149)\tData 0.000 (0.001)\tLoss 1.2401 (2.2647)\tTop 1-err 32.8125 (50.4297)\tTop 5-err 7.8125 (24.9570)\n",
            "Epoch: [51/290][299/782]\tLR: 0.004846\tTime 0.147 (0.148)\tData 0.000 (0.001)\tLoss 0.9878 (2.1439)\tTop 1-err 29.6875 (48.4375)\tTop 5-err 4.6875 (23.1484)\n",
            "Epoch: [51/290][399/782]\tLR: 0.004846\tTime 0.153 (0.148)\tData 0.000 (0.001)\tLoss 3.5898 (2.1416)\tTop 1-err 75.0000 (48.5547)\tTop 5-err 53.1250 (23.2559)\n",
            "Epoch: [51/290][499/782]\tLR: 0.004846\tTime 0.145 (0.148)\tData 0.000 (0.001)\tLoss 3.4229 (2.1492)\tTop 1-err 64.0625 (48.6828)\tTop 5-err 26.5625 (23.3109)\n",
            "Epoch: [51/290][599/782]\tLR: 0.004846\tTime 0.146 (0.148)\tData 0.000 (0.000)\tLoss 3.4684 (2.1193)\tTop 1-err 75.0000 (48.1458)\tTop 5-err 43.7500 (22.8529)\n",
            "Epoch: [51/290][699/782]\tLR: 0.004846\tTime 0.144 (0.148)\tData 0.000 (0.000)\tLoss 3.8231 (2.1325)\tTop 1-err 87.5000 (48.6194)\tTop 5-err 54.6875 (23.2054)\n",
            "* Epoch: [51/290]\t Top 1-err 48.713  Top 5-err 23.275\t Train Loss 2.139\n",
            "* Epoch: [51/290]\t Top 1-err 35.690  Top 5-err 10.410\t Test Loss 1.260\n",
            "Current best accuracy (top-1 and 5 error): 35.31 10.58\n",
            "Epoch: [52/290][99/782]\tLR: 0.004878\tTime 0.147 (0.150)\tData 0.000 (0.002)\tLoss 3.2250 (2.1888)\tTop 1-err 64.0625 (49.7422)\tTop 5-err 35.9375 (24.4609)\n",
            "Epoch: [52/290][199/782]\tLR: 0.004878\tTime 0.147 (0.149)\tData 0.000 (0.001)\tLoss 1.2249 (2.1423)\tTop 1-err 31.2500 (48.8828)\tTop 5-err 4.6875 (23.2539)\n",
            "Epoch: [52/290][299/782]\tLR: 0.004878\tTime 0.152 (0.148)\tData 0.000 (0.001)\tLoss 1.5503 (2.1337)\tTop 1-err 40.6250 (48.8464)\tTop 5-err 14.0625 (23.2448)\n",
            "Epoch: [52/290][399/782]\tLR: 0.004878\tTime 0.144 (0.148)\tData 0.000 (0.001)\tLoss 1.0730 (2.1142)\tTop 1-err 34.3750 (48.1230)\tTop 5-err 4.6875 (22.5156)\n",
            "Epoch: [52/290][499/782]\tLR: 0.004878\tTime 0.145 (0.148)\tData 0.000 (0.001)\tLoss 3.5538 (2.0824)\tTop 1-err 79.6875 (47.4531)\tTop 5-err 54.6875 (22.0188)\n",
            "Epoch: [52/290][599/782]\tLR: 0.004878\tTime 0.145 (0.148)\tData 0.000 (0.000)\tLoss 3.2433 (2.1048)\tTop 1-err 51.5625 (47.9102)\tTop 5-err 32.8125 (22.4961)\n",
            "Epoch: [52/290][699/782]\tLR: 0.004878\tTime 0.145 (0.148)\tData 0.000 (0.000)\tLoss 2.4515 (2.0974)\tTop 1-err 46.8750 (47.6406)\tTop 5-err 25.0000 (22.1395)\n",
            "* Epoch: [52/290]\t Top 1-err 47.677  Top 5-err 22.236\t Train Loss 2.092\n",
            "* Epoch: [52/290]\t Top 1-err 36.240  Top 5-err 11.020\t Test Loss 1.289\n",
            "Current best accuracy (top-1 and 5 error): 35.31 10.58\n",
            "Epoch: [53/290][99/782]\tLR: 0.004912\tTime 0.150 (0.150)\tData 0.000 (0.002)\tLoss 4.1956 (2.1607)\tTop 1-err 95.3125 (50.2656)\tTop 5-err 75.0000 (25.4844)\n",
            "Epoch: [53/290][199/782]\tLR: 0.004912\tTime 0.145 (0.149)\tData 0.000 (0.001)\tLoss 3.1087 (2.1145)\tTop 1-err 48.4375 (48.5664)\tTop 5-err 34.3750 (23.6836)\n",
            "Epoch: [53/290][299/782]\tLR: 0.004912\tTime 0.145 (0.148)\tData 0.000 (0.001)\tLoss 3.2028 (2.0958)\tTop 1-err 68.7500 (47.9453)\tTop 5-err 31.2500 (22.9167)\n",
            "Epoch: [53/290][399/782]\tLR: 0.004912\tTime 0.145 (0.148)\tData 0.000 (0.000)\tLoss 3.1757 (2.0988)\tTop 1-err 62.5000 (48.1895)\tTop 5-err 32.8125 (23.1504)\n",
            "Epoch: [53/290][499/782]\tLR: 0.004912\tTime 0.144 (0.148)\tData 0.000 (0.000)\tLoss 1.0942 (2.0870)\tTop 1-err 31.2500 (48.0063)\tTop 5-err 4.6875 (22.7688)\n",
            "Epoch: [53/290][599/782]\tLR: 0.004912\tTime 0.142 (0.148)\tData 0.000 (0.000)\tLoss 1.4785 (2.0780)\tTop 1-err 48.4375 (48.0417)\tTop 5-err 12.5000 (22.8828)\n",
            "Epoch: [53/290][699/782]\tLR: 0.004912\tTime 0.145 (0.148)\tData 0.000 (0.000)\tLoss 1.3283 (2.0670)\tTop 1-err 39.0625 (47.7031)\tTop 5-err 9.3750 (22.6663)\n",
            "* Epoch: [53/290]\t Top 1-err 47.509  Top 5-err 22.471\t Train Loss 2.057\n",
            "* Epoch: [53/290]\t Top 1-err 36.060  Top 5-err 10.600\t Test Loss 1.264\n",
            "Current best accuracy (top-1 and 5 error): 35.31 10.58\n",
            "Epoch: [54/290][99/782]\tLR: 0.004946\tTime 0.143 (0.150)\tData 0.000 (0.002)\tLoss 1.2882 (2.1310)\tTop 1-err 28.1250 (48.5703)\tTop 5-err 12.5000 (23.9531)\n",
            "Epoch: [54/290][199/782]\tLR: 0.004946\tTime 0.145 (0.148)\tData 0.000 (0.001)\tLoss 1.3852 (2.1020)\tTop 1-err 43.7500 (48.0352)\tTop 5-err 7.8125 (22.3398)\n",
            "Epoch: [54/290][299/782]\tLR: 0.004946\tTime 0.146 (0.148)\tData 0.000 (0.001)\tLoss 3.8228 (2.1154)\tTop 1-err 79.6875 (48.5833)\tTop 5-err 50.0000 (22.9583)\n",
            "Epoch: [54/290][399/782]\tLR: 0.004946\tTime 0.143 (0.148)\tData 0.000 (0.001)\tLoss 1.2330 (2.0689)\tTop 1-err 42.1875 (47.8398)\tTop 5-err 10.9375 (22.2520)\n",
            "Epoch: [54/290][499/782]\tLR: 0.004946\tTime 0.144 (0.148)\tData 0.000 (0.000)\tLoss 1.2227 (2.0586)\tTop 1-err 35.9375 (47.6750)\tTop 5-err 9.3750 (22.0938)\n",
            "Epoch: [54/290][599/782]\tLR: 0.004946\tTime 0.146 (0.148)\tData 0.000 (0.000)\tLoss 3.1863 (2.0552)\tTop 1-err 62.5000 (47.6094)\tTop 5-err 21.8750 (22.0221)\n",
            "Epoch: [54/290][699/782]\tLR: 0.004946\tTime 0.144 (0.148)\tData 0.000 (0.000)\tLoss 1.9417 (2.0543)\tTop 1-err 48.4375 (47.6150)\tTop 5-err 15.6250 (21.9911)\n",
            "* Epoch: [54/290]\t Top 1-err 47.859  Top 5-err 22.243\t Train Loss 2.067\n",
            "* Epoch: [54/290]\t Top 1-err 35.810  Top 5-err 10.560\t Test Loss 1.267\n",
            "Current best accuracy (top-1 and 5 error): 35.31 10.58\n",
            "Epoch: [55/290][99/782]\tLR: 0.004980\tTime 0.145 (0.150)\tData 0.000 (0.002)\tLoss 1.3891 (2.1322)\tTop 1-err 37.5000 (48.6328)\tTop 5-err 7.8125 (23.1953)\n",
            "Epoch: [55/290][199/782]\tLR: 0.004980\tTime 0.145 (0.149)\tData 0.000 (0.001)\tLoss 1.3046 (2.0038)\tTop 1-err 35.9375 (46.3242)\tTop 5-err 9.3750 (21.3633)\n",
            "Epoch: [55/290][299/782]\tLR: 0.004980\tTime 0.153 (0.149)\tData 0.000 (0.001)\tLoss 1.1529 (1.9701)\tTop 1-err 34.3750 (45.4740)\tTop 5-err 7.8125 (20.6693)\n",
            "Epoch: [55/290][399/782]\tLR: 0.004980\tTime 0.144 (0.148)\tData 0.000 (0.001)\tLoss 3.9433 (2.0269)\tTop 1-err 93.7500 (46.4316)\tTop 5-err 62.5000 (21.4883)\n",
            "Epoch: [55/290][499/782]\tLR: 0.004980\tTime 0.147 (0.148)\tData 0.000 (0.001)\tLoss 1.3381 (2.0287)\tTop 1-err 37.5000 (46.3969)\tTop 5-err 10.9375 (21.3703)\n",
            "Epoch: [55/290][599/782]\tLR: 0.004980\tTime 0.146 (0.148)\tData 0.000 (0.000)\tLoss 2.9721 (2.0301)\tTop 1-err 56.2500 (46.5859)\tTop 5-err 23.4375 (21.5547)\n",
            "Epoch: [55/290][699/782]\tLR: 0.004980\tTime 0.152 (0.148)\tData 0.000 (0.000)\tLoss 2.2496 (2.0397)\tTop 1-err 59.3750 (46.5993)\tTop 5-err 23.4375 (21.4643)\n",
            "* Epoch: [55/290]\t Top 1-err 46.643  Top 5-err 21.534\t Train Loss 2.037\n",
            "* Epoch: [55/290]\t Top 1-err 36.280  Top 5-err 10.890\t Test Loss 1.283\n",
            "Current best accuracy (top-1 and 5 error): 35.31 10.58\n",
            "Epoch: [56/290][99/782]\tLR: 0.005016\tTime 0.145 (0.150)\tData 0.000 (0.002)\tLoss 3.9090 (2.3768)\tTop 1-err 92.1875 (51.4844)\tTop 5-err 64.0625 (26.0781)\n",
            "Epoch: [56/290][199/782]\tLR: 0.005016\tTime 0.145 (0.148)\tData 0.000 (0.001)\tLoss 4.4297 (2.2251)\tTop 1-err 92.1875 (49.5703)\tTop 5-err 79.6875 (24.3789)\n",
            "Epoch: [56/290][299/782]\tLR: 0.005016\tTime 0.143 (0.148)\tData 0.000 (0.001)\tLoss 1.2143 (2.2087)\tTop 1-err 40.6250 (49.2995)\tTop 5-err 9.3750 (24.2812)\n",
            "Epoch: [56/290][399/782]\tLR: 0.005016\tTime 0.146 (0.148)\tData 0.000 (0.001)\tLoss 3.3425 (2.1892)\tTop 1-err 56.2500 (49.0879)\tTop 5-err 21.8750 (23.9785)\n",
            "Epoch: [56/290][499/782]\tLR: 0.005016\tTime 0.151 (0.148)\tData 0.000 (0.000)\tLoss 5.0810 (2.1352)\tTop 1-err 98.4375 (48.2328)\tTop 5-err 87.5000 (23.2766)\n",
            "Epoch: [56/290][599/782]\tLR: 0.005016\tTime 0.155 (0.148)\tData 0.000 (0.000)\tLoss 3.2121 (2.1142)\tTop 1-err 64.0625 (47.9753)\tTop 5-err 32.8125 (23.1367)\n",
            "Epoch: [56/290][699/782]\tLR: 0.005016\tTime 0.145 (0.148)\tData 0.000 (0.000)\tLoss 2.9377 (2.1232)\tTop 1-err 60.9375 (48.1228)\tTop 5-err 28.1250 (23.2958)\n",
            "* Epoch: [56/290]\t Top 1-err 47.886  Top 5-err 23.052\t Train Loss 2.107\n",
            "* Epoch: [56/290]\t Top 1-err 35.590  Top 5-err 10.660\t Test Loss 1.270\n",
            "Current best accuracy (top-1 and 5 error): 35.31 10.58\n",
            "Epoch: [57/290][99/782]\tLR: 0.005051\tTime 0.150 (0.151)\tData 0.000 (0.003)\tLoss 3.0124 (1.8814)\tTop 1-err 53.1250 (44.1094)\tTop 5-err 26.5625 (20.5156)\n",
            "Epoch: [57/290][199/782]\tLR: 0.005051\tTime 0.152 (0.150)\tData 0.000 (0.001)\tLoss 3.4784 (1.9963)\tTop 1-err 73.4375 (46.1914)\tTop 5-err 43.7500 (21.7539)\n",
            "Epoch: [57/290][299/782]\tLR: 0.005051\tTime 0.148 (0.149)\tData 0.000 (0.001)\tLoss 1.2851 (2.0038)\tTop 1-err 34.3750 (45.6875)\tTop 5-err 7.8125 (21.0990)\n",
            "Epoch: [57/290][399/782]\tLR: 0.005051\tTime 0.145 (0.149)\tData 0.000 (0.001)\tLoss 1.5547 (2.0260)\tTop 1-err 35.9375 (46.2793)\tTop 5-err 7.8125 (21.6289)\n",
            "Epoch: [57/290][499/782]\tLR: 0.005051\tTime 0.147 (0.149)\tData 0.000 (0.001)\tLoss 1.0907 (2.0365)\tTop 1-err 29.6875 (46.6234)\tTop 5-err 9.3750 (21.8672)\n",
            "Epoch: [57/290][599/782]\tLR: 0.005051\tTime 0.148 (0.149)\tData 0.000 (0.001)\tLoss 1.4408 (2.0362)\tTop 1-err 39.0625 (46.6341)\tTop 5-err 15.6250 (21.8802)\n",
            "Epoch: [57/290][699/782]\tLR: 0.005051\tTime 0.155 (0.148)\tData 0.000 (0.000)\tLoss 0.9209 (2.0434)\tTop 1-err 28.1250 (46.8884)\tTop 5-err 4.6875 (22.0837)\n",
            "* Epoch: [57/290]\t Top 1-err 47.161  Top 5-err 22.294\t Train Loss 2.060\n",
            "* Epoch: [57/290]\t Top 1-err 34.230  Top 5-err 9.620\t Test Loss 1.191\n",
            "Current best accuracy (top-1 and 5 error): 34.23 9.62\n",
            "Epoch: [58/290][99/782]\tLR: 0.005088\tTime 0.150 (0.151)\tData 0.000 (0.002)\tLoss 1.0359 (2.1135)\tTop 1-err 32.8125 (46.6406)\tTop 5-err 9.3750 (21.4375)\n",
            "Epoch: [58/290][199/782]\tLR: 0.005088\tTime 0.145 (0.150)\tData 0.000 (0.001)\tLoss 3.3302 (2.0873)\tTop 1-err 71.8750 (46.7773)\tTop 5-err 35.9375 (21.7461)\n",
            "Epoch: [58/290][299/782]\tLR: 0.005088\tTime 0.159 (0.150)\tData 0.000 (0.001)\tLoss 0.9783 (2.0646)\tTop 1-err 28.1250 (46.0339)\tTop 5-err 4.6875 (21.1797)\n",
            "Epoch: [58/290][399/782]\tLR: 0.005088\tTime 0.151 (0.150)\tData 0.000 (0.001)\tLoss 3.9769 (2.0859)\tTop 1-err 81.2500 (46.8320)\tTop 5-err 60.9375 (21.7246)\n",
            "Epoch: [58/290][499/782]\tLR: 0.005088\tTime 0.144 (0.149)\tData 0.000 (0.001)\tLoss 1.1611 (2.0678)\tTop 1-err 37.5000 (46.7484)\tTop 5-err 10.9375 (21.6828)\n",
            "Epoch: [58/290][599/782]\tLR: 0.005088\tTime 0.145 (0.149)\tData 0.000 (0.000)\tLoss 3.8995 (2.0454)\tTop 1-err 65.6250 (46.2591)\tTop 5-err 43.7500 (21.1901)\n",
            "Epoch: [58/290][699/782]\tLR: 0.005088\tTime 0.145 (0.149)\tData 0.000 (0.000)\tLoss 1.0282 (2.0347)\tTop 1-err 35.9375 (46.0681)\tTop 5-err 3.1250 (21.0949)\n",
            "* Epoch: [58/290]\t Top 1-err 45.719  Top 5-err 20.768\t Train Loss 2.013\n",
            "* Epoch: [58/290]\t Top 1-err 34.480  Top 5-err 9.930\t Test Loss 1.228\n",
            "Current best accuracy (top-1 and 5 error): 34.23 9.62\n",
            "Epoch: [59/290][99/782]\tLR: 0.005125\tTime 0.145 (0.151)\tData 0.000 (0.002)\tLoss 1.2110 (1.8792)\tTop 1-err 34.3750 (43.0078)\tTop 5-err 12.5000 (18.5078)\n",
            "Epoch: [59/290][199/782]\tLR: 0.005125\tTime 0.145 (0.149)\tData 0.000 (0.001)\tLoss 3.4077 (1.9599)\tTop 1-err 85.9375 (45.1953)\tTop 5-err 51.5625 (20.3633)\n",
            "Epoch: [59/290][299/782]\tLR: 0.005125\tTime 0.149 (0.148)\tData 0.000 (0.001)\tLoss 3.5736 (1.9829)\tTop 1-err 70.3125 (45.5469)\tTop 5-err 35.9375 (20.5443)\n",
            "Epoch: [59/290][399/782]\tLR: 0.005125\tTime 0.144 (0.148)\tData 0.000 (0.001)\tLoss 2.2329 (2.0037)\tTop 1-err 50.0000 (45.6328)\tTop 5-err 25.0000 (20.5820)\n",
            "Epoch: [59/290][499/782]\tLR: 0.005125\tTime 0.152 (0.148)\tData 0.000 (0.001)\tLoss 1.2110 (2.0153)\tTop 1-err 31.2500 (45.9594)\tTop 5-err 9.3750 (20.8266)\n",
            "Epoch: [59/290][599/782]\tLR: 0.005125\tTime 0.143 (0.148)\tData 0.000 (0.000)\tLoss 1.3970 (2.0224)\tTop 1-err 40.6250 (46.1510)\tTop 5-err 14.0625 (21.1133)\n",
            "Epoch: [59/290][699/782]\tLR: 0.005125\tTime 0.152 (0.148)\tData 0.000 (0.000)\tLoss 1.0534 (2.0016)\tTop 1-err 29.6875 (45.6998)\tTop 5-err 7.8125 (20.7165)\n",
            "* Epoch: [59/290]\t Top 1-err 45.905  Top 5-err 21.041\t Train Loss 2.020\n",
            "* Epoch: [59/290]\t Top 1-err 37.150  Top 5-err 11.970\t Test Loss 1.361\n",
            "Current best accuracy (top-1 and 5 error): 34.23 9.62\n",
            "Epoch: [60/290][99/782]\tLR: 0.005162\tTime 0.145 (0.150)\tData 0.000 (0.002)\tLoss 4.3321 (2.0635)\tTop 1-err 93.7500 (46.5547)\tTop 5-err 84.3750 (22.9844)\n",
            "Epoch: [60/290][199/782]\tLR: 0.005162\tTime 0.152 (0.150)\tData 0.000 (0.001)\tLoss 1.1837 (2.0761)\tTop 1-err 28.1250 (46.5938)\tTop 5-err 10.9375 (22.5586)\n",
            "Epoch: [60/290][299/782]\tLR: 0.005162\tTime 0.146 (0.150)\tData 0.000 (0.001)\tLoss 2.9975 (2.0633)\tTop 1-err 60.9375 (46.2161)\tTop 5-err 29.6875 (21.6042)\n",
            "Epoch: [60/290][399/782]\tLR: 0.005162\tTime 0.145 (0.149)\tData 0.000 (0.001)\tLoss 1.1498 (2.0475)\tTop 1-err 31.2500 (46.1621)\tTop 5-err 7.8125 (21.5332)\n",
            "Epoch: [60/290][499/782]\tLR: 0.005162\tTime 0.153 (0.149)\tData 0.000 (0.001)\tLoss 3.3903 (2.0315)\tTop 1-err 71.8750 (45.8844)\tTop 5-err 39.0625 (21.2063)\n",
            "Epoch: [60/290][599/782]\tLR: 0.005162\tTime 0.147 (0.149)\tData 0.000 (0.000)\tLoss 0.7436 (2.0607)\tTop 1-err 21.8750 (46.5039)\tTop 5-err 4.6875 (21.6901)\n",
            "Epoch: [60/290][699/782]\tLR: 0.005162\tTime 0.145 (0.148)\tData 0.000 (0.000)\tLoss 3.2221 (2.0610)\tTop 1-err 64.0625 (46.6663)\tTop 5-err 40.6250 (21.9520)\n",
            "* Epoch: [60/290]\t Top 1-err 46.637  Top 5-err 21.921\t Train Loss 2.058\n",
            "* Epoch: [60/290]\t Top 1-err 34.570  Top 5-err 10.230\t Test Loss 1.221\n",
            "Current best accuracy (top-1 and 5 error): 34.23 9.62\n",
            "Epoch: [61/290][99/782]\tLR: 0.005201\tTime 0.145 (0.151)\tData 0.000 (0.002)\tLoss 3.5351 (2.1569)\tTop 1-err 73.4375 (48.5859)\tTop 5-err 48.4375 (24.3125)\n",
            "Epoch: [61/290][199/782]\tLR: 0.005201\tTime 0.144 (0.149)\tData 0.000 (0.001)\tLoss 0.9574 (2.0697)\tTop 1-err 20.3125 (46.9453)\tTop 5-err 9.3750 (22.6172)\n",
            "Epoch: [61/290][299/782]\tLR: 0.005201\tTime 0.145 (0.149)\tData 0.000 (0.001)\tLoss 3.4519 (2.1033)\tTop 1-err 73.4375 (47.7682)\tTop 5-err 40.6250 (23.2422)\n",
            "Epoch: [61/290][399/782]\tLR: 0.005201\tTime 0.148 (0.148)\tData 0.000 (0.001)\tLoss 1.0720 (2.0976)\tTop 1-err 26.5625 (47.7578)\tTop 5-err 7.8125 (23.1680)\n",
            "Epoch: [61/290][499/782]\tLR: 0.005201\tTime 0.156 (0.148)\tData 0.000 (0.001)\tLoss 3.1988 (2.0826)\tTop 1-err 68.7500 (47.3922)\tTop 5-err 31.2500 (22.7344)\n",
            "Epoch: [61/290][599/782]\tLR: 0.005201\tTime 0.146 (0.148)\tData 0.000 (0.000)\tLoss 4.0879 (2.0838)\tTop 1-err 96.8750 (47.2565)\tTop 5-err 90.6250 (22.4857)\n",
            "Epoch: [61/290][699/782]\tLR: 0.005201\tTime 0.152 (0.148)\tData 0.000 (0.000)\tLoss 0.9428 (2.0808)\tTop 1-err 32.8125 (47.3192)\tTop 5-err 3.1250 (22.5312)\n",
            "* Epoch: [61/290]\t Top 1-err 47.025  Top 5-err 22.342\t Train Loss 2.063\n",
            "* Epoch: [61/290]\t Top 1-err 34.720  Top 5-err 10.460\t Test Loss 1.224\n",
            "Current best accuracy (top-1 and 5 error): 34.23 9.62\n",
            "Epoch: [62/290][99/782]\tLR: 0.005240\tTime 0.152 (0.149)\tData 0.000 (0.002)\tLoss 1.4286 (2.0751)\tTop 1-err 37.5000 (46.0234)\tTop 5-err 14.0625 (21.5156)\n",
            "Epoch: [62/290][199/782]\tLR: 0.005240\tTime 0.144 (0.148)\tData 0.000 (0.001)\tLoss 1.2177 (2.1768)\tTop 1-err 32.8125 (48.0039)\tTop 5-err 3.1250 (23.9844)\n",
            "Epoch: [62/290][299/782]\tLR: 0.005240\tTime 0.145 (0.148)\tData 0.000 (0.001)\tLoss 4.4168 (2.1892)\tTop 1-err 90.6250 (48.8151)\tTop 5-err 76.5625 (24.6198)\n",
            "Epoch: [62/290][399/782]\tLR: 0.005240\tTime 0.144 (0.148)\tData 0.000 (0.001)\tLoss 2.6384 (2.1321)\tTop 1-err 45.3125 (47.5273)\tTop 5-err 15.6250 (23.3457)\n",
            "Epoch: [62/290][499/782]\tLR: 0.005240\tTime 0.144 (0.148)\tData 0.000 (0.001)\tLoss 0.7563 (2.1246)\tTop 1-err 20.3125 (47.6906)\tTop 5-err 3.1250 (23.3750)\n",
            "Epoch: [62/290][599/782]\tLR: 0.005240\tTime 0.150 (0.147)\tData 0.000 (0.000)\tLoss 3.7322 (2.1165)\tTop 1-err 75.0000 (47.6133)\tTop 5-err 40.6250 (23.1055)\n",
            "Epoch: [62/290][699/782]\tLR: 0.005240\tTime 0.152 (0.147)\tData 0.000 (0.000)\tLoss 3.3443 (2.1145)\tTop 1-err 96.8750 (47.6819)\tTop 5-err 71.8750 (23.1830)\n",
            "* Epoch: [62/290]\t Top 1-err 47.659  Top 5-err 23.220\t Train Loss 2.116\n",
            "* Epoch: [62/290]\t Top 1-err 37.170  Top 5-err 11.700\t Test Loss 1.353\n",
            "Current best accuracy (top-1 and 5 error): 34.23 9.62\n",
            "Epoch: [63/290][99/782]\tLR: 0.005279\tTime 0.142 (0.151)\tData 0.000 (0.002)\tLoss 1.2009 (2.1288)\tTop 1-err 35.9375 (48.0625)\tTop 5-err 14.0625 (23.7734)\n",
            "Epoch: [63/290][199/782]\tLR: 0.005279\tTime 0.154 (0.149)\tData 0.000 (0.001)\tLoss 1.8460 (2.2001)\tTop 1-err 42.1875 (48.9297)\tTop 5-err 12.5000 (24.2656)\n",
            "Epoch: [63/290][299/782]\tLR: 0.005279\tTime 0.152 (0.149)\tData 0.000 (0.001)\tLoss 1.2006 (2.1732)\tTop 1-err 34.3750 (48.9010)\tTop 5-err 7.8125 (24.1693)\n",
            "Epoch: [63/290][399/782]\tLR: 0.005279\tTime 0.151 (0.148)\tData 0.000 (0.001)\tLoss 1.1748 (2.1248)\tTop 1-err 32.8125 (47.7324)\tTop 5-err 6.2500 (23.0898)\n",
            "Epoch: [63/290][499/782]\tLR: 0.005279\tTime 0.145 (0.148)\tData 0.000 (0.000)\tLoss 2.2315 (2.0786)\tTop 1-err 48.4375 (46.9016)\tTop 5-err 20.3125 (22.2156)\n",
            "Epoch: [63/290][599/782]\tLR: 0.005279\tTime 0.149 (0.148)\tData 0.000 (0.000)\tLoss 3.3624 (2.0840)\tTop 1-err 56.2500 (46.9245)\tTop 5-err 29.6875 (22.1797)\n",
            "Epoch: [63/290][699/782]\tLR: 0.005279\tTime 0.146 (0.148)\tData 0.000 (0.000)\tLoss 3.6400 (2.0807)\tTop 1-err 73.4375 (46.7232)\tTop 5-err 62.5000 (21.9665)\n",
            "* Epoch: [63/290]\t Top 1-err 46.500  Top 5-err 21.671\t Train Loss 2.067\n",
            "* Epoch: [63/290]\t Top 1-err 35.190  Top 5-err 10.500\t Test Loss 1.264\n",
            "Current best accuracy (top-1 and 5 error): 34.23 9.62\n",
            "Epoch: [64/290][99/782]\tLR: 0.005319\tTime 0.145 (0.150)\tData 0.000 (0.002)\tLoss 1.6785 (1.8754)\tTop 1-err 46.8750 (43.1094)\tTop 5-err 14.0625 (19.7891)\n",
            "Epoch: [64/290][199/782]\tLR: 0.005319\tTime 0.146 (0.150)\tData 0.000 (0.001)\tLoss 3.8876 (1.9762)\tTop 1-err 100.0000 (44.9062)\tTop 5-err 95.3125 (20.8164)\n",
            "Epoch: [64/290][299/782]\tLR: 0.005319\tTime 0.145 (0.149)\tData 0.000 (0.001)\tLoss 3.4376 (1.9839)\tTop 1-err 98.4375 (45.1693)\tTop 5-err 82.8125 (21.2500)\n",
            "Epoch: [64/290][399/782]\tLR: 0.005319\tTime 0.149 (0.149)\tData 0.000 (0.000)\tLoss 4.7269 (1.9562)\tTop 1-err 93.7500 (45.0352)\tTop 5-err 82.8125 (21.2422)\n",
            "Epoch: [64/290][499/782]\tLR: 0.005319\tTime 0.145 (0.149)\tData 0.000 (0.000)\tLoss 1.2361 (2.0291)\tTop 1-err 37.5000 (46.2016)\tTop 5-err 9.3750 (22.1359)\n",
            "Epoch: [64/290][599/782]\tLR: 0.005319\tTime 0.145 (0.149)\tData 0.000 (0.000)\tLoss 1.3052 (2.0364)\tTop 1-err 40.6250 (46.6224)\tTop 5-err 14.0625 (22.5872)\n",
            "Epoch: [64/290][699/782]\tLR: 0.005319\tTime 0.149 (0.149)\tData 0.000 (0.000)\tLoss 1.1025 (2.0189)\tTop 1-err 35.9375 (46.3069)\tTop 5-err 6.2500 (22.3192)\n",
            "* Epoch: [64/290]\t Top 1-err 46.606  Top 5-err 22.547\t Train Loss 2.034\n",
            "* Epoch: [64/290]\t Top 1-err 35.840  Top 5-err 10.180\t Test Loss 1.273\n",
            "Current best accuracy (top-1 and 5 error): 34.23 9.62\n",
            "Epoch: [65/290][99/782]\tLR: 0.005360\tTime 0.144 (0.149)\tData 0.000 (0.002)\tLoss 1.0085 (1.7710)\tTop 1-err 29.6875 (40.6484)\tTop 5-err 7.8125 (16.5156)\n",
            "Epoch: [65/290][199/782]\tLR: 0.005360\tTime 0.149 (0.148)\tData 0.000 (0.001)\tLoss 0.9164 (1.8156)\tTop 1-err 28.1250 (42.0820)\tTop 5-err 6.2500 (17.8086)\n",
            "Epoch: [65/290][299/782]\tLR: 0.005360\tTime 0.146 (0.148)\tData 0.000 (0.001)\tLoss 3.0416 (1.8878)\tTop 1-err 68.7500 (43.0130)\tTop 5-err 28.1250 (19.2370)\n",
            "Epoch: [65/290][399/782]\tLR: 0.005360\tTime 0.144 (0.148)\tData 0.000 (0.001)\tLoss 1.3861 (1.8830)\tTop 1-err 35.9375 (43.1172)\tTop 5-err 12.5000 (19.3477)\n",
            "Epoch: [65/290][499/782]\tLR: 0.005360\tTime 0.148 (0.148)\tData 0.000 (0.000)\tLoss 1.0871 (1.8992)\tTop 1-err 29.6875 (43.3281)\tTop 5-err 7.8125 (19.3953)\n",
            "Epoch: [65/290][599/782]\tLR: 0.005360\tTime 0.144 (0.148)\tData 0.000 (0.000)\tLoss 1.0104 (1.9168)\tTop 1-err 25.0000 (43.7448)\tTop 5-err 7.8125 (19.6185)\n",
            "Epoch: [65/290][699/782]\tLR: 0.005360\tTime 0.145 (0.148)\tData 0.000 (0.000)\tLoss 4.0406 (1.8947)\tTop 1-err 82.8125 (43.4029)\tTop 5-err 62.5000 (19.2489)\n",
            "* Epoch: [65/290]\t Top 1-err 43.505  Top 5-err 19.286\t Train Loss 1.893\n",
            "* Epoch: [65/290]\t Top 1-err 33.840  Top 5-err 9.800\t Test Loss 1.187\n",
            "Current best accuracy (top-1 and 5 error): 33.84 9.8\n",
            "Epoch: [66/290][99/782]\tLR: 0.005401\tTime 0.147 (0.150)\tData 0.000 (0.003)\tLoss 1.1747 (1.7491)\tTop 1-err 34.3750 (40.2812)\tTop 5-err 9.3750 (16.3750)\n",
            "Epoch: [66/290][199/782]\tLR: 0.005401\tTime 0.145 (0.149)\tData 0.000 (0.001)\tLoss 0.8852 (1.9066)\tTop 1-err 23.4375 (43.2656)\tTop 5-err 4.6875 (19.4062)\n",
            "Epoch: [66/290][299/782]\tLR: 0.005401\tTime 0.144 (0.148)\tData 0.000 (0.001)\tLoss 1.0246 (1.9169)\tTop 1-err 31.2500 (43.6536)\tTop 5-err 7.8125 (19.8568)\n",
            "Epoch: [66/290][399/782]\tLR: 0.005401\tTime 0.152 (0.148)\tData 0.000 (0.001)\tLoss 2.4435 (1.8547)\tTop 1-err 37.5000 (42.8906)\tTop 5-err 12.5000 (19.0664)\n",
            "Epoch: [66/290][499/782]\tLR: 0.005401\tTime 0.151 (0.148)\tData 0.000 (0.001)\tLoss 3.9936 (1.9185)\tTop 1-err 90.6250 (43.8219)\tTop 5-err 64.0625 (19.7891)\n",
            "Epoch: [66/290][599/782]\tLR: 0.005401\tTime 0.153 (0.148)\tData 0.000 (0.000)\tLoss 1.2187 (1.9277)\tTop 1-err 37.5000 (43.8750)\tTop 5-err 7.8125 (19.6823)\n",
            "Epoch: [66/290][699/782]\tLR: 0.005401\tTime 0.144 (0.148)\tData 0.000 (0.000)\tLoss 3.0006 (1.9433)\tTop 1-err 54.6875 (44.0491)\tTop 5-err 35.9375 (19.7556)\n",
            "* Epoch: [66/290]\t Top 1-err 44.019  Top 5-err 19.795\t Train Loss 1.935\n",
            "* Epoch: [66/290]\t Top 1-err 33.580  Top 5-err 9.760\t Test Loss 1.188\n",
            "Current best accuracy (top-1 and 5 error): 33.58 9.76\n",
            "Epoch: [67/290][99/782]\tLR: 0.005443\tTime 0.147 (0.149)\tData 0.000 (0.002)\tLoss 3.9832 (2.1247)\tTop 1-err 85.9375 (47.0625)\tTop 5-err 65.6250 (23.7266)\n",
            "Epoch: [67/290][199/782]\tLR: 0.005443\tTime 0.152 (0.148)\tData 0.000 (0.001)\tLoss 0.9868 (2.0095)\tTop 1-err 29.6875 (45.0586)\tTop 5-err 6.2500 (21.1484)\n",
            "Epoch: [67/290][299/782]\tLR: 0.005443\tTime 0.150 (0.148)\tData 0.000 (0.001)\tLoss 3.0778 (2.0025)\tTop 1-err 57.8125 (44.6927)\tTop 5-err 21.8750 (20.5885)\n",
            "Epoch: [67/290][399/782]\tLR: 0.005443\tTime 0.145 (0.148)\tData 0.000 (0.001)\tLoss 3.9529 (2.0155)\tTop 1-err 73.4375 (45.2207)\tTop 5-err 59.3750 (21.3320)\n",
            "Epoch: [67/290][499/782]\tLR: 0.005443\tTime 0.149 (0.148)\tData 0.000 (0.001)\tLoss 1.0649 (2.0264)\tTop 1-err 23.4375 (45.3453)\tTop 5-err 6.2500 (21.3313)\n",
            "Epoch: [67/290][599/782]\tLR: 0.005443\tTime 0.152 (0.148)\tData 0.000 (0.000)\tLoss 3.4891 (2.0324)\tTop 1-err 73.4375 (45.6211)\tTop 5-err 46.8750 (21.4935)\n",
            "Epoch: [67/290][699/782]\tLR: 0.005443\tTime 0.155 (0.148)\tData 0.000 (0.000)\tLoss 0.8254 (2.0212)\tTop 1-err 21.8750 (45.6540)\tTop 5-err 4.6875 (21.5547)\n",
            "* Epoch: [67/290]\t Top 1-err 45.846  Top 5-err 21.638\t Train Loss 2.034\n",
            "* Epoch: [67/290]\t Top 1-err 35.020  Top 5-err 10.420\t Test Loss 1.245\n",
            "Current best accuracy (top-1 and 5 error): 33.58 9.76\n",
            "Epoch: [68/290][99/782]\tLR: 0.005486\tTime 0.146 (0.150)\tData 0.000 (0.002)\tLoss 4.0525 (2.0131)\tTop 1-err 93.7500 (46.0859)\tTop 5-err 68.7500 (22.1875)\n",
            "Epoch: [68/290][199/782]\tLR: 0.005486\tTime 0.145 (0.148)\tData 0.000 (0.001)\tLoss 3.5798 (2.0168)\tTop 1-err 81.2500 (45.6016)\tTop 5-err 45.3125 (21.6914)\n",
            "Epoch: [68/290][299/782]\tLR: 0.005486\tTime 0.144 (0.148)\tData 0.000 (0.001)\tLoss 1.2826 (2.0235)\tTop 1-err 37.5000 (45.6536)\tTop 5-err 9.3750 (21.5703)\n",
            "Epoch: [68/290][399/782]\tLR: 0.005486\tTime 0.152 (0.148)\tData 0.000 (0.001)\tLoss 1.1958 (1.9910)\tTop 1-err 35.9375 (45.2441)\tTop 5-err 6.2500 (21.1523)\n",
            "Epoch: [68/290][499/782]\tLR: 0.005486\tTime 0.146 (0.148)\tData 0.000 (0.000)\tLoss 3.1957 (1.9815)\tTop 1-err 70.3125 (44.9234)\tTop 5-err 40.6250 (20.9109)\n",
            "Epoch: [68/290][599/782]\tLR: 0.005486\tTime 0.147 (0.148)\tData 0.000 (0.000)\tLoss 1.2658 (1.9867)\tTop 1-err 32.8125 (44.9622)\tTop 5-err 14.0625 (20.8659)\n",
            "Epoch: [68/290][699/782]\tLR: 0.005486\tTime 0.145 (0.148)\tData 0.000 (0.000)\tLoss 3.4962 (2.0012)\tTop 1-err 68.7500 (45.2076)\tTop 5-err 39.0625 (20.9721)\n",
            "* Epoch: [68/290]\t Top 1-err 45.506  Top 5-err 21.247\t Train Loss 2.018\n",
            "* Epoch: [68/290]\t Top 1-err 37.880  Top 5-err 12.190\t Test Loss 1.437\n",
            "Current best accuracy (top-1 and 5 error): 33.58 9.76\n",
            "Epoch: [69/290][99/782]\tLR: 0.005529\tTime 0.151 (0.151)\tData 0.000 (0.002)\tLoss 1.0185 (1.9064)\tTop 1-err 26.5625 (43.9062)\tTop 5-err 9.3750 (20.6250)\n",
            "Epoch: [69/290][199/782]\tLR: 0.005529\tTime 0.144 (0.150)\tData 0.000 (0.001)\tLoss 1.1514 (2.0278)\tTop 1-err 32.8125 (46.0938)\tTop 5-err 6.2500 (21.8867)\n",
            "Epoch: [69/290][299/782]\tLR: 0.005529\tTime 0.146 (0.149)\tData 0.000 (0.001)\tLoss 3.2402 (2.0317)\tTop 1-err 65.6250 (45.9297)\tTop 5-err 42.1875 (21.7995)\n",
            "Epoch: [69/290][399/782]\tLR: 0.005529\tTime 0.144 (0.148)\tData 0.000 (0.001)\tLoss 0.8887 (2.0015)\tTop 1-err 23.4375 (45.2383)\tTop 5-err 4.6875 (20.9141)\n",
            "Epoch: [69/290][499/782]\tLR: 0.005529\tTime 0.152 (0.148)\tData 0.000 (0.001)\tLoss 4.7116 (1.9836)\tTop 1-err 92.1875 (44.7687)\tTop 5-err 79.6875 (20.6547)\n",
            "Epoch: [69/290][599/782]\tLR: 0.005529\tTime 0.143 (0.148)\tData 0.000 (0.000)\tLoss 1.1346 (1.9841)\tTop 1-err 32.8125 (44.8841)\tTop 5-err 6.2500 (20.8021)\n",
            "Epoch: [69/290][699/782]\tLR: 0.005529\tTime 0.157 (0.148)\tData 0.000 (0.000)\tLoss 0.9891 (1.9813)\tTop 1-err 29.6875 (44.7176)\tTop 5-err 7.8125 (20.5670)\n",
            "* Epoch: [69/290]\t Top 1-err 44.848  Top 5-err 20.623\t Train Loss 1.993\n",
            "* Epoch: [69/290]\t Top 1-err 34.530  Top 5-err 9.680\t Test Loss 1.219\n",
            "Current best accuracy (top-1 and 5 error): 33.58 9.76\n",
            "Epoch: [70/290][99/782]\tLR: 0.005573\tTime 0.156 (0.151)\tData 0.000 (0.002)\tLoss 3.6441 (2.1935)\tTop 1-err 87.5000 (48.9297)\tTop 5-err 67.1875 (24.3828)\n",
            "Epoch: [70/290][199/782]\tLR: 0.005573\tTime 0.145 (0.150)\tData 0.000 (0.001)\tLoss 1.0481 (2.0056)\tTop 1-err 25.0000 (45.7461)\tTop 5-err 6.2500 (21.5352)\n",
            "Epoch: [70/290][299/782]\tLR: 0.005573\tTime 0.153 (0.149)\tData 0.000 (0.001)\tLoss 0.9292 (2.0068)\tTop 1-err 29.6875 (45.8047)\tTop 5-err 6.2500 (22.0312)\n",
            "Epoch: [70/290][399/782]\tLR: 0.005573\tTime 0.144 (0.149)\tData 0.000 (0.001)\tLoss 1.1834 (1.9995)\tTop 1-err 37.5000 (45.6133)\tTop 5-err 9.3750 (22.0098)\n",
            "Epoch: [70/290][499/782]\tLR: 0.005573\tTime 0.146 (0.148)\tData 0.000 (0.001)\tLoss 3.9152 (1.9613)\tTop 1-err 71.8750 (44.8641)\tTop 5-err 39.0625 (21.1750)\n",
            "Epoch: [70/290][599/782]\tLR: 0.005573\tTime 0.150 (0.148)\tData 0.000 (0.000)\tLoss 0.8555 (1.9809)\tTop 1-err 28.1250 (45.1263)\tTop 5-err 4.6875 (21.3490)\n",
            "Epoch: [70/290][699/782]\tLR: 0.005573\tTime 0.147 (0.148)\tData 0.000 (0.000)\tLoss 3.1861 (1.9613)\tTop 1-err 73.4375 (44.6674)\tTop 5-err 39.0625 (20.8438)\n",
            "* Epoch: [70/290]\t Top 1-err 44.527  Top 5-err 20.689\t Train Loss 1.967\n",
            "* Epoch: [70/290]\t Top 1-err 34.450  Top 5-err 10.720\t Test Loss 1.287\n",
            "Current best accuracy (top-1 and 5 error): 33.58 9.76\n",
            "Epoch: [71/290][99/782]\tLR: 0.005617\tTime 0.154 (0.150)\tData 0.000 (0.002)\tLoss 1.3788 (1.8685)\tTop 1-err 50.0000 (42.6016)\tTop 5-err 6.2500 (19.4062)\n",
            "Epoch: [71/290][199/782]\tLR: 0.005617\tTime 0.146 (0.149)\tData 0.000 (0.001)\tLoss 3.7080 (2.0043)\tTop 1-err 75.0000 (44.3359)\tTop 5-err 53.1250 (20.8008)\n",
            "Epoch: [71/290][299/782]\tLR: 0.005617\tTime 0.144 (0.149)\tData 0.000 (0.001)\tLoss 0.9822 (1.9799)\tTop 1-err 29.6875 (43.9974)\tTop 5-err 3.1250 (20.5391)\n",
            "Epoch: [71/290][399/782]\tLR: 0.005617\tTime 0.150 (0.148)\tData 0.000 (0.000)\tLoss 0.7864 (2.0025)\tTop 1-err 20.3125 (44.7539)\tTop 5-err 6.2500 (21.1035)\n",
            "Epoch: [71/290][499/782]\tLR: 0.005617\tTime 0.149 (0.148)\tData 0.000 (0.000)\tLoss 1.4017 (1.9883)\tTop 1-err 34.3750 (44.6859)\tTop 5-err 14.0625 (20.8984)\n",
            "Epoch: [71/290][599/782]\tLR: 0.005617\tTime 0.150 (0.148)\tData 0.000 (0.000)\tLoss 0.9286 (2.0225)\tTop 1-err 25.0000 (45.5013)\tTop 5-err 3.1250 (21.5169)\n",
            "Epoch: [71/290][699/782]\tLR: 0.005617\tTime 0.145 (0.148)\tData 0.000 (0.000)\tLoss 4.0528 (2.0237)\tTop 1-err 90.6250 (45.5792)\tTop 5-err 73.4375 (21.6172)\n",
            "* Epoch: [71/290]\t Top 1-err 45.735  Top 5-err 21.692\t Train Loss 2.030\n",
            "* Epoch: [71/290]\t Top 1-err 33.620  Top 5-err 9.920\t Test Loss 1.203\n",
            "Current best accuracy (top-1 and 5 error): 33.58 9.76\n",
            "Epoch: [72/290][99/782]\tLR: 0.005662\tTime 0.149 (0.151)\tData 0.000 (0.002)\tLoss 1.1154 (2.0611)\tTop 1-err 31.2500 (46.8906)\tTop 5-err 7.8125 (21.5547)\n",
            "Epoch: [72/290][199/782]\tLR: 0.005662\tTime 0.149 (0.150)\tData 0.000 (0.001)\tLoss 1.0452 (2.0658)\tTop 1-err 31.2500 (46.3164)\tTop 5-err 4.6875 (21.4492)\n",
            "Epoch: [72/290][299/782]\tLR: 0.005662\tTime 0.146 (0.149)\tData 0.000 (0.001)\tLoss 3.1013 (2.0249)\tTop 1-err 73.4375 (45.6589)\tTop 5-err 46.8750 (21.5885)\n",
            "Epoch: [72/290][399/782]\tLR: 0.005662\tTime 0.152 (0.148)\tData 0.000 (0.001)\tLoss 0.7396 (2.0374)\tTop 1-err 20.3125 (45.9863)\tTop 5-err 3.1250 (21.8652)\n",
            "Epoch: [72/290][499/782]\tLR: 0.005662\tTime 0.145 (0.148)\tData 0.000 (0.000)\tLoss 1.1677 (2.0311)\tTop 1-err 29.6875 (45.9141)\tTop 5-err 7.8125 (22.0672)\n",
            "Epoch: [72/290][599/782]\tLR: 0.005662\tTime 0.149 (0.148)\tData 0.000 (0.000)\tLoss 2.3240 (2.0097)\tTop 1-err 50.0000 (45.2995)\tTop 5-err 18.7500 (21.4648)\n",
            "Epoch: [72/290][699/782]\tLR: 0.005662\tTime 0.147 (0.148)\tData 0.000 (0.000)\tLoss 4.0813 (1.9979)\tTop 1-err 84.3750 (44.9342)\tTop 5-err 59.3750 (21.1283)\n",
            "* Epoch: [72/290]\t Top 1-err 44.842  Top 5-err 20.917\t Train Loss 1.987\n",
            "* Epoch: [72/290]\t Top 1-err 33.260  Top 5-err 9.120\t Test Loss 1.166\n",
            "Current best accuracy (top-1 and 5 error): 33.26 9.12\n",
            "Epoch: [73/290][99/782]\tLR: 0.005707\tTime 0.145 (0.151)\tData 0.000 (0.002)\tLoss 1.0562 (1.9519)\tTop 1-err 32.8125 (43.8125)\tTop 5-err 4.6875 (20.3594)\n",
            "Epoch: [73/290][199/782]\tLR: 0.005707\tTime 0.153 (0.150)\tData 0.000 (0.001)\tLoss 3.6844 (1.8742)\tTop 1-err 78.1250 (42.7305)\tTop 5-err 57.8125 (19.2930)\n",
            "Epoch: [73/290][299/782]\tLR: 0.005707\tTime 0.147 (0.149)\tData 0.000 (0.001)\tLoss 4.1874 (1.9165)\tTop 1-err 87.5000 (43.3099)\tTop 5-err 71.8750 (20.1484)\n",
            "Epoch: [73/290][399/782]\tLR: 0.005707\tTime 0.147 (0.149)\tData 0.000 (0.000)\tLoss 3.4400 (1.9316)\tTop 1-err 87.5000 (43.5547)\tTop 5-err 75.0000 (19.9844)\n",
            "Epoch: [73/290][499/782]\tLR: 0.005707\tTime 0.145 (0.149)\tData 0.000 (0.000)\tLoss 0.9204 (1.9609)\tTop 1-err 21.8750 (44.1969)\tTop 5-err 3.1250 (20.5063)\n",
            "Epoch: [73/290][599/782]\tLR: 0.005707\tTime 0.144 (0.148)\tData 0.000 (0.000)\tLoss 1.2548 (1.9444)\tTop 1-err 35.9375 (44.1341)\tTop 5-err 14.0625 (20.3789)\n",
            "Epoch: [73/290][699/782]\tLR: 0.005707\tTime 0.144 (0.148)\tData 0.000 (0.000)\tLoss 1.2260 (1.9202)\tTop 1-err 37.5000 (43.7489)\tTop 5-err 15.6250 (20.0859)\n",
            "* Epoch: [73/290]\t Top 1-err 43.430  Top 5-err 19.756\t Train Loss 1.905\n",
            "* Epoch: [73/290]\t Top 1-err 34.680  Top 5-err 10.080\t Test Loss 1.225\n",
            "Current best accuracy (top-1 and 5 error): 33.26 9.12\n",
            "Epoch: [74/290][99/782]\tLR: 0.005754\tTime 0.145 (0.149)\tData 0.000 (0.002)\tLoss 3.4857 (2.1861)\tTop 1-err 81.2500 (48.7422)\tTop 5-err 50.0000 (25.1250)\n",
            "Epoch: [74/290][199/782]\tLR: 0.005754\tTime 0.144 (0.148)\tData 0.000 (0.001)\tLoss 0.8031 (2.0891)\tTop 1-err 23.4375 (46.9961)\tTop 5-err 4.6875 (22.8438)\n",
            "Epoch: [74/290][299/782]\tLR: 0.005754\tTime 0.152 (0.148)\tData 0.000 (0.001)\tLoss 1.0993 (2.0149)\tTop 1-err 28.1250 (45.3411)\tTop 5-err 6.2500 (21.3229)\n",
            "Epoch: [74/290][399/782]\tLR: 0.005754\tTime 0.145 (0.148)\tData 0.000 (0.001)\tLoss 0.9582 (2.0008)\tTop 1-err 29.6875 (45.4277)\tTop 5-err 1.5625 (21.5781)\n",
            "Epoch: [74/290][499/782]\tLR: 0.005754\tTime 0.145 (0.148)\tData 0.000 (0.001)\tLoss 1.5209 (2.0134)\tTop 1-err 42.1875 (45.6578)\tTop 5-err 12.5000 (21.6781)\n",
            "Epoch: [74/290][599/782]\tLR: 0.005754\tTime 0.144 (0.148)\tData 0.000 (0.000)\tLoss 1.5855 (2.0214)\tTop 1-err 48.4375 (45.6940)\tTop 5-err 18.7500 (21.7839)\n",
            "Epoch: [74/290][699/782]\tLR: 0.005754\tTime 0.153 (0.148)\tData 0.000 (0.000)\tLoss 4.1460 (2.0261)\tTop 1-err 89.0625 (45.7243)\tTop 5-err 71.8750 (21.6864)\n",
            "* Epoch: [74/290]\t Top 1-err 45.652  Top 5-err 21.640\t Train Loss 2.025\n",
            "* Epoch: [74/290]\t Top 1-err 32.930  Top 5-err 9.110\t Test Loss 1.159\n",
            "Current best accuracy (top-1 and 5 error): 32.93 9.11\n",
            "Epoch: [75/290][99/782]\tLR: 0.005800\tTime 0.153 (0.150)\tData 0.000 (0.002)\tLoss 3.5959 (1.9304)\tTop 1-err 95.3125 (42.9688)\tTop 5-err 81.2500 (18.2578)\n",
            "Epoch: [75/290][199/782]\tLR: 0.005800\tTime 0.146 (0.150)\tData 0.000 (0.001)\tLoss 3.4570 (1.9462)\tTop 1-err 65.6250 (43.7266)\tTop 5-err 31.2500 (19.3477)\n",
            "Epoch: [75/290][299/782]\tLR: 0.005800\tTime 0.146 (0.149)\tData 0.000 (0.001)\tLoss 1.5988 (2.0193)\tTop 1-err 34.3750 (45.3464)\tTop 5-err 9.3750 (21.2969)\n",
            "Epoch: [75/290][399/782]\tLR: 0.005800\tTime 0.147 (0.149)\tData 0.000 (0.001)\tLoss 1.6073 (1.9746)\tTop 1-err 42.1875 (44.4531)\tTop 5-err 9.3750 (20.4414)\n",
            "Epoch: [75/290][499/782]\tLR: 0.005800\tTime 0.146 (0.148)\tData 0.000 (0.000)\tLoss 2.5272 (1.9748)\tTop 1-err 39.0625 (44.4094)\tTop 5-err 15.6250 (20.4438)\n",
            "Epoch: [75/290][599/782]\tLR: 0.005800\tTime 0.152 (0.148)\tData 0.000 (0.000)\tLoss 1.1316 (1.9762)\tTop 1-err 34.3750 (44.3711)\tTop 5-err 4.6875 (20.3464)\n",
            "Epoch: [75/290][699/782]\tLR: 0.005800\tTime 0.146 (0.148)\tData 0.000 (0.000)\tLoss 3.5769 (1.9730)\tTop 1-err 78.1250 (44.2667)\tTop 5-err 50.0000 (20.1830)\n",
            "* Epoch: [75/290]\t Top 1-err 44.234  Top 5-err 20.291\t Train Loss 1.964\n",
            "* Epoch: [75/290]\t Top 1-err 33.560  Top 5-err 10.000\t Test Loss 1.191\n",
            "Current best accuracy (top-1 and 5 error): 32.93 9.11\n",
            "Epoch: [76/290][99/782]\tLR: 0.005848\tTime 0.153 (0.150)\tData 0.000 (0.002)\tLoss 0.9501 (1.8379)\tTop 1-err 28.1250 (41.5547)\tTop 5-err 4.6875 (17.9609)\n",
            "Epoch: [76/290][199/782]\tLR: 0.005848\tTime 0.146 (0.149)\tData 0.000 (0.001)\tLoss 3.9428 (1.7632)\tTop 1-err 84.3750 (39.8633)\tTop 5-err 57.8125 (16.6992)\n",
            "Epoch: [76/290][299/782]\tLR: 0.005848\tTime 0.145 (0.148)\tData 0.000 (0.001)\tLoss 2.5799 (1.8706)\tTop 1-err 39.0625 (42.0286)\tTop 5-err 9.3750 (18.2135)\n",
            "Epoch: [76/290][399/782]\tLR: 0.005848\tTime 0.152 (0.148)\tData 0.000 (0.000)\tLoss 0.9502 (1.8665)\tTop 1-err 31.2500 (42.1836)\tTop 5-err 4.6875 (18.4902)\n",
            "Epoch: [76/290][499/782]\tLR: 0.005848\tTime 0.150 (0.148)\tData 0.000 (0.000)\tLoss 1.2673 (1.8977)\tTop 1-err 37.5000 (42.8453)\tTop 5-err 7.8125 (19.1016)\n",
            "Epoch: [76/290][599/782]\tLR: 0.005848\tTime 0.145 (0.148)\tData 0.000 (0.000)\tLoss 4.0716 (1.9222)\tTop 1-err 90.6250 (43.2357)\tTop 5-err 60.9375 (19.5286)\n",
            "Epoch: [76/290][699/782]\tLR: 0.005848\tTime 0.145 (0.148)\tData 0.000 (0.000)\tLoss 0.9658 (1.9388)\tTop 1-err 34.3750 (43.7321)\tTop 5-err 6.2500 (19.9252)\n",
            "* Epoch: [76/290]\t Top 1-err 43.619  Top 5-err 19.696\t Train Loss 1.935\n",
            "* Epoch: [76/290]\t Top 1-err 34.740  Top 5-err 9.910\t Test Loss 1.240\n",
            "Current best accuracy (top-1 and 5 error): 32.93 9.11\n",
            "Epoch: [77/290][99/782]\tLR: 0.005896\tTime 0.150 (0.150)\tData 0.000 (0.002)\tLoss 0.8353 (1.8786)\tTop 1-err 23.4375 (42.8203)\tTop 5-err 6.2500 (20.4844)\n",
            "Epoch: [77/290][199/782]\tLR: 0.005896\tTime 0.146 (0.149)\tData 0.000 (0.001)\tLoss 1.3967 (1.8543)\tTop 1-err 34.3750 (42.7266)\tTop 5-err 4.6875 (19.6953)\n",
            "Epoch: [77/290][299/782]\tLR: 0.005896\tTime 0.147 (0.149)\tData 0.000 (0.001)\tLoss 0.9723 (1.8275)\tTop 1-err 28.1250 (42.3073)\tTop 5-err 6.2500 (19.1224)\n",
            "Epoch: [77/290][399/782]\tLR: 0.005896\tTime 0.144 (0.148)\tData 0.000 (0.001)\tLoss 0.7452 (1.8398)\tTop 1-err 20.3125 (42.4492)\tTop 5-err 7.8125 (19.2988)\n",
            "Epoch: [77/290][499/782]\tLR: 0.005896\tTime 0.144 (0.148)\tData 0.000 (0.001)\tLoss 3.4616 (1.8363)\tTop 1-err 73.4375 (42.1250)\tTop 5-err 46.8750 (18.8156)\n",
            "Epoch: [77/290][599/782]\tLR: 0.005896\tTime 0.144 (0.148)\tData 0.000 (0.000)\tLoss 0.9712 (1.8809)\tTop 1-err 21.8750 (42.9622)\tTop 5-err 6.2500 (19.5208)\n",
            "Epoch: [77/290][699/782]\tLR: 0.005896\tTime 0.151 (0.148)\tData 0.000 (0.000)\tLoss 1.0024 (1.8808)\tTop 1-err 32.8125 (42.9989)\tTop 5-err 4.6875 (19.3795)\n",
            "* Epoch: [77/290]\t Top 1-err 43.034  Top 5-err 19.304\t Train Loss 1.895\n",
            "* Epoch: [77/290]\t Top 1-err 33.560  Top 5-err 9.740\t Test Loss 1.183\n",
            "Current best accuracy (top-1 and 5 error): 32.93 9.11\n",
            "Epoch: [78/290][99/782]\tLR: 0.005944\tTime 0.149 (0.150)\tData 0.000 (0.002)\tLoss 2.6557 (2.0026)\tTop 1-err 50.0000 (44.6328)\tTop 5-err 17.1875 (20.6328)\n",
            "Epoch: [78/290][199/782]\tLR: 0.005944\tTime 0.145 (0.149)\tData 0.000 (0.001)\tLoss 2.0672 (1.9133)\tTop 1-err 45.3125 (43.4453)\tTop 5-err 17.1875 (19.8359)\n",
            "Epoch: [78/290][299/782]\tLR: 0.005944\tTime 0.156 (0.148)\tData 0.000 (0.001)\tLoss 2.4004 (1.8889)\tTop 1-err 57.8125 (42.7943)\tTop 5-err 23.4375 (19.0729)\n",
            "Epoch: [78/290][399/782]\tLR: 0.005944\tTime 0.145 (0.148)\tData 0.000 (0.001)\tLoss 2.6100 (1.8642)\tTop 1-err 45.3125 (42.4805)\tTop 5-err 14.0625 (18.7051)\n",
            "Epoch: [78/290][499/782]\tLR: 0.005944\tTime 0.145 (0.148)\tData 0.000 (0.001)\tLoss 3.4428 (1.8748)\tTop 1-err 68.7500 (42.6625)\tTop 5-err 35.9375 (18.8047)\n",
            "Epoch: [78/290][599/782]\tLR: 0.005944\tTime 0.146 (0.148)\tData 0.000 (0.000)\tLoss 4.3123 (1.8749)\tTop 1-err 75.0000 (42.5404)\tTop 5-err 42.1875 (18.6094)\n",
            "Epoch: [78/290][699/782]\tLR: 0.005944\tTime 0.154 (0.148)\tData 0.000 (0.000)\tLoss 2.8897 (1.8908)\tTop 1-err 50.0000 (42.7589)\tTop 5-err 26.5625 (18.8850)\n",
            "* Epoch: [78/290]\t Top 1-err 43.061  Top 5-err 19.116\t Train Loss 1.904\n",
            "* Epoch: [78/290]\t Top 1-err 32.920  Top 5-err 9.200\t Test Loss 1.156\n",
            "Current best accuracy (top-1 and 5 error): 32.92 9.2\n",
            "Epoch: [79/290][99/782]\tLR: 0.005994\tTime 0.145 (0.150)\tData 0.000 (0.002)\tLoss 1.6167 (1.9915)\tTop 1-err 43.7500 (45.9453)\tTop 5-err 10.9375 (22.7578)\n",
            "Epoch: [79/290][199/782]\tLR: 0.005994\tTime 0.145 (0.149)\tData 0.000 (0.001)\tLoss 2.1781 (1.9511)\tTop 1-err 54.6875 (44.3828)\tTop 5-err 17.1875 (20.7305)\n",
            "Epoch: [79/290][299/782]\tLR: 0.005994\tTime 0.151 (0.148)\tData 0.000 (0.001)\tLoss 0.9374 (1.9403)\tTop 1-err 26.5625 (43.9401)\tTop 5-err 3.1250 (20.0885)\n",
            "Epoch: [79/290][399/782]\tLR: 0.005994\tTime 0.145 (0.148)\tData 0.000 (0.001)\tLoss 0.9121 (1.9303)\tTop 1-err 25.0000 (43.7207)\tTop 5-err 3.1250 (19.7363)\n",
            "Epoch: [79/290][499/782]\tLR: 0.005994\tTime 0.145 (0.148)\tData 0.000 (0.001)\tLoss 3.0490 (1.9005)\tTop 1-err 43.7500 (43.1656)\tTop 5-err 20.3125 (19.4500)\n",
            "Epoch: [79/290][599/782]\tLR: 0.005994\tTime 0.145 (0.148)\tData 0.000 (0.000)\tLoss 1.0064 (1.8980)\tTop 1-err 32.8125 (43.1432)\tTop 5-err 3.1250 (19.5820)\n",
            "Epoch: [79/290][699/782]\tLR: 0.005994\tTime 0.145 (0.148)\tData 0.000 (0.000)\tLoss 3.2099 (1.9018)\tTop 1-err 56.2500 (43.0826)\tTop 5-err 29.6875 (19.6016)\n",
            "* Epoch: [79/290]\t Top 1-err 43.016  Top 5-err 19.564\t Train Loss 1.896\n",
            "* Epoch: [79/290]\t Top 1-err 33.450  Top 5-err 9.450\t Test Loss 1.195\n",
            "Current best accuracy (top-1 and 5 error): 32.92 9.2\n",
            "Epoch: [80/290][99/782]\tLR: 0.006043\tTime 0.145 (0.150)\tData 0.000 (0.002)\tLoss 3.3674 (1.7577)\tTop 1-err 76.5625 (40.7188)\tTop 5-err 46.8750 (17.6719)\n",
            "Epoch: [80/290][199/782]\tLR: 0.006043\tTime 0.145 (0.149)\tData 0.000 (0.001)\tLoss 2.8437 (1.7874)\tTop 1-err 50.0000 (41.4492)\tTop 5-err 20.3125 (18.0117)\n",
            "Epoch: [80/290][299/782]\tLR: 0.006043\tTime 0.154 (0.148)\tData 0.000 (0.001)\tLoss 0.9619 (1.8274)\tTop 1-err 28.1250 (41.8828)\tTop 5-err 4.6875 (18.5625)\n",
            "Epoch: [80/290][399/782]\tLR: 0.006043\tTime 0.146 (0.149)\tData 0.000 (0.001)\tLoss 0.9341 (1.8957)\tTop 1-err 25.0000 (43.2324)\tTop 5-err 4.6875 (19.7207)\n",
            "Epoch: [80/290][499/782]\tLR: 0.006043\tTime 0.144 (0.149)\tData 0.000 (0.001)\tLoss 2.9680 (1.9185)\tTop 1-err 98.4375 (43.6688)\tTop 5-err 81.2500 (20.1078)\n",
            "Epoch: [80/290][599/782]\tLR: 0.006043\tTime 0.149 (0.148)\tData 0.000 (0.000)\tLoss 4.9013 (1.9419)\tTop 1-err 98.4375 (44.3073)\tTop 5-err 89.0625 (20.8385)\n",
            "Epoch: [80/290][699/782]\tLR: 0.006043\tTime 0.152 (0.148)\tData 0.000 (0.000)\tLoss 0.8946 (1.9458)\tTop 1-err 23.4375 (44.0960)\tTop 5-err 6.2500 (20.4520)\n",
            "* Epoch: [80/290]\t Top 1-err 44.275  Top 5-err 20.562\t Train Loss 1.953\n",
            "* Epoch: [80/290]\t Top 1-err 33.480  Top 5-err 9.590\t Test Loss 1.197\n",
            "Current best accuracy (top-1 and 5 error): 32.92 9.2\n",
            "Epoch: [81/290][99/782]\tLR: 0.006094\tTime 0.151 (0.149)\tData 0.000 (0.002)\tLoss 3.2528 (1.8736)\tTop 1-err 57.8125 (42.0234)\tTop 5-err 29.6875 (19.1953)\n",
            "Epoch: [81/290][199/782]\tLR: 0.006094\tTime 0.145 (0.149)\tData 0.000 (0.001)\tLoss 3.7450 (1.8104)\tTop 1-err 84.3750 (40.1250)\tTop 5-err 60.9375 (16.8320)\n",
            "Epoch: [81/290][299/782]\tLR: 0.006094\tTime 0.147 (0.148)\tData 0.000 (0.001)\tLoss 3.7925 (1.8697)\tTop 1-err 82.8125 (41.0573)\tTop 5-err 51.5625 (17.5964)\n",
            "Epoch: [81/290][399/782]\tLR: 0.006094\tTime 0.155 (0.148)\tData 0.000 (0.001)\tLoss 1.7018 (1.9211)\tTop 1-err 43.7500 (42.3633)\tTop 5-err 10.9375 (18.8750)\n",
            "Epoch: [81/290][499/782]\tLR: 0.006094\tTime 0.144 (0.148)\tData 0.000 (0.000)\tLoss 0.9875 (1.9271)\tTop 1-err 25.0000 (42.5719)\tTop 5-err 7.8125 (18.9250)\n",
            "Epoch: [81/290][599/782]\tLR: 0.006094\tTime 0.145 (0.148)\tData 0.000 (0.000)\tLoss 3.3230 (1.9436)\tTop 1-err 76.5625 (42.9766)\tTop 5-err 46.8750 (19.3216)\n",
            "Epoch: [81/290][699/782]\tLR: 0.006094\tTime 0.145 (0.148)\tData 0.000 (0.000)\tLoss 0.7877 (1.9435)\tTop 1-err 23.4375 (43.1797)\tTop 5-err 1.5625 (19.4007)\n",
            "* Epoch: [81/290]\t Top 1-err 43.074  Top 5-err 19.362\t Train Loss 1.937\n",
            "* Epoch: [81/290]\t Top 1-err 32.510  Top 5-err 8.700\t Test Loss 1.124\n",
            "Current best accuracy (top-1 and 5 error): 32.51 8.7\n",
            "Epoch: [82/290][99/782]\tLR: 0.006145\tTime 0.145 (0.151)\tData 0.000 (0.002)\tLoss 3.0351 (2.0148)\tTop 1-err 59.3750 (45.3047)\tTop 5-err 29.6875 (20.9141)\n",
            "Epoch: [82/290][199/782]\tLR: 0.006145\tTime 0.141 (0.150)\tData 0.000 (0.001)\tLoss 1.3120 (1.8956)\tTop 1-err 40.6250 (43.1133)\tTop 5-err 10.9375 (19.3594)\n",
            "Epoch: [82/290][299/782]\tLR: 0.006145\tTime 0.143 (0.149)\tData 0.000 (0.001)\tLoss 0.9644 (1.8780)\tTop 1-err 29.6875 (42.6146)\tTop 5-err 3.1250 (19.1172)\n",
            "Epoch: [82/290][399/782]\tLR: 0.006145\tTime 0.145 (0.149)\tData 0.000 (0.001)\tLoss 1.0091 (1.9010)\tTop 1-err 26.5625 (43.0703)\tTop 5-err 7.8125 (19.4609)\n",
            "Epoch: [82/290][499/782]\tLR: 0.006145\tTime 0.159 (0.149)\tData 0.000 (0.000)\tLoss 5.9586 (1.9394)\tTop 1-err 93.7500 (43.4984)\tTop 5-err 89.0625 (19.8172)\n",
            "Epoch: [82/290][599/782]\tLR: 0.006145\tTime 0.145 (0.149)\tData 0.000 (0.000)\tLoss 0.7202 (1.9422)\tTop 1-err 21.8750 (43.5391)\tTop 5-err 3.1250 (19.8529)\n",
            "Epoch: [82/290][699/782]\tLR: 0.006145\tTime 0.146 (0.148)\tData 0.000 (0.000)\tLoss 1.0286 (1.9189)\tTop 1-err 31.2500 (43.1150)\tTop 5-err 10.9375 (19.5379)\n",
            "* Epoch: [82/290]\t Top 1-err 43.060  Top 5-err 19.441\t Train Loss 1.911\n",
            "* Epoch: [82/290]\t Top 1-err 31.480  Top 5-err 8.520\t Test Loss 1.120\n",
            "Current best accuracy (top-1 and 5 error): 31.48 8.52\n",
            "Epoch: [83/290][99/782]\tLR: 0.006196\tTime 0.148 (0.151)\tData 0.000 (0.002)\tLoss 2.7660 (1.6709)\tTop 1-err 42.1875 (37.6953)\tTop 5-err 12.5000 (15.5000)\n",
            "Epoch: [83/290][199/782]\tLR: 0.006196\tTime 0.145 (0.149)\tData 0.000 (0.001)\tLoss 0.7568 (1.7539)\tTop 1-err 20.3125 (39.5156)\tTop 5-err 4.6875 (16.7852)\n",
            "Epoch: [83/290][299/782]\tLR: 0.006196\tTime 0.145 (0.148)\tData 0.000 (0.001)\tLoss 2.5959 (1.8514)\tTop 1-err 53.1250 (41.7240)\tTop 5-err 20.3125 (18.5495)\n",
            "Epoch: [83/290][399/782]\tLR: 0.006196\tTime 0.146 (0.148)\tData 0.000 (0.001)\tLoss 4.1081 (1.8897)\tTop 1-err 92.1875 (42.2988)\tTop 5-err 73.4375 (19.0000)\n",
            "Epoch: [83/290][499/782]\tLR: 0.006196\tTime 0.145 (0.148)\tData 0.000 (0.000)\tLoss 3.4589 (1.8780)\tTop 1-err 93.7500 (42.1969)\tTop 5-err 76.5625 (19.1266)\n",
            "Epoch: [83/290][599/782]\tLR: 0.006196\tTime 0.146 (0.148)\tData 0.000 (0.000)\tLoss 3.6189 (1.9044)\tTop 1-err 76.5625 (42.7174)\tTop 5-err 40.6250 (19.4076)\n",
            "Epoch: [83/290][699/782]\tLR: 0.006196\tTime 0.145 (0.148)\tData 0.000 (0.000)\tLoss 1.1022 (1.9446)\tTop 1-err 35.9375 (43.4364)\tTop 5-err 6.2500 (20.0402)\n",
            "* Epoch: [83/290]\t Top 1-err 43.711  Top 5-err 20.333\t Train Loss 1.955\n",
            "* Epoch: [83/290]\t Top 1-err 33.370  Top 5-err 9.460\t Test Loss 1.187\n",
            "Current best accuracy (top-1 and 5 error): 31.48 8.52\n",
            "Epoch: [84/290][99/782]\tLR: 0.006248\tTime 0.151 (0.151)\tData 0.000 (0.002)\tLoss 2.8654 (1.9620)\tTop 1-err 64.0625 (44.6406)\tTop 5-err 32.8125 (22.4297)\n",
            "Epoch: [84/290][199/782]\tLR: 0.006248\tTime 0.152 (0.149)\tData 0.000 (0.001)\tLoss 1.0379 (1.8664)\tTop 1-err 34.3750 (42.4922)\tTop 5-err 4.6875 (19.6562)\n",
            "Epoch: [84/290][299/782]\tLR: 0.006248\tTime 0.145 (0.149)\tData 0.000 (0.001)\tLoss 3.6138 (1.8765)\tTop 1-err 75.0000 (42.5990)\tTop 5-err 56.2500 (19.6849)\n",
            "Epoch: [84/290][399/782]\tLR: 0.006248\tTime 0.144 (0.148)\tData 0.000 (0.001)\tLoss 0.9494 (1.9133)\tTop 1-err 26.5625 (43.3906)\tTop 5-err 4.6875 (20.2441)\n",
            "Epoch: [84/290][499/782]\tLR: 0.006248\tTime 0.151 (0.148)\tData 0.000 (0.000)\tLoss 2.8682 (1.9084)\tTop 1-err 75.0000 (43.1500)\tTop 5-err 45.3125 (19.8469)\n",
            "Epoch: [84/290][599/782]\tLR: 0.006248\tTime 0.145 (0.148)\tData 0.000 (0.000)\tLoss 0.9083 (1.8972)\tTop 1-err 28.1250 (42.9609)\tTop 5-err 4.6875 (19.5039)\n",
            "Epoch: [84/290][699/782]\tLR: 0.006248\tTime 0.154 (0.148)\tData 0.000 (0.000)\tLoss 1.1255 (1.8760)\tTop 1-err 32.8125 (42.5837)\tTop 5-err 10.9375 (19.1228)\n",
            "* Epoch: [84/290]\t Top 1-err 42.847  Top 5-err 19.401\t Train Loss 1.889\n",
            "* Epoch: [84/290]\t Top 1-err 32.480  Top 5-err 9.080\t Test Loss 1.144\n",
            "Current best accuracy (top-1 and 5 error): 31.48 8.52\n",
            "Epoch: [85/290][99/782]\tLR: 0.006301\tTime 0.145 (0.149)\tData 0.000 (0.002)\tLoss 4.0070 (2.0137)\tTop 1-err 84.3750 (43.6016)\tTop 5-err 67.1875 (21.0000)\n",
            "Epoch: [85/290][199/782]\tLR: 0.006301\tTime 0.143 (0.149)\tData 0.000 (0.001)\tLoss 0.9016 (1.9397)\tTop 1-err 26.5625 (42.5859)\tTop 5-err 4.6875 (19.3789)\n",
            "Epoch: [85/290][299/782]\tLR: 0.006301\tTime 0.147 (0.149)\tData 0.000 (0.001)\tLoss 2.9085 (1.9567)\tTop 1-err 51.5625 (43.3073)\tTop 5-err 31.2500 (19.9089)\n",
            "Epoch: [85/290][399/782]\tLR: 0.006301\tTime 0.152 (0.148)\tData 0.000 (0.001)\tLoss 0.8746 (1.9463)\tTop 1-err 28.1250 (43.0352)\tTop 5-err 3.1250 (19.7559)\n",
            "Epoch: [85/290][499/782]\tLR: 0.006301\tTime 0.146 (0.148)\tData 0.000 (0.000)\tLoss 4.1411 (1.9613)\tTop 1-err 85.9375 (43.5875)\tTop 5-err 65.6250 (20.1062)\n",
            "Epoch: [85/290][599/782]\tLR: 0.006301\tTime 0.145 (0.148)\tData 0.000 (0.000)\tLoss 2.8646 (1.9350)\tTop 1-err 89.0625 (43.0729)\tTop 5-err 68.7500 (19.6068)\n",
            "Epoch: [85/290][699/782]\tLR: 0.006301\tTime 0.148 (0.148)\tData 0.000 (0.000)\tLoss 3.7633 (1.9413)\tTop 1-err 89.0625 (43.3058)\tTop 5-err 68.7500 (19.7757)\n",
            "* Epoch: [85/290]\t Top 1-err 43.254  Top 5-err 19.711\t Train Loss 1.933\n",
            "* Epoch: [85/290]\t Top 1-err 33.690  Top 5-err 9.330\t Test Loss 1.196\n",
            "Current best accuracy (top-1 and 5 error): 31.48 8.52\n",
            "Epoch: [86/290][99/782]\tLR: 0.006355\tTime 0.145 (0.150)\tData 0.000 (0.002)\tLoss 0.7619 (1.9970)\tTop 1-err 21.8750 (43.7266)\tTop 5-err 4.6875 (18.9844)\n",
            "Epoch: [86/290][199/782]\tLR: 0.006355\tTime 0.149 (0.148)\tData 0.000 (0.001)\tLoss 3.7943 (2.0158)\tTop 1-err 85.9375 (44.2891)\tTop 5-err 60.9375 (19.9844)\n",
            "Epoch: [86/290][299/782]\tLR: 0.006355\tTime 0.145 (0.148)\tData 0.000 (0.001)\tLoss 1.0343 (1.9761)\tTop 1-err 26.5625 (43.6276)\tTop 5-err 6.2500 (19.6458)\n",
            "Epoch: [86/290][399/782]\tLR: 0.006355\tTime 0.144 (0.148)\tData 0.000 (0.001)\tLoss 0.9990 (1.9240)\tTop 1-err 23.4375 (42.7695)\tTop 5-err 6.2500 (18.8145)\n",
            "Epoch: [86/290][499/782]\tLR: 0.006355\tTime 0.151 (0.148)\tData 0.000 (0.001)\tLoss 1.2176 (1.9282)\tTop 1-err 29.6875 (42.9609)\tTop 5-err 12.5000 (19.1172)\n",
            "Epoch: [86/290][599/782]\tLR: 0.006355\tTime 0.151 (0.148)\tData 0.000 (0.000)\tLoss 1.1217 (1.9155)\tTop 1-err 34.3750 (42.8555)\tTop 5-err 4.6875 (19.1484)\n",
            "Epoch: [86/290][699/782]\tLR: 0.006355\tTime 0.149 (0.148)\tData 0.000 (0.000)\tLoss 1.0327 (1.9277)\tTop 1-err 25.0000 (43.1685)\tTop 5-err 7.8125 (19.4888)\n",
            "* Epoch: [86/290]\t Top 1-err 43.160  Top 5-err 19.494\t Train Loss 1.919\n",
            "* Epoch: [86/290]\t Top 1-err 34.290  Top 5-err 10.240\t Test Loss 1.251\n",
            "Current best accuracy (top-1 and 5 error): 31.48 8.52\n",
            "Epoch: [87/290][99/782]\tLR: 0.006409\tTime 0.147 (0.151)\tData 0.000 (0.002)\tLoss 1.1432 (1.7968)\tTop 1-err 32.8125 (41.2422)\tTop 5-err 9.3750 (18.8047)\n",
            "Epoch: [87/290][199/782]\tLR: 0.006409\tTime 0.152 (0.150)\tData 0.000 (0.001)\tLoss 0.8284 (1.7877)\tTop 1-err 25.0000 (41.1094)\tTop 5-err 6.2500 (18.4688)\n",
            "Epoch: [87/290][299/782]\tLR: 0.006409\tTime 0.145 (0.149)\tData 0.000 (0.001)\tLoss 4.6174 (1.8060)\tTop 1-err 95.3125 (41.3021)\tTop 5-err 82.8125 (18.1849)\n",
            "Epoch: [87/290][399/782]\tLR: 0.006409\tTime 0.146 (0.148)\tData 0.000 (0.001)\tLoss 3.1030 (1.8464)\tTop 1-err 60.9375 (41.9805)\tTop 5-err 25.0000 (18.5645)\n",
            "Epoch: [87/290][499/782]\tLR: 0.006409\tTime 0.146 (0.148)\tData 0.000 (0.001)\tLoss 3.3697 (1.8421)\tTop 1-err 68.7500 (41.8078)\tTop 5-err 34.3750 (18.3891)\n",
            "Epoch: [87/290][599/782]\tLR: 0.006409\tTime 0.145 (0.148)\tData 0.000 (0.000)\tLoss 4.2232 (1.8240)\tTop 1-err 93.7500 (41.5755)\tTop 5-err 81.2500 (18.1432)\n",
            "Epoch: [87/290][699/782]\tLR: 0.006409\tTime 0.148 (0.148)\tData 0.000 (0.000)\tLoss 3.7431 (1.8445)\tTop 1-err 81.2500 (41.8426)\tTop 5-err 57.8125 (18.3192)\n",
            "* Epoch: [87/290]\t Top 1-err 42.166  Top 5-err 18.551\t Train Loss 1.867\n",
            "* Epoch: [87/290]\t Top 1-err 32.160  Top 5-err 8.420\t Test Loss 1.109\n",
            "Current best accuracy (top-1 and 5 error): 31.48 8.52\n",
            "Epoch: [88/290][99/782]\tLR: 0.006463\tTime 0.146 (0.149)\tData 0.000 (0.002)\tLoss 3.3882 (2.1885)\tTop 1-err 76.5625 (47.8828)\tTop 5-err 48.4375 (23.5078)\n",
            "Epoch: [88/290][199/782]\tLR: 0.006463\tTime 0.144 (0.148)\tData 0.000 (0.001)\tLoss 3.4848 (2.0907)\tTop 1-err 70.3125 (46.3125)\tTop 5-err 35.9375 (22.4727)\n",
            "Epoch: [88/290][299/782]\tLR: 0.006463\tTime 0.154 (0.149)\tData 0.000 (0.001)\tLoss 1.2260 (1.9908)\tTop 1-err 32.8125 (44.5026)\tTop 5-err 7.8125 (20.9036)\n",
            "Epoch: [88/290][399/782]\tLR: 0.006463\tTime 0.149 (0.148)\tData 0.000 (0.001)\tLoss 0.9941 (1.9085)\tTop 1-err 31.2500 (43.2578)\tTop 5-err 4.6875 (19.8320)\n",
            "Epoch: [88/290][499/782]\tLR: 0.006463\tTime 0.145 (0.148)\tData 0.000 (0.000)\tLoss 3.3681 (1.8820)\tTop 1-err 87.5000 (42.6000)\tTop 5-err 71.8750 (19.1359)\n",
            "Epoch: [88/290][599/782]\tLR: 0.006463\tTime 0.147 (0.148)\tData 0.000 (0.000)\tLoss 3.4645 (1.8590)\tTop 1-err 59.3750 (42.2852)\tTop 5-err 32.8125 (18.9023)\n",
            "Epoch: [88/290][699/782]\tLR: 0.006463\tTime 0.145 (0.148)\tData 0.000 (0.000)\tLoss 4.1603 (1.8557)\tTop 1-err 79.6875 (42.1373)\tTop 5-err 46.8750 (18.7355)\n",
            "* Epoch: [88/290]\t Top 1-err 41.915  Top 5-err 18.482\t Train Loss 1.853\n",
            "* Epoch: [88/290]\t Top 1-err 33.630  Top 5-err 9.590\t Test Loss 1.203\n",
            "Current best accuracy (top-1 and 5 error): 31.48 8.52\n",
            "Epoch: [89/290][99/782]\tLR: 0.006518\tTime 0.145 (0.150)\tData 0.000 (0.002)\tLoss 3.2674 (1.7889)\tTop 1-err 53.1250 (39.7188)\tTop 5-err 28.1250 (17.2422)\n",
            "Epoch: [89/290][199/782]\tLR: 0.006518\tTime 0.144 (0.149)\tData 0.000 (0.001)\tLoss 1.1119 (1.8560)\tTop 1-err 32.8125 (41.8516)\tTop 5-err 6.2500 (19.2109)\n",
            "Epoch: [89/290][299/782]\tLR: 0.006518\tTime 0.157 (0.148)\tData 0.001 (0.001)\tLoss 3.5324 (1.8816)\tTop 1-err 71.8750 (42.5078)\tTop 5-err 34.3750 (19.4844)\n",
            "Epoch: [89/290][399/782]\tLR: 0.006518\tTime 0.154 (0.148)\tData 0.000 (0.001)\tLoss 3.3482 (1.9023)\tTop 1-err 79.6875 (42.8809)\tTop 5-err 54.6875 (19.7324)\n",
            "Epoch: [89/290][499/782]\tLR: 0.006518\tTime 0.144 (0.148)\tData 0.000 (0.000)\tLoss 1.0589 (1.8606)\tTop 1-err 25.0000 (41.7188)\tTop 5-err 10.9375 (18.5734)\n",
            "Epoch: [89/290][599/782]\tLR: 0.006518\tTime 0.151 (0.148)\tData 0.000 (0.000)\tLoss 1.0286 (1.8443)\tTop 1-err 28.1250 (41.6432)\tTop 5-err 4.6875 (18.6068)\n",
            "Epoch: [89/290][699/782]\tLR: 0.006518\tTime 0.150 (0.147)\tData 0.000 (0.000)\tLoss 2.8650 (1.8622)\tTop 1-err 42.1875 (41.7946)\tTop 5-err 10.9375 (18.6417)\n",
            "* Epoch: [89/290]\t Top 1-err 41.764  Top 5-err 18.653\t Train Loss 1.860\n",
            "* Epoch: [89/290]\t Top 1-err 32.640  Top 5-err 8.720\t Test Loss 1.136\n",
            "Current best accuracy (top-1 and 5 error): 31.48 8.52\n",
            "Epoch: [90/290][99/782]\tLR: 0.006574\tTime 0.146 (0.151)\tData 0.000 (0.002)\tLoss 3.8071 (2.0241)\tTop 1-err 81.2500 (43.7969)\tTop 5-err 57.8125 (20.9375)\n",
            "Epoch: [90/290][199/782]\tLR: 0.006574\tTime 0.144 (0.150)\tData 0.000 (0.001)\tLoss 3.2189 (1.9258)\tTop 1-err 71.8750 (42.0234)\tTop 5-err 35.9375 (18.8906)\n",
            "Epoch: [90/290][299/782]\tLR: 0.006574\tTime 0.154 (0.149)\tData 0.000 (0.001)\tLoss 3.8397 (1.9344)\tTop 1-err 93.7500 (42.8359)\tTop 5-err 81.2500 (19.8281)\n",
            "Epoch: [90/290][399/782]\tLR: 0.006574\tTime 0.142 (0.149)\tData 0.000 (0.001)\tLoss 1.1527 (1.8990)\tTop 1-err 34.3750 (42.3242)\tTop 5-err 10.9375 (19.2715)\n",
            "Epoch: [90/290][499/782]\tLR: 0.006574\tTime 0.145 (0.149)\tData 0.000 (0.000)\tLoss 1.1326 (1.9407)\tTop 1-err 29.6875 (43.3578)\tTop 5-err 9.3750 (20.4906)\n",
            "Epoch: [90/290][599/782]\tLR: 0.006574\tTime 0.145 (0.149)\tData 0.000 (0.000)\tLoss 1.0197 (1.9281)\tTop 1-err 23.4375 (43.1667)\tTop 5-err 6.2500 (20.1589)\n",
            "Epoch: [90/290][699/782]\tLR: 0.006574\tTime 0.145 (0.148)\tData 0.000 (0.000)\tLoss 3.2215 (1.8983)\tTop 1-err 62.5000 (42.6652)\tTop 5-err 29.6875 (19.6440)\n",
            "* Epoch: [90/290]\t Top 1-err 43.169  Top 5-err 20.089\t Train Loss 1.923\n",
            "* Epoch: [90/290]\t Top 1-err 33.340  Top 5-err 9.140\t Test Loss 1.158\n",
            "Current best accuracy (top-1 and 5 error): 31.48 8.52\n",
            "Epoch: [91/290][99/782]\tLR: 0.006631\tTime 0.146 (0.149)\tData 0.000 (0.002)\tLoss 0.9097 (1.8479)\tTop 1-err 26.5625 (41.1250)\tTop 5-err 6.2500 (18.6016)\n",
            "Epoch: [91/290][199/782]\tLR: 0.006631\tTime 0.149 (0.148)\tData 0.000 (0.001)\tLoss 3.4478 (1.7852)\tTop 1-err 79.6875 (39.7461)\tTop 5-err 51.5625 (17.1836)\n",
            "Epoch: [91/290][299/782]\tLR: 0.006631\tTime 0.152 (0.148)\tData 0.000 (0.001)\tLoss 0.8655 (1.8214)\tTop 1-err 21.8750 (40.4922)\tTop 5-err 6.2500 (17.7214)\n",
            "Epoch: [91/290][399/782]\tLR: 0.006631\tTime 0.145 (0.148)\tData 0.000 (0.000)\tLoss 0.9281 (1.8152)\tTop 1-err 26.5625 (40.7617)\tTop 5-err 4.6875 (17.8379)\n",
            "Epoch: [91/290][499/782]\tLR: 0.006631\tTime 0.152 (0.147)\tData 0.000 (0.000)\tLoss 2.5811 (1.8266)\tTop 1-err 51.5625 (40.9609)\tTop 5-err 14.0625 (18.0578)\n",
            "Epoch: [91/290][599/782]\tLR: 0.006631\tTime 0.148 (0.147)\tData 0.000 (0.000)\tLoss 2.9281 (1.8563)\tTop 1-err 65.6250 (41.6940)\tTop 5-err 43.7500 (18.6875)\n",
            "Epoch: [91/290][699/782]\tLR: 0.006631\tTime 0.153 (0.147)\tData 0.000 (0.000)\tLoss 1.0892 (1.8736)\tTop 1-err 39.0625 (42.1629)\tTop 5-err 7.8125 (19.1339)\n",
            "* Epoch: [91/290]\t Top 1-err 41.963  Top 5-err 18.894\t Train Loss 1.861\n",
            "* Epoch: [91/290]\t Top 1-err 32.740  Top 5-err 8.810\t Test Loss 1.135\n",
            "Current best accuracy (top-1 and 5 error): 31.48 8.52\n",
            "Epoch: [92/290][99/782]\tLR: 0.006687\tTime 0.153 (0.151)\tData 0.000 (0.002)\tLoss 0.9820 (1.8062)\tTop 1-err 26.5625 (40.7656)\tTop 5-err 9.3750 (17.4219)\n",
            "Epoch: [92/290][199/782]\tLR: 0.006687\tTime 0.145 (0.149)\tData 0.000 (0.001)\tLoss 2.1745 (1.8253)\tTop 1-err 45.3125 (41.2500)\tTop 5-err 9.3750 (17.9883)\n",
            "Epoch: [92/290][299/782]\tLR: 0.006687\tTime 0.152 (0.149)\tData 0.000 (0.001)\tLoss 0.9346 (1.8663)\tTop 1-err 23.4375 (41.9870)\tTop 5-err 7.8125 (18.9427)\n",
            "Epoch: [92/290][399/782]\tLR: 0.006687\tTime 0.144 (0.149)\tData 0.000 (0.001)\tLoss 1.1413 (1.8952)\tTop 1-err 31.2500 (42.5000)\tTop 5-err 7.8125 (19.3555)\n",
            "Epoch: [92/290][499/782]\tLR: 0.006687\tTime 0.150 (0.149)\tData 0.000 (0.001)\tLoss 2.9585 (1.9024)\tTop 1-err 51.5625 (42.2328)\tTop 5-err 25.0000 (19.0406)\n",
            "Epoch: [92/290][599/782]\tLR: 0.006687\tTime 0.145 (0.148)\tData 0.000 (0.000)\tLoss 2.2271 (1.9326)\tTop 1-err 50.0000 (42.8073)\tTop 5-err 21.8750 (19.5664)\n",
            "Epoch: [92/290][699/782]\tLR: 0.006687\tTime 0.144 (0.148)\tData 0.000 (0.000)\tLoss 0.9987 (1.9402)\tTop 1-err 28.1250 (43.0312)\tTop 5-err 6.2500 (19.7210)\n",
            "* Epoch: [92/290]\t Top 1-err 43.101  Top 5-err 19.736\t Train Loss 1.946\n",
            "* Epoch: [92/290]\t Top 1-err 32.780  Top 5-err 9.060\t Test Loss 1.169\n",
            "Current best accuracy (top-1 and 5 error): 31.48 8.52\n",
            "Epoch: [93/290][99/782]\tLR: 0.006745\tTime 0.152 (0.151)\tData 0.000 (0.002)\tLoss 0.8938 (1.8216)\tTop 1-err 28.1250 (40.7969)\tTop 5-err 3.1250 (18.4609)\n",
            "Epoch: [93/290][199/782]\tLR: 0.006745\tTime 0.153 (0.149)\tData 0.000 (0.001)\tLoss 0.8672 (1.8340)\tTop 1-err 20.3125 (41.2500)\tTop 5-err 0.0000 (18.4297)\n",
            "Epoch: [93/290][299/782]\tLR: 0.006745\tTime 0.150 (0.148)\tData 0.000 (0.001)\tLoss 1.0661 (1.8349)\tTop 1-err 29.6875 (40.9349)\tTop 5-err 7.8125 (18.0521)\n",
            "Epoch: [93/290][399/782]\tLR: 0.006745\tTime 0.147 (0.148)\tData 0.000 (0.000)\tLoss 3.9571 (1.8167)\tTop 1-err 79.6875 (40.4414)\tTop 5-err 54.6875 (17.5449)\n",
            "Epoch: [93/290][499/782]\tLR: 0.006745\tTime 0.153 (0.148)\tData 0.000 (0.000)\tLoss 3.0839 (1.8175)\tTop 1-err 90.6250 (40.3328)\tTop 5-err 59.3750 (17.2437)\n",
            "Epoch: [93/290][599/782]\tLR: 0.006745\tTime 0.145 (0.148)\tData 0.000 (0.000)\tLoss 1.1852 (1.8354)\tTop 1-err 34.3750 (40.8932)\tTop 5-err 9.3750 (17.8477)\n",
            "Epoch: [93/290][699/782]\tLR: 0.006745\tTime 0.151 (0.148)\tData 0.000 (0.000)\tLoss 1.1168 (1.8526)\tTop 1-err 32.8125 (41.3862)\tTop 5-err 7.8125 (18.3393)\n",
            "* Epoch: [93/290]\t Top 1-err 41.867  Top 5-err 18.775\t Train Loss 1.877\n",
            "* Epoch: [93/290]\t Top 1-err 33.590  Top 5-err 9.790\t Test Loss 1.195\n",
            "Current best accuracy (top-1 and 5 error): 31.48 8.52\n",
            "Epoch: [94/290][99/782]\tLR: 0.006803\tTime 0.145 (0.149)\tData 0.000 (0.002)\tLoss 0.9626 (1.9177)\tTop 1-err 31.2500 (42.1562)\tTop 5-err 7.8125 (19.1406)\n",
            "Epoch: [94/290][199/782]\tLR: 0.006803\tTime 0.146 (0.149)\tData 0.000 (0.001)\tLoss 3.1712 (1.9886)\tTop 1-err 45.3125 (43.0156)\tTop 5-err 21.8750 (19.5273)\n",
            "Epoch: [94/290][299/782]\tLR: 0.006803\tTime 0.145 (0.148)\tData 0.000 (0.001)\tLoss 1.0031 (1.9018)\tTop 1-err 23.4375 (41.8854)\tTop 5-err 9.3750 (18.8750)\n",
            "Epoch: [94/290][399/782]\tLR: 0.006803\tTime 0.146 (0.148)\tData 0.000 (0.001)\tLoss 3.7174 (1.9259)\tTop 1-err 85.9375 (42.2578)\tTop 5-err 57.8125 (19.2559)\n",
            "Epoch: [94/290][499/782]\tLR: 0.006803\tTime 0.148 (0.148)\tData 0.000 (0.000)\tLoss 0.8932 (1.9006)\tTop 1-err 25.0000 (41.8109)\tTop 5-err 7.8125 (18.9000)\n",
            "Epoch: [94/290][599/782]\tLR: 0.006803\tTime 0.158 (0.148)\tData 0.000 (0.000)\tLoss 0.7876 (1.8849)\tTop 1-err 25.0000 (41.5339)\tTop 5-err 4.6875 (18.5990)\n",
            "Epoch: [94/290][699/782]\tLR: 0.006803\tTime 0.145 (0.148)\tData 0.000 (0.000)\tLoss 3.4475 (1.8943)\tTop 1-err 84.3750 (41.7511)\tTop 5-err 53.1250 (18.6406)\n",
            "* Epoch: [94/290]\t Top 1-err 41.437  Top 5-err 18.336\t Train Loss 1.872\n",
            "* Epoch: [94/290]\t Top 1-err 31.740  Top 5-err 8.390\t Test Loss 1.095\n",
            "Current best accuracy (top-1 and 5 error): 31.48 8.52\n",
            "Epoch: [95/290][99/782]\tLR: 0.006862\tTime 0.146 (0.150)\tData 0.000 (0.002)\tLoss 2.7069 (1.6942)\tTop 1-err 50.0000 (38.2656)\tTop 5-err 6.2500 (16.3359)\n",
            "Epoch: [95/290][199/782]\tLR: 0.006862\tTime 0.147 (0.148)\tData 0.000 (0.001)\tLoss 3.2066 (1.7928)\tTop 1-err 67.1875 (40.2148)\tTop 5-err 42.1875 (17.9336)\n",
            "Epoch: [95/290][299/782]\tLR: 0.006862\tTime 0.143 (0.148)\tData 0.000 (0.001)\tLoss 3.0527 (1.8204)\tTop 1-err 59.3750 (41.0599)\tTop 5-err 32.8125 (18.7161)\n",
            "Epoch: [95/290][399/782]\tLR: 0.006862\tTime 0.146 (0.148)\tData 0.000 (0.001)\tLoss 3.8188 (1.8838)\tTop 1-err 79.6875 (42.0527)\tTop 5-err 59.3750 (19.3047)\n",
            "Epoch: [95/290][499/782]\tLR: 0.006862\tTime 0.144 (0.148)\tData 0.000 (0.001)\tLoss 0.8956 (1.8777)\tTop 1-err 26.5625 (42.0000)\tTop 5-err 7.8125 (19.1922)\n",
            "Epoch: [95/290][599/782]\tLR: 0.006862\tTime 0.151 (0.148)\tData 0.000 (0.000)\tLoss 3.9402 (1.8661)\tTop 1-err 82.8125 (41.8750)\tTop 5-err 62.5000 (19.0560)\n",
            "Epoch: [95/290][699/782]\tLR: 0.006862\tTime 0.148 (0.148)\tData 0.000 (0.000)\tLoss 3.5159 (1.8776)\tTop 1-err 60.9375 (42.1529)\tTop 5-err 26.5625 (19.2511)\n",
            "* Epoch: [95/290]\t Top 1-err 42.016  Top 5-err 19.090\t Train Loss 1.871\n",
            "* Epoch: [95/290]\t Top 1-err 31.510  Top 5-err 8.940\t Test Loss 1.109\n",
            "Current best accuracy (top-1 and 5 error): 31.48 8.52\n",
            "Epoch: [96/290][99/782]\tLR: 0.006921\tTime 0.145 (0.150)\tData 0.000 (0.002)\tLoss 3.7947 (2.0020)\tTop 1-err 87.5000 (42.1562)\tTop 5-err 56.2500 (18.8906)\n",
            "Epoch: [96/290][199/782]\tLR: 0.006921\tTime 0.148 (0.149)\tData 0.000 (0.001)\tLoss 1.0718 (1.9190)\tTop 1-err 31.2500 (42.1680)\tTop 5-err 6.2500 (19.2773)\n",
            "Epoch: [96/290][299/782]\tLR: 0.006921\tTime 0.142 (0.148)\tData 0.000 (0.001)\tLoss 0.8910 (1.9107)\tTop 1-err 28.1250 (42.2630)\tTop 5-err 4.6875 (19.3333)\n",
            "Epoch: [96/290][399/782]\tLR: 0.006921\tTime 0.146 (0.148)\tData 0.000 (0.001)\tLoss 2.4142 (1.8876)\tTop 1-err 53.1250 (42.0195)\tTop 5-err 20.3125 (18.9746)\n",
            "Epoch: [96/290][499/782]\tLR: 0.006921\tTime 0.144 (0.148)\tData 0.000 (0.000)\tLoss 2.2100 (1.8714)\tTop 1-err 59.3750 (41.7094)\tTop 5-err 18.7500 (18.7859)\n",
            "Epoch: [96/290][599/782]\tLR: 0.006921\tTime 0.143 (0.148)\tData 0.000 (0.000)\tLoss 2.4621 (1.8825)\tTop 1-err 50.0000 (41.8633)\tTop 5-err 15.6250 (18.8945)\n",
            "Epoch: [96/290][699/782]\tLR: 0.006921\tTime 0.146 (0.148)\tData 0.000 (0.000)\tLoss 1.4293 (1.8752)\tTop 1-err 37.5000 (41.7779)\tTop 5-err 7.8125 (18.7478)\n",
            "* Epoch: [96/290]\t Top 1-err 41.409  Top 5-err 18.381\t Train Loss 1.855\n",
            "* Epoch: [96/290]\t Top 1-err 32.200  Top 5-err 8.690\t Test Loss 1.132\n",
            "Current best accuracy (top-1 and 5 error): 31.48 8.52\n",
            "Epoch: [97/290][99/782]\tLR: 0.006981\tTime 0.151 (0.152)\tData 0.000 (0.002)\tLoss 3.2996 (1.8500)\tTop 1-err 64.0625 (40.2812)\tTop 5-err 14.0625 (17.4297)\n",
            "Epoch: [97/290][199/782]\tLR: 0.006981\tTime 0.146 (0.150)\tData 0.000 (0.001)\tLoss 0.9457 (1.8256)\tTop 1-err 26.5625 (39.7852)\tTop 5-err 7.8125 (17.3086)\n",
            "Epoch: [97/290][299/782]\tLR: 0.006981\tTime 0.156 (0.150)\tData 0.000 (0.001)\tLoss 2.7403 (1.9104)\tTop 1-err 56.2500 (41.7031)\tTop 5-err 28.1250 (18.6432)\n",
            "Epoch: [97/290][399/782]\tLR: 0.006981\tTime 0.160 (0.150)\tData 0.000 (0.001)\tLoss 1.2592 (1.8725)\tTop 1-err 39.0625 (41.0918)\tTop 5-err 9.3750 (18.0820)\n",
            "Epoch: [97/290][499/782]\tLR: 0.006981\tTime 0.151 (0.150)\tData 0.000 (0.000)\tLoss 1.9351 (1.8916)\tTop 1-err 37.5000 (41.7031)\tTop 5-err 9.3750 (18.5813)\n",
            "Epoch: [97/290][599/782]\tLR: 0.006981\tTime 0.153 (0.149)\tData 0.000 (0.000)\tLoss 0.8036 (1.8817)\tTop 1-err 25.0000 (41.6172)\tTop 5-err 9.3750 (18.3815)\n",
            "Epoch: [97/290][699/782]\tLR: 0.006981\tTime 0.144 (0.149)\tData 0.000 (0.000)\tLoss 2.9481 (1.8720)\tTop 1-err 53.1250 (41.6228)\tTop 5-err 21.8750 (18.2991)\n",
            "* Epoch: [97/290]\t Top 1-err 41.307  Top 5-err 18.005\t Train Loss 1.851\n",
            "* Epoch: [97/290]\t Top 1-err 32.330  Top 5-err 8.710\t Test Loss 1.116\n",
            "Current best accuracy (top-1 and 5 error): 31.48 8.52\n",
            "Epoch: [98/290][99/782]\tLR: 0.007042\tTime 0.148 (0.154)\tData 0.000 (0.002)\tLoss 3.1543 (1.7345)\tTop 1-err 56.2500 (38.8828)\tTop 5-err 28.1250 (16.8281)\n",
            "Epoch: [98/290][199/782]\tLR: 0.007042\tTime 0.147 (0.152)\tData 0.000 (0.001)\tLoss 3.8977 (1.7737)\tTop 1-err 92.1875 (39.8672)\tTop 5-err 81.2500 (18.1680)\n",
            "Epoch: [98/290][299/782]\tLR: 0.007042\tTime 0.149 (0.151)\tData 0.000 (0.001)\tLoss 3.5231 (1.8188)\tTop 1-err 76.5625 (41.1276)\tTop 5-err 46.8750 (19.0365)\n",
            "Epoch: [98/290][399/782]\tLR: 0.007042\tTime 0.149 (0.150)\tData 0.000 (0.001)\tLoss 3.9396 (1.8242)\tTop 1-err 70.3125 (41.0801)\tTop 5-err 40.6250 (18.9746)\n",
            "Epoch: [98/290][499/782]\tLR: 0.007042\tTime 0.154 (0.150)\tData 0.000 (0.001)\tLoss 1.0459 (1.8424)\tTop 1-err 26.5625 (41.3281)\tTop 5-err 7.8125 (18.8141)\n",
            "Epoch: [98/290][599/782]\tLR: 0.007042\tTime 0.147 (0.150)\tData 0.000 (0.000)\tLoss 4.1088 (1.8779)\tTop 1-err 81.2500 (42.1224)\tTop 5-err 64.0625 (19.4232)\n",
            "Epoch: [98/290][699/782]\tLR: 0.007042\tTime 0.149 (0.150)\tData 0.000 (0.000)\tLoss 3.7878 (1.8901)\tTop 1-err 81.2500 (42.3873)\tTop 5-err 56.2500 (19.5089)\n",
            "* Epoch: [98/290]\t Top 1-err 42.323  Top 5-err 19.357\t Train Loss 1.890\n",
            "* Epoch: [98/290]\t Top 1-err 33.150  Top 5-err 9.210\t Test Loss 1.190\n",
            "Current best accuracy (top-1 and 5 error): 31.48 8.52\n",
            "Epoch: [99/290][99/782]\tLR: 0.007103\tTime 0.146 (0.153)\tData 0.000 (0.002)\tLoss 0.8259 (1.8750)\tTop 1-err 26.5625 (40.5078)\tTop 5-err 4.6875 (17.8359)\n",
            "Epoch: [99/290][199/782]\tLR: 0.007103\tTime 0.147 (0.151)\tData 0.000 (0.001)\tLoss 3.3985 (1.9074)\tTop 1-err 79.6875 (41.4766)\tTop 5-err 65.6250 (19.0508)\n",
            "Epoch: [99/290][299/782]\tLR: 0.007103\tTime 0.145 (0.151)\tData 0.000 (0.001)\tLoss 0.9826 (1.8647)\tTop 1-err 28.1250 (41.1771)\tTop 5-err 4.6875 (18.6901)\n",
            "Epoch: [99/290][399/782]\tLR: 0.007103\tTime 0.147 (0.150)\tData 0.000 (0.001)\tLoss 0.8504 (1.8108)\tTop 1-err 26.5625 (40.7148)\tTop 5-err 3.1250 (18.0410)\n",
            "Epoch: [99/290][499/782]\tLR: 0.007103\tTime 0.144 (0.150)\tData 0.000 (0.000)\tLoss 0.9668 (1.8394)\tTop 1-err 28.1250 (41.4062)\tTop 5-err 4.6875 (18.4547)\n",
            "Epoch: [99/290][599/782]\tLR: 0.007103\tTime 0.147 (0.150)\tData 0.000 (0.000)\tLoss 3.1274 (1.8712)\tTop 1-err 62.5000 (42.1745)\tTop 5-err 40.6250 (19.2826)\n",
            "Epoch: [99/290][699/782]\tLR: 0.007103\tTime 0.145 (0.149)\tData 0.000 (0.000)\tLoss 0.7507 (1.8602)\tTop 1-err 23.4375 (41.9576)\tTop 5-err 3.1250 (19.1172)\n",
            "* Epoch: [99/290]\t Top 1-err 41.446  Top 5-err 18.681\t Train Loss 1.836\n",
            "* Epoch: [99/290]\t Top 1-err 31.380  Top 5-err 8.360\t Test Loss 1.092\n",
            "Current best accuracy (top-1 and 5 error): 31.38 8.36\n",
            "Epoch: [100/290][99/782]\tLR: 0.007164\tTime 0.146 (0.153)\tData 0.000 (0.002)\tLoss 0.8488 (1.7820)\tTop 1-err 29.6875 (40.2188)\tTop 5-err 3.1250 (17.7500)\n",
            "Epoch: [100/290][199/782]\tLR: 0.007164\tTime 0.147 (0.151)\tData 0.000 (0.001)\tLoss 0.8734 (1.8841)\tTop 1-err 25.0000 (42.2734)\tTop 5-err 7.8125 (19.4727)\n",
            "Epoch: [100/290][299/782]\tLR: 0.007164\tTime 0.152 (0.150)\tData 0.000 (0.001)\tLoss 3.8275 (1.8946)\tTop 1-err 79.6875 (42.4141)\tTop 5-err 56.2500 (19.5391)\n",
            "Epoch: [100/290][399/782]\tLR: 0.007164\tTime 0.144 (0.150)\tData 0.000 (0.001)\tLoss 3.7803 (1.9227)\tTop 1-err 85.9375 (42.6895)\tTop 5-err 64.0625 (19.7051)\n",
            "Epoch: [100/290][499/782]\tLR: 0.007164\tTime 0.153 (0.149)\tData 0.000 (0.000)\tLoss 1.4535 (1.8927)\tTop 1-err 35.9375 (42.1219)\tTop 5-err 10.9375 (19.2063)\n",
            "Epoch: [100/290][599/782]\tLR: 0.007164\tTime 0.149 (0.149)\tData 0.000 (0.000)\tLoss 0.9350 (1.8901)\tTop 1-err 25.0000 (42.2331)\tTop 5-err 7.8125 (19.3164)\n",
            "Epoch: [100/290][699/782]\tLR: 0.007164\tTime 0.147 (0.149)\tData 0.000 (0.000)\tLoss 1.1207 (1.8741)\tTop 1-err 26.5625 (41.9743)\tTop 5-err 14.0625 (18.9353)\n",
            "* Epoch: [100/290]\t Top 1-err 42.344  Top 5-err 19.250\t Train Loss 1.893\n",
            "* Epoch: [100/290]\t Top 1-err 32.440  Top 5-err 9.360\t Test Loss 1.170\n",
            "Current best accuracy (top-1 and 5 error): 31.38 8.36\n",
            "Epoch: [101/290][99/782]\tLR: 0.007227\tTime 0.151 (0.151)\tData 0.000 (0.002)\tLoss 2.8930 (1.9189)\tTop 1-err 57.8125 (41.1406)\tTop 5-err 20.3125 (18.4375)\n",
            "Epoch: [101/290][199/782]\tLR: 0.007227\tTime 0.146 (0.150)\tData 0.000 (0.001)\tLoss 2.6979 (1.8590)\tTop 1-err 48.4375 (41.0195)\tTop 5-err 26.5625 (18.3945)\n",
            "Epoch: [101/290][299/782]\tLR: 0.007227\tTime 0.146 (0.150)\tData 0.000 (0.001)\tLoss 1.1805 (1.8525)\tTop 1-err 31.2500 (40.8203)\tTop 5-err 9.3750 (18.2135)\n",
            "Epoch: [101/290][399/782]\tLR: 0.007227\tTime 0.149 (0.150)\tData 0.000 (0.001)\tLoss 3.2087 (1.8178)\tTop 1-err 60.9375 (40.4863)\tTop 5-err 29.6875 (17.8379)\n",
            "Epoch: [101/290][499/782]\tLR: 0.007227\tTime 0.149 (0.150)\tData 0.000 (0.001)\tLoss 0.8475 (1.8134)\tTop 1-err 29.6875 (40.5750)\tTop 5-err 4.6875 (17.8781)\n",
            "Epoch: [101/290][599/782]\tLR: 0.007227\tTime 0.152 (0.150)\tData 0.000 (0.000)\tLoss 3.5592 (1.8175)\tTop 1-err 79.6875 (40.8854)\tTop 5-err 60.9375 (18.3047)\n",
            "Epoch: [101/290][699/782]\tLR: 0.007227\tTime 0.150 (0.150)\tData 0.000 (0.000)\tLoss 1.0448 (1.8137)\tTop 1-err 31.2500 (40.8092)\tTop 5-err 7.8125 (18.1328)\n",
            "* Epoch: [101/290]\t Top 1-err 40.923  Top 5-err 18.172\t Train Loss 1.820\n",
            "* Epoch: [101/290]\t Top 1-err 32.180  Top 5-err 8.900\t Test Loss 1.128\n",
            "Current best accuracy (top-1 and 5 error): 31.38 8.36\n",
            "Epoch: [102/290][99/782]\tLR: 0.007289\tTime 0.145 (0.153)\tData 0.000 (0.002)\tLoss 0.9247 (1.9081)\tTop 1-err 31.2500 (40.3438)\tTop 5-err 6.2500 (17.8984)\n",
            "Epoch: [102/290][199/782]\tLR: 0.007289\tTime 0.147 (0.152)\tData 0.000 (0.001)\tLoss 1.0357 (1.8619)\tTop 1-err 29.6875 (40.6992)\tTop 5-err 9.3750 (17.7422)\n",
            "Epoch: [102/290][299/782]\tLR: 0.007289\tTime 0.148 (0.152)\tData 0.000 (0.001)\tLoss 3.0769 (1.8146)\tTop 1-err 67.1875 (40.0443)\tTop 5-err 34.3750 (17.1198)\n",
            "Epoch: [102/290][399/782]\tLR: 0.007289\tTime 0.147 (0.151)\tData 0.000 (0.001)\tLoss 0.6559 (1.8364)\tTop 1-err 25.0000 (40.4766)\tTop 5-err 1.5625 (17.4648)\n",
            "Epoch: [102/290][499/782]\tLR: 0.007289\tTime 0.147 (0.151)\tData 0.000 (0.000)\tLoss 3.0594 (1.8355)\tTop 1-err 53.1250 (40.3625)\tTop 5-err 28.1250 (17.2375)\n",
            "Epoch: [102/290][599/782]\tLR: 0.007289\tTime 0.146 (0.150)\tData 0.000 (0.000)\tLoss 0.7689 (1.8816)\tTop 1-err 23.4375 (41.4440)\tTop 5-err 3.1250 (18.2383)\n",
            "Epoch: [102/290][699/782]\tLR: 0.007289\tTime 0.147 (0.150)\tData 0.000 (0.000)\tLoss 1.2308 (1.8616)\tTop 1-err 32.8125 (41.1953)\tTop 5-err 9.3750 (18.0714)\n",
            "* Epoch: [102/290]\t Top 1-err 41.104  Top 5-err 18.071\t Train Loss 1.855\n",
            "* Epoch: [102/290]\t Top 1-err 32.080  Top 5-err 8.400\t Test Loss 1.100\n",
            "Current best accuracy (top-1 and 5 error): 31.38 8.36\n",
            "Epoch: [103/290][99/782]\tLR: 0.007353\tTime 0.153 (0.151)\tData 0.000 (0.002)\tLoss 1.1226 (1.7348)\tTop 1-err 32.8125 (37.0234)\tTop 5-err 10.9375 (14.5078)\n",
            "Epoch: [103/290][199/782]\tLR: 0.007353\tTime 0.147 (0.150)\tData 0.000 (0.001)\tLoss 3.9571 (1.7423)\tTop 1-err 75.0000 (38.2656)\tTop 5-err 46.8750 (15.4727)\n",
            "Epoch: [103/290][299/782]\tLR: 0.007353\tTime 0.156 (0.150)\tData 0.000 (0.001)\tLoss 0.8280 (1.7534)\tTop 1-err 26.5625 (38.9141)\tTop 5-err 6.2500 (16.1354)\n",
            "Epoch: [103/290][399/782]\tLR: 0.007353\tTime 0.147 (0.150)\tData 0.000 (0.001)\tLoss 1.0761 (1.8078)\tTop 1-err 37.5000 (40.2031)\tTop 5-err 4.6875 (17.1230)\n",
            "Epoch: [103/290][499/782]\tLR: 0.007353\tTime 0.146 (0.150)\tData 0.000 (0.000)\tLoss 2.1208 (1.7935)\tTop 1-err 35.9375 (40.0000)\tTop 5-err 9.3750 (16.9047)\n",
            "Epoch: [103/290][599/782]\tLR: 0.007353\tTime 0.127 (0.150)\tData 0.000 (0.000)\tLoss 2.6701 (1.8160)\tTop 1-err 53.1250 (40.2409)\tTop 5-err 28.1250 (17.1732)\n",
            "Epoch: [103/290][699/782]\tLR: 0.007353\tTime 0.160 (0.150)\tData 0.000 (0.000)\tLoss 2.2416 (1.8053)\tTop 1-err 37.5000 (40.0011)\tTop 5-err 7.8125 (16.9833)\n",
            "* Epoch: [103/290]\t Top 1-err 40.603  Top 5-err 17.449\t Train Loss 1.833\n",
            "* Epoch: [103/290]\t Top 1-err 32.070  Top 5-err 8.810\t Test Loss 1.125\n",
            "Current best accuracy (top-1 and 5 error): 31.38 8.36\n",
            "Epoch: [104/290][99/782]\tLR: 0.007417\tTime 0.149 (0.152)\tData 0.000 (0.002)\tLoss 2.4082 (1.8980)\tTop 1-err 62.5000 (42.2969)\tTop 5-err 29.6875 (19.2891)\n",
            "Epoch: [104/290][199/782]\tLR: 0.007417\tTime 0.155 (0.152)\tData 0.000 (0.001)\tLoss 3.0662 (1.7637)\tTop 1-err 45.3125 (39.9375)\tTop 5-err 20.3125 (17.4258)\n",
            "Epoch: [104/290][299/782]\tLR: 0.007417\tTime 0.156 (0.151)\tData 0.000 (0.001)\tLoss 3.8409 (1.7787)\tTop 1-err 90.6250 (40.0078)\tTop 5-err 64.0625 (17.3698)\n",
            "Epoch: [104/290][399/782]\tLR: 0.007417\tTime 0.146 (0.151)\tData 0.000 (0.001)\tLoss 3.3618 (1.8035)\tTop 1-err 64.0625 (40.3809)\tTop 5-err 43.7500 (17.6738)\n",
            "Epoch: [104/290][499/782]\tLR: 0.007417\tTime 0.147 (0.151)\tData 0.000 (0.000)\tLoss 3.9054 (1.8365)\tTop 1-err 96.8750 (40.7750)\tTop 5-err 84.3750 (17.9594)\n",
            "Epoch: [104/290][599/782]\tLR: 0.007417\tTime 0.151 (0.150)\tData 0.000 (0.000)\tLoss 4.5671 (1.8243)\tTop 1-err 98.4375 (40.8229)\tTop 5-err 87.5000 (17.9961)\n",
            "Epoch: [104/290][699/782]\tLR: 0.007417\tTime 0.148 (0.150)\tData 0.000 (0.000)\tLoss 2.7957 (1.8425)\tTop 1-err 64.0625 (41.1328)\tTop 5-err 40.6250 (18.3304)\n",
            "* Epoch: [104/290]\t Top 1-err 41.194  Top 5-err 18.378\t Train Loss 1.839\n",
            "* Epoch: [104/290]\t Top 1-err 31.240  Top 5-err 8.150\t Test Loss 1.079\n",
            "Current best accuracy (top-1 and 5 error): 31.24 8.15\n",
            "Epoch: [105/290][99/782]\tLR: 0.007481\tTime 0.148 (0.152)\tData 0.000 (0.003)\tLoss 2.7710 (1.9328)\tTop 1-err 70.3125 (42.7344)\tTop 5-err 26.5625 (19.7734)\n",
            "Epoch: [105/290][199/782]\tLR: 0.007481\tTime 0.153 (0.150)\tData 0.000 (0.001)\tLoss 2.9593 (1.9319)\tTop 1-err 45.3125 (43.0820)\tTop 5-err 23.4375 (20.2930)\n",
            "Epoch: [105/290][299/782]\tLR: 0.007481\tTime 0.149 (0.150)\tData 0.000 (0.001)\tLoss 1.4836 (1.9371)\tTop 1-err 35.9375 (42.6667)\tTop 5-err 7.8125 (19.7708)\n",
            "Epoch: [105/290][399/782]\tLR: 0.007481\tTime 0.149 (0.150)\tData 0.000 (0.001)\tLoss 0.8473 (1.9199)\tTop 1-err 28.1250 (42.2598)\tTop 5-err 3.1250 (19.5371)\n",
            "Epoch: [105/290][499/782]\tLR: 0.007481\tTime 0.156 (0.150)\tData 0.000 (0.001)\tLoss 0.8767 (1.9095)\tTop 1-err 25.0000 (42.1141)\tTop 5-err 3.1250 (19.4859)\n",
            "Epoch: [105/290][599/782]\tLR: 0.007481\tTime 0.151 (0.150)\tData 0.000 (0.001)\tLoss 1.2206 (1.9170)\tTop 1-err 32.8125 (42.5221)\tTop 5-err 9.3750 (19.8607)\n",
            "Epoch: [105/290][699/782]\tLR: 0.007481\tTime 0.153 (0.150)\tData 0.000 (0.000)\tLoss 3.4106 (1.9483)\tTop 1-err 62.5000 (43.0938)\tTop 5-err 40.6250 (20.3404)\n",
            "* Epoch: [105/290]\t Top 1-err 43.038  Top 5-err 20.276\t Train Loss 1.941\n",
            "* Epoch: [105/290]\t Top 1-err 30.940  Top 5-err 8.330\t Test Loss 1.084\n",
            "Current best accuracy (top-1 and 5 error): 30.94 8.33\n",
            "Epoch: [106/290][99/782]\tLR: 0.007547\tTime 0.156 (0.155)\tData 0.000 (0.003)\tLoss 0.9966 (1.9147)\tTop 1-err 28.1250 (41.4609)\tTop 5-err 6.2500 (19.4219)\n",
            "Epoch: [106/290][199/782]\tLR: 0.007547\tTime 0.146 (0.153)\tData 0.000 (0.001)\tLoss 2.9593 (1.8717)\tTop 1-err 64.0625 (41.0938)\tTop 5-err 26.5625 (18.6641)\n",
            "Epoch: [106/290][299/782]\tLR: 0.007547\tTime 0.151 (0.152)\tData 0.000 (0.001)\tLoss 0.7960 (1.9328)\tTop 1-err 23.4375 (42.0234)\tTop 5-err 7.8125 (19.6328)\n",
            "Epoch: [106/290][399/782]\tLR: 0.007547\tTime 0.150 (0.151)\tData 0.000 (0.001)\tLoss 0.8366 (1.8704)\tTop 1-err 28.1250 (41.1582)\tTop 5-err 4.6875 (18.9688)\n",
            "Epoch: [106/290][499/782]\tLR: 0.007547\tTime 0.149 (0.151)\tData 0.000 (0.001)\tLoss 0.9580 (1.8742)\tTop 1-err 28.1250 (41.2922)\tTop 5-err 7.8125 (18.9953)\n",
            "Epoch: [106/290][599/782]\tLR: 0.007547\tTime 0.156 (0.151)\tData 0.000 (0.001)\tLoss 3.1070 (1.8910)\tTop 1-err 65.6250 (41.6549)\tTop 5-err 35.9375 (18.9870)\n",
            "Epoch: [106/290][699/782]\tLR: 0.007547\tTime 0.149 (0.151)\tData 0.000 (0.000)\tLoss 2.2758 (1.9134)\tTop 1-err 34.3750 (42.1016)\tTop 5-err 14.0625 (19.2645)\n",
            "* Epoch: [106/290]\t Top 1-err 42.384  Top 5-err 19.587\t Train Loss 1.921\n",
            "* Epoch: [106/290]\t Top 1-err 31.380  Top 5-err 8.560\t Test Loss 1.097\n",
            "Current best accuracy (top-1 and 5 error): 30.94 8.33\n",
            "Epoch: [107/290][99/782]\tLR: 0.007612\tTime 0.156 (0.154)\tData 0.000 (0.002)\tLoss 0.5995 (1.9601)\tTop 1-err 10.9375 (41.9766)\tTop 5-err 3.1250 (18.3516)\n",
            "Epoch: [107/290][199/782]\tLR: 0.007612\tTime 0.149 (0.152)\tData 0.000 (0.001)\tLoss 4.5801 (1.9160)\tTop 1-err 95.3125 (41.3086)\tTop 5-err 75.0000 (17.8672)\n",
            "Epoch: [107/290][299/782]\tLR: 0.007612\tTime 0.149 (0.151)\tData 0.000 (0.001)\tLoss 1.2882 (1.8278)\tTop 1-err 37.5000 (40.0417)\tTop 5-err 10.9375 (16.9010)\n",
            "Epoch: [107/290][399/782]\tLR: 0.007612\tTime 0.156 (0.151)\tData 0.000 (0.001)\tLoss 0.8261 (1.8420)\tTop 1-err 23.4375 (40.6035)\tTop 5-err 4.6875 (17.5000)\n",
            "Epoch: [107/290][499/782]\tLR: 0.007612\tTime 0.150 (0.151)\tData 0.000 (0.001)\tLoss 3.0056 (1.8449)\tTop 1-err 57.8125 (40.8500)\tTop 5-err 26.5625 (17.9109)\n",
            "Epoch: [107/290][599/782]\tLR: 0.007612\tTime 0.148 (0.151)\tData 0.000 (0.000)\tLoss 3.5478 (1.8460)\tTop 1-err 56.2500 (41.1133)\tTop 5-err 34.3750 (18.1654)\n",
            "Epoch: [107/290][699/782]\tLR: 0.007612\tTime 0.148 (0.151)\tData 0.000 (0.000)\tLoss 3.3234 (1.8390)\tTop 1-err 56.2500 (41.1641)\tTop 5-err 31.2500 (18.3259)\n",
            "* Epoch: [107/290]\t Top 1-err 41.558  Top 5-err 18.597\t Train Loss 1.860\n",
            "* Epoch: [107/290]\t Top 1-err 32.500  Top 5-err 8.850\t Test Loss 1.140\n",
            "Current best accuracy (top-1 and 5 error): 30.94 8.33\n",
            "Epoch: [108/290][99/782]\tLR: 0.007679\tTime 0.145 (0.152)\tData 0.000 (0.002)\tLoss 0.9386 (1.7444)\tTop 1-err 29.6875 (39.2812)\tTop 5-err 3.1250 (16.0859)\n",
            "Epoch: [108/290][199/782]\tLR: 0.007679\tTime 0.154 (0.151)\tData 0.000 (0.001)\tLoss 2.3359 (1.8043)\tTop 1-err 34.3750 (40.2695)\tTop 5-err 9.3750 (17.2695)\n",
            "Epoch: [108/290][299/782]\tLR: 0.007679\tTime 0.155 (0.151)\tData 0.000 (0.001)\tLoss 0.7120 (1.7847)\tTop 1-err 25.0000 (39.8333)\tTop 5-err 1.5625 (17.0312)\n",
            "Epoch: [108/290][399/782]\tLR: 0.007679\tTime 0.155 (0.151)\tData 0.000 (0.001)\tLoss 0.9343 (1.7941)\tTop 1-err 23.4375 (40.0742)\tTop 5-err 7.8125 (17.3984)\n",
            "Epoch: [108/290][499/782]\tLR: 0.007679\tTime 0.150 (0.150)\tData 0.000 (0.001)\tLoss 0.7679 (1.8275)\tTop 1-err 31.2500 (40.9672)\tTop 5-err 1.5625 (18.1125)\n",
            "Epoch: [108/290][599/782]\tLR: 0.007679\tTime 0.149 (0.150)\tData 0.000 (0.000)\tLoss 0.7249 (1.8561)\tTop 1-err 18.7500 (41.5703)\tTop 5-err 3.1250 (18.6471)\n",
            "Epoch: [108/290][699/782]\tLR: 0.007679\tTime 0.156 (0.150)\tData 0.000 (0.000)\tLoss 3.5358 (1.8570)\tTop 1-err 89.0625 (41.6317)\tTop 5-err 57.8125 (18.7243)\n",
            "* Epoch: [108/290]\t Top 1-err 41.383  Top 5-err 18.534\t Train Loss 1.838\n",
            "* Epoch: [108/290]\t Top 1-err 31.530  Top 5-err 8.220\t Test Loss 1.091\n",
            "Current best accuracy (top-1 and 5 error): 30.94 8.33\n",
            "Epoch: [109/290][99/782]\tLR: 0.007746\tTime 0.150 (0.152)\tData 0.000 (0.002)\tLoss 3.3905 (1.8193)\tTop 1-err 59.3750 (39.9844)\tTop 5-err 28.1250 (17.8203)\n",
            "Epoch: [109/290][199/782]\tLR: 0.007746\tTime 0.147 (0.151)\tData 0.000 (0.001)\tLoss 3.1091 (1.8088)\tTop 1-err 76.5625 (39.9609)\tTop 5-err 42.1875 (17.6953)\n",
            "Epoch: [109/290][299/782]\tLR: 0.007746\tTime 0.147 (0.150)\tData 0.000 (0.001)\tLoss 2.8087 (1.8130)\tTop 1-err 48.4375 (40.2292)\tTop 5-err 23.4375 (17.8438)\n",
            "Epoch: [109/290][399/782]\tLR: 0.007746\tTime 0.156 (0.150)\tData 0.000 (0.001)\tLoss 3.0280 (1.8223)\tTop 1-err 57.8125 (40.5898)\tTop 5-err 29.6875 (17.9082)\n",
            "Epoch: [109/290][499/782]\tLR: 0.007746\tTime 0.153 (0.150)\tData 0.000 (0.001)\tLoss 3.9476 (1.8191)\tTop 1-err 84.3750 (40.4906)\tTop 5-err 48.4375 (17.6500)\n",
            "Epoch: [109/290][599/782]\tLR: 0.007746\tTime 0.146 (0.150)\tData 0.000 (0.000)\tLoss 4.1122 (1.8248)\tTop 1-err 71.8750 (40.7565)\tTop 5-err 37.5000 (18.0182)\n",
            "Epoch: [109/290][699/782]\tLR: 0.007746\tTime 0.148 (0.149)\tData 0.000 (0.000)\tLoss 0.8373 (1.8395)\tTop 1-err 25.0000 (41.0469)\tTop 5-err 7.8125 (18.3125)\n",
            "* Epoch: [109/290]\t Top 1-err 40.659  Top 5-err 18.053\t Train Loss 1.817\n",
            "* Epoch: [109/290]\t Top 1-err 31.210  Top 5-err 8.230\t Test Loss 1.085\n",
            "Current best accuracy (top-1 and 5 error): 30.94 8.33\n",
            "Epoch: [110/290][99/782]\tLR: 0.007813\tTime 0.145 (0.151)\tData 0.000 (0.002)\tLoss 1.3219 (1.7931)\tTop 1-err 31.2500 (39.8828)\tTop 5-err 14.0625 (18.1016)\n",
            "Epoch: [110/290][199/782]\tLR: 0.007813\tTime 0.145 (0.151)\tData 0.000 (0.001)\tLoss 3.5265 (1.8061)\tTop 1-err 73.4375 (40.5117)\tTop 5-err 42.1875 (18.4648)\n",
            "Epoch: [110/290][299/782]\tLR: 0.007813\tTime 0.145 (0.150)\tData 0.000 (0.001)\tLoss 0.9916 (1.8546)\tTop 1-err 26.5625 (41.6823)\tTop 5-err 9.3750 (19.4401)\n",
            "Epoch: [110/290][399/782]\tLR: 0.007813\tTime 0.148 (0.150)\tData 0.000 (0.001)\tLoss 2.6334 (1.7987)\tTop 1-err 60.9375 (40.6875)\tTop 5-err 21.8750 (18.6523)\n",
            "Epoch: [110/290][499/782]\tLR: 0.007813\tTime 0.147 (0.150)\tData 0.000 (0.000)\tLoss 1.2098 (1.7652)\tTop 1-err 28.1250 (39.9891)\tTop 5-err 12.5000 (17.8656)\n",
            "Epoch: [110/290][599/782]\tLR: 0.007813\tTime 0.146 (0.150)\tData 0.000 (0.000)\tLoss 0.7863 (1.8029)\tTop 1-err 20.3125 (40.5964)\tTop 5-err 4.6875 (18.3542)\n",
            "Epoch: [110/290][699/782]\tLR: 0.007813\tTime 0.152 (0.150)\tData 0.000 (0.000)\tLoss 3.4291 (1.8139)\tTop 1-err 79.6875 (40.8627)\tTop 5-err 39.0625 (18.6931)\n",
            "* Epoch: [110/290]\t Top 1-err 40.608  Top 5-err 18.529\t Train Loss 1.803\n",
            "* Epoch: [110/290]\t Top 1-err 33.260  Top 5-err 9.210\t Test Loss 1.258\n",
            "Current best accuracy (top-1 and 5 error): 30.94 8.33\n",
            "Epoch: [111/290][99/782]\tLR: 0.007881\tTime 0.147 (0.153)\tData 0.000 (0.002)\tLoss 2.5993 (1.9186)\tTop 1-err 39.0625 (42.4609)\tTop 5-err 10.9375 (18.9453)\n",
            "Epoch: [111/290][199/782]\tLR: 0.007881\tTime 0.144 (0.151)\tData 0.000 (0.001)\tLoss 0.8217 (1.8562)\tTop 1-err 21.8750 (41.5156)\tTop 5-err 1.5625 (18.4414)\n",
            "Epoch: [111/290][299/782]\tLR: 0.007881\tTime 0.152 (0.151)\tData 0.000 (0.001)\tLoss 3.2283 (1.8871)\tTop 1-err 62.5000 (41.5911)\tTop 5-err 28.1250 (18.7760)\n",
            "Epoch: [111/290][399/782]\tLR: 0.007881\tTime 0.148 (0.150)\tData 0.000 (0.001)\tLoss 0.9350 (1.8700)\tTop 1-err 25.0000 (41.3789)\tTop 5-err 7.8125 (18.4414)\n",
            "Epoch: [111/290][499/782]\tLR: 0.007881\tTime 0.146 (0.150)\tData 0.000 (0.000)\tLoss 2.3262 (1.8761)\tTop 1-err 43.7500 (41.5500)\tTop 5-err 26.5625 (18.5672)\n",
            "Epoch: [111/290][599/782]\tLR: 0.007881\tTime 0.151 (0.150)\tData 0.000 (0.000)\tLoss 1.1686 (1.8671)\tTop 1-err 37.5000 (41.3763)\tTop 5-err 10.9375 (18.4818)\n",
            "Epoch: [111/290][699/782]\tLR: 0.007881\tTime 0.151 (0.150)\tData 0.000 (0.000)\tLoss 3.5165 (1.8674)\tTop 1-err 87.5000 (41.5435)\tTop 5-err 57.8125 (18.7076)\n",
            "* Epoch: [111/290]\t Top 1-err 41.644  Top 5-err 18.851\t Train Loss 1.872\n",
            "* Epoch: [111/290]\t Top 1-err 33.810  Top 5-err 9.830\t Test Loss 1.233\n",
            "Current best accuracy (top-1 and 5 error): 30.94 8.33\n",
            "Epoch: [112/290][99/782]\tLR: 0.007950\tTime 0.152 (0.152)\tData 0.000 (0.002)\tLoss 1.9288 (1.6487)\tTop 1-err 25.0000 (36.5781)\tTop 5-err 12.5000 (15.2812)\n",
            "Epoch: [112/290][199/782]\tLR: 0.007950\tTime 0.147 (0.152)\tData 0.000 (0.001)\tLoss 0.6364 (1.7496)\tTop 1-err 14.0625 (39.3945)\tTop 5-err 3.1250 (17.3789)\n",
            "Epoch: [112/290][299/782]\tLR: 0.007950\tTime 0.151 (0.151)\tData 0.000 (0.001)\tLoss 1.1386 (1.7497)\tTop 1-err 35.9375 (39.3594)\tTop 5-err 10.9375 (17.1354)\n",
            "Epoch: [112/290][399/782]\tLR: 0.007950\tTime 0.148 (0.151)\tData 0.000 (0.001)\tLoss 0.7866 (1.7303)\tTop 1-err 26.5625 (39.0078)\tTop 5-err 4.6875 (16.8164)\n",
            "Epoch: [112/290][499/782]\tLR: 0.007950\tTime 0.149 (0.150)\tData 0.000 (0.000)\tLoss 0.9579 (1.7409)\tTop 1-err 28.1250 (39.1922)\tTop 5-err 3.1250 (16.9047)\n",
            "Epoch: [112/290][599/782]\tLR: 0.007950\tTime 0.156 (0.150)\tData 0.000 (0.000)\tLoss 3.4883 (1.7471)\tTop 1-err 79.6875 (39.4154)\tTop 5-err 51.5625 (17.0378)\n",
            "Epoch: [112/290][699/782]\tLR: 0.007950\tTime 0.149 (0.150)\tData 0.000 (0.000)\tLoss 3.0211 (1.7791)\tTop 1-err 59.3750 (40.1786)\tTop 5-err 32.8125 (17.6975)\n",
            "* Epoch: [112/290]\t Top 1-err 40.487  Top 5-err 17.828\t Train Loss 1.801\n",
            "* Epoch: [112/290]\t Top 1-err 33.320  Top 5-err 9.320\t Test Loss 1.233\n",
            "Current best accuracy (top-1 and 5 error): 30.94 8.33\n",
            "Epoch: [113/290][99/782]\tLR: 0.008019\tTime 0.145 (0.150)\tData 0.000 (0.002)\tLoss 2.8651 (1.9270)\tTop 1-err 59.3750 (41.7266)\tTop 5-err 26.5625 (18.9062)\n",
            "Epoch: [113/290][199/782]\tLR: 0.008019\tTime 0.147 (0.150)\tData 0.000 (0.001)\tLoss 0.9651 (1.8929)\tTop 1-err 21.8750 (40.9297)\tTop 5-err 6.2500 (18.3281)\n",
            "Epoch: [113/290][299/782]\tLR: 0.008019\tTime 0.149 (0.150)\tData 0.000 (0.001)\tLoss 0.8382 (1.8538)\tTop 1-err 23.4375 (40.5990)\tTop 5-err 6.2500 (18.0833)\n",
            "Epoch: [113/290][399/782]\tLR: 0.008019\tTime 0.145 (0.150)\tData 0.000 (0.001)\tLoss 0.7662 (1.9027)\tTop 1-err 18.7500 (41.4570)\tTop 5-err 3.1250 (18.7285)\n",
            "Epoch: [113/290][499/782]\tLR: 0.008019\tTime 0.155 (0.150)\tData 0.000 (0.000)\tLoss 2.5041 (1.8864)\tTop 1-err 48.4375 (41.1859)\tTop 5-err 21.8750 (18.4656)\n",
            "Epoch: [113/290][599/782]\tLR: 0.008019\tTime 0.145 (0.149)\tData 0.000 (0.000)\tLoss 0.9275 (1.8937)\tTop 1-err 31.2500 (41.4076)\tTop 5-err 4.6875 (18.6523)\n",
            "Epoch: [113/290][699/782]\tLR: 0.008019\tTime 0.151 (0.149)\tData 0.000 (0.000)\tLoss 4.5986 (1.8856)\tTop 1-err 96.8750 (41.5324)\tTop 5-err 92.1875 (18.9029)\n",
            "* Epoch: [113/290]\t Top 1-err 41.898  Top 5-err 19.149\t Train Loss 1.905\n",
            "* Epoch: [113/290]\t Top 1-err 33.100  Top 5-err 9.500\t Test Loss 1.200\n",
            "Current best accuracy (top-1 and 5 error): 30.94 8.33\n",
            "Epoch: [114/290][99/782]\tLR: 0.008089\tTime 0.148 (0.153)\tData 0.000 (0.003)\tLoss 2.4326 (2.0454)\tTop 1-err 43.7500 (44.3281)\tTop 5-err 15.6250 (21.2656)\n",
            "Epoch: [114/290][199/782]\tLR: 0.008089\tTime 0.147 (0.151)\tData 0.000 (0.001)\tLoss 1.3123 (1.9322)\tTop 1-err 37.5000 (42.0312)\tTop 5-err 7.8125 (19.0508)\n",
            "Epoch: [114/290][299/782]\tLR: 0.008089\tTime 0.146 (0.151)\tData 0.000 (0.001)\tLoss 4.4506 (1.9029)\tTop 1-err 92.1875 (42.1068)\tTop 5-err 78.1250 (19.3932)\n",
            "Epoch: [114/290][399/782]\tLR: 0.008089\tTime 0.160 (0.150)\tData 0.000 (0.001)\tLoss 0.8617 (1.9147)\tTop 1-err 26.5625 (42.4766)\tTop 5-err 6.2500 (19.6641)\n",
            "Epoch: [114/290][499/782]\tLR: 0.008089\tTime 0.147 (0.150)\tData 0.000 (0.001)\tLoss 0.7105 (1.8855)\tTop 1-err 17.1875 (41.8766)\tTop 5-err 1.5625 (19.0391)\n",
            "Epoch: [114/290][599/782]\tLR: 0.008089\tTime 0.156 (0.150)\tData 0.000 (0.001)\tLoss 4.6669 (1.8759)\tTop 1-err 78.1250 (41.4701)\tTop 5-err 43.7500 (18.6419)\n",
            "Epoch: [114/290][699/782]\tLR: 0.008089\tTime 0.156 (0.150)\tData 0.000 (0.000)\tLoss 1.1033 (1.8633)\tTop 1-err 31.2500 (41.1975)\tTop 5-err 10.9375 (18.4263)\n",
            "* Epoch: [114/290]\t Top 1-err 41.203  Top 5-err 18.312\t Train Loss 1.865\n",
            "* Epoch: [114/290]\t Top 1-err 31.780  Top 5-err 8.710\t Test Loss 1.109\n",
            "Current best accuracy (top-1 and 5 error): 30.94 8.33\n",
            "Epoch: [115/290][99/782]\tLR: 0.008159\tTime 0.161 (0.154)\tData 0.000 (0.002)\tLoss 4.5711 (1.6468)\tTop 1-err 95.3125 (37.4688)\tTop 5-err 85.9375 (16.3828)\n",
            "Epoch: [115/290][199/782]\tLR: 0.008159\tTime 0.164 (0.152)\tData 0.000 (0.001)\tLoss 3.3128 (1.8237)\tTop 1-err 67.1875 (40.1523)\tTop 5-err 37.5000 (18.3828)\n",
            "Epoch: [115/290][299/782]\tLR: 0.008159\tTime 0.147 (0.151)\tData 0.000 (0.001)\tLoss 0.8012 (1.8742)\tTop 1-err 25.0000 (41.1927)\tTop 5-err 1.5625 (19.0365)\n",
            "Epoch: [115/290][399/782]\tLR: 0.008159\tTime 0.146 (0.151)\tData 0.000 (0.001)\tLoss 1.0217 (1.8535)\tTop 1-err 28.1250 (41.0527)\tTop 5-err 4.6875 (18.8535)\n",
            "Epoch: [115/290][499/782]\tLR: 0.008159\tTime 0.151 (0.151)\tData 0.000 (0.001)\tLoss 1.0418 (1.8440)\tTop 1-err 28.1250 (40.8875)\tTop 5-err 9.3750 (18.5641)\n",
            "Epoch: [115/290][599/782]\tLR: 0.008159\tTime 0.153 (0.151)\tData 0.000 (0.000)\tLoss 4.0452 (1.8165)\tTop 1-err 90.6250 (40.4818)\tTop 5-err 76.5625 (18.3711)\n",
            "Epoch: [115/290][699/782]\tLR: 0.008159\tTime 0.149 (0.151)\tData 0.000 (0.000)\tLoss 0.9336 (1.8255)\tTop 1-err 23.4375 (40.7790)\tTop 5-err 6.2500 (18.5480)\n",
            "* Epoch: [115/290]\t Top 1-err 40.949  Top 5-err 18.471\t Train Loss 1.843\n",
            "* Epoch: [115/290]\t Top 1-err 32.150  Top 5-err 8.940\t Test Loss 1.141\n",
            "Current best accuracy (top-1 and 5 error): 30.94 8.33\n",
            "Epoch: [116/290][99/782]\tLR: 0.008230\tTime 0.148 (0.153)\tData 0.000 (0.002)\tLoss 3.2855 (1.8582)\tTop 1-err 43.7500 (40.9922)\tTop 5-err 10.9375 (17.5703)\n",
            "Epoch: [116/290][199/782]\tLR: 0.008230\tTime 0.148 (0.152)\tData 0.000 (0.001)\tLoss 2.5349 (1.9558)\tTop 1-err 45.3125 (42.7109)\tTop 5-err 21.8750 (19.6406)\n",
            "Epoch: [116/290][299/782]\tLR: 0.008230\tTime 0.148 (0.151)\tData 0.000 (0.001)\tLoss 0.7198 (1.9375)\tTop 1-err 25.0000 (42.8151)\tTop 5-err 1.5625 (19.9297)\n",
            "Epoch: [116/290][399/782]\tLR: 0.008230\tTime 0.155 (0.151)\tData 0.000 (0.001)\tLoss 1.8492 (1.9029)\tTop 1-err 34.3750 (42.1387)\tTop 5-err 15.6250 (19.4609)\n",
            "Epoch: [116/290][499/782]\tLR: 0.008230\tTime 0.147 (0.151)\tData 0.000 (0.000)\tLoss 0.9079 (1.8895)\tTop 1-err 25.0000 (41.8766)\tTop 5-err 9.3750 (18.9766)\n",
            "Epoch: [116/290][599/782]\tLR: 0.008230\tTime 0.149 (0.151)\tData 0.000 (0.000)\tLoss 0.9245 (1.8934)\tTop 1-err 26.5625 (41.9062)\tTop 5-err 6.2500 (18.9727)\n",
            "Epoch: [116/290][699/782]\tLR: 0.008230\tTime 0.149 (0.151)\tData 0.000 (0.000)\tLoss 3.8269 (1.8965)\tTop 1-err 76.5625 (42.0246)\tTop 5-err 43.7500 (18.9866)\n",
            "* Epoch: [116/290]\t Top 1-err 42.001  Top 5-err 19.012\t Train Loss 1.894\n",
            "* Epoch: [116/290]\t Top 1-err 30.850  Top 5-err 8.190\t Test Loss 1.087\n",
            "Current best accuracy (top-1 and 5 error): 30.85 8.19\n",
            "Epoch: [117/290][99/782]\tLR: 0.008302\tTime 0.149 (0.153)\tData 0.000 (0.002)\tLoss 3.3049 (1.8060)\tTop 1-err 60.9375 (39.5156)\tTop 5-err 37.5000 (17.2500)\n",
            "Epoch: [117/290][199/782]\tLR: 0.008302\tTime 0.148 (0.152)\tData 0.000 (0.001)\tLoss 0.8709 (1.8370)\tTop 1-err 28.1250 (40.8125)\tTop 5-err 3.1250 (18.8945)\n",
            "Epoch: [117/290][299/782]\tLR: 0.008302\tTime 0.156 (0.151)\tData 0.000 (0.001)\tLoss 0.8907 (1.8179)\tTop 1-err 23.4375 (40.1094)\tTop 5-err 4.6875 (17.8646)\n",
            "Epoch: [117/290][399/782]\tLR: 0.008302\tTime 0.150 (0.151)\tData 0.000 (0.001)\tLoss 2.4480 (1.8062)\tTop 1-err 54.6875 (39.8633)\tTop 5-err 21.8750 (17.6895)\n",
            "Epoch: [117/290][499/782]\tLR: 0.008302\tTime 0.146 (0.151)\tData 0.000 (0.001)\tLoss 1.5859 (1.7943)\tTop 1-err 32.8125 (39.7625)\tTop 5-err 10.9375 (17.4906)\n",
            "Epoch: [117/290][599/782]\tLR: 0.008302\tTime 0.148 (0.151)\tData 0.000 (0.001)\tLoss 0.7920 (1.8061)\tTop 1-err 25.0000 (39.8568)\tTop 5-err 3.1250 (17.4258)\n",
            "Epoch: [117/290][699/782]\tLR: 0.008302\tTime 0.148 (0.151)\tData 0.000 (0.000)\tLoss 0.8160 (1.8000)\tTop 1-err 25.0000 (39.8650)\tTop 5-err 3.1250 (17.3571)\n",
            "* Epoch: [117/290]\t Top 1-err 40.416  Top 5-err 17.904\t Train Loss 1.826\n",
            "* Epoch: [117/290]\t Top 1-err 33.210  Top 5-err 10.000\t Test Loss 1.244\n",
            "Current best accuracy (top-1 and 5 error): 30.85 8.19\n",
            "Epoch: [118/290][99/782]\tLR: 0.008374\tTime 0.150 (0.154)\tData 0.000 (0.002)\tLoss 3.6303 (1.8344)\tTop 1-err 81.2500 (40.4844)\tTop 5-err 51.5625 (18.0938)\n",
            "Epoch: [118/290][199/782]\tLR: 0.008374\tTime 0.154 (0.152)\tData 0.000 (0.001)\tLoss 0.9922 (1.8375)\tTop 1-err 25.0000 (40.1367)\tTop 5-err 7.8125 (17.3477)\n",
            "Epoch: [118/290][299/782]\tLR: 0.008374\tTime 0.149 (0.151)\tData 0.000 (0.001)\tLoss 3.4751 (1.8397)\tTop 1-err 81.2500 (40.3672)\tTop 5-err 39.0625 (17.6667)\n",
            "Epoch: [118/290][399/782]\tLR: 0.008374\tTime 0.149 (0.151)\tData 0.000 (0.001)\tLoss 0.9875 (1.8485)\tTop 1-err 29.6875 (40.7051)\tTop 5-err 6.2500 (18.1875)\n",
            "Epoch: [118/290][499/782]\tLR: 0.008374\tTime 0.147 (0.151)\tData 0.000 (0.001)\tLoss 0.8933 (1.8893)\tTop 1-err 31.2500 (41.4859)\tTop 5-err 6.2500 (18.8641)\n",
            "Epoch: [118/290][599/782]\tLR: 0.008374\tTime 0.145 (0.151)\tData 0.000 (0.000)\tLoss 1.0800 (1.8635)\tTop 1-err 35.9375 (41.0365)\tTop 5-err 4.6875 (18.3372)\n",
            "Epoch: [118/290][699/782]\tLR: 0.008374\tTime 0.148 (0.151)\tData 0.000 (0.000)\tLoss 0.7416 (1.8671)\tTop 1-err 20.3125 (41.0837)\tTop 5-err 7.8125 (18.3036)\n",
            "* Epoch: [118/290]\t Top 1-err 41.312  Top 5-err 18.503\t Train Loss 1.882\n",
            "* Epoch: [118/290]\t Top 1-err 34.710  Top 5-err 10.110\t Test Loss 1.292\n",
            "Current best accuracy (top-1 and 5 error): 30.85 8.19\n",
            "Epoch: [119/290][99/782]\tLR: 0.008446\tTime 0.148 (0.153)\tData 0.000 (0.003)\tLoss 3.1233 (1.8108)\tTop 1-err 89.0625 (38.7344)\tTop 5-err 76.5625 (15.4844)\n",
            "Epoch: [119/290][199/782]\tLR: 0.008446\tTime 0.209 (0.151)\tData 0.000 (0.001)\tLoss 0.7409 (1.8470)\tTop 1-err 25.0000 (40.2695)\tTop 5-err 1.5625 (17.6172)\n",
            "Epoch: [119/290][299/782]\tLR: 0.008446\tTime 0.148 (0.151)\tData 0.000 (0.001)\tLoss 1.5440 (1.8756)\tTop 1-err 32.8125 (41.0833)\tTop 5-err 4.6875 (18.4036)\n",
            "Epoch: [119/290][399/782]\tLR: 0.008446\tTime 0.148 (0.151)\tData 0.000 (0.001)\tLoss 0.8097 (1.8845)\tTop 1-err 23.4375 (41.2793)\tTop 5-err 3.1250 (18.5586)\n",
            "Epoch: [119/290][499/782]\tLR: 0.008446\tTime 0.147 (0.151)\tData 0.000 (0.001)\tLoss 1.0143 (1.8884)\tTop 1-err 28.1250 (41.1203)\tTop 5-err 9.3750 (18.2656)\n",
            "Epoch: [119/290][599/782]\tLR: 0.008446\tTime 0.149 (0.150)\tData 0.000 (0.001)\tLoss 3.8428 (1.8857)\tTop 1-err 96.8750 (41.2057)\tTop 5-err 85.9375 (18.2943)\n",
            "Epoch: [119/290][699/782]\tLR: 0.008446\tTime 0.147 (0.150)\tData 0.000 (0.001)\tLoss 0.8299 (1.8777)\tTop 1-err 25.0000 (41.0089)\tTop 5-err 3.1250 (18.0949)\n",
            "* Epoch: [119/290]\t Top 1-err 40.597  Top 5-err 17.806\t Train Loss 1.856\n",
            "* Epoch: [119/290]\t Top 1-err 31.890  Top 5-err 8.540\t Test Loss 1.124\n",
            "Current best accuracy (top-1 and 5 error): 30.85 8.19\n",
            "Epoch: [120/290][99/782]\tLR: 0.008520\tTime 0.153 (0.153)\tData 0.000 (0.002)\tLoss 0.7061 (1.8346)\tTop 1-err 20.3125 (40.4844)\tTop 5-err 3.1250 (17.8984)\n",
            "Epoch: [120/290][199/782]\tLR: 0.008520\tTime 0.151 (0.152)\tData 0.000 (0.001)\tLoss 0.9956 (1.8722)\tTop 1-err 32.8125 (41.4766)\tTop 5-err 4.6875 (19.2305)\n",
            "Epoch: [120/290][299/782]\tLR: 0.008520\tTime 0.148 (0.151)\tData 0.000 (0.001)\tLoss 3.7427 (1.8877)\tTop 1-err 84.3750 (41.5286)\tTop 5-err 59.3750 (19.2240)\n",
            "Epoch: [120/290][399/782]\tLR: 0.008520\tTime 0.147 (0.151)\tData 0.000 (0.001)\tLoss 0.7810 (1.8939)\tTop 1-err 20.3125 (41.7383)\tTop 5-err 4.6875 (19.0879)\n",
            "Epoch: [120/290][499/782]\tLR: 0.008520\tTime 0.154 (0.151)\tData 0.000 (0.001)\tLoss 1.0550 (1.8591)\tTop 1-err 31.2500 (40.9984)\tTop 5-err 9.3750 (18.4062)\n",
            "Epoch: [120/290][599/782]\tLR: 0.008520\tTime 0.159 (0.151)\tData 0.000 (0.000)\tLoss 3.4020 (1.8499)\tTop 1-err 70.3125 (40.9401)\tTop 5-err 46.8750 (18.2839)\n",
            "Epoch: [120/290][699/782]\tLR: 0.008520\tTime 0.150 (0.151)\tData 0.000 (0.000)\tLoss 3.3103 (1.8323)\tTop 1-err 75.0000 (40.6384)\tTop 5-err 34.3750 (17.9554)\n",
            "* Epoch: [120/290]\t Top 1-err 41.010  Top 5-err 18.345\t Train Loss 1.844\n",
            "* Epoch: [120/290]\t Top 1-err 32.610  Top 5-err 9.100\t Test Loss 1.170\n",
            "Current best accuracy (top-1 and 5 error): 30.85 8.19\n",
            "Epoch: [121/290][99/782]\tLR: 0.008593\tTime 0.149 (0.154)\tData 0.000 (0.002)\tLoss 2.5645 (1.8305)\tTop 1-err 45.3125 (40.5938)\tTop 5-err 17.1875 (19.3594)\n",
            "Epoch: [121/290][199/782]\tLR: 0.008593\tTime 0.148 (0.152)\tData 0.000 (0.001)\tLoss 0.9899 (1.9061)\tTop 1-err 23.4375 (41.7109)\tTop 5-err 10.9375 (19.6602)\n",
            "Epoch: [121/290][299/782]\tLR: 0.008593\tTime 0.148 (0.151)\tData 0.000 (0.001)\tLoss 3.9280 (1.8724)\tTop 1-err 76.5625 (41.3411)\tTop 5-err 48.4375 (19.0417)\n",
            "Epoch: [121/290][399/782]\tLR: 0.008593\tTime 0.150 (0.151)\tData 0.000 (0.001)\tLoss 1.9606 (1.8532)\tTop 1-err 40.6250 (41.0508)\tTop 5-err 9.3750 (18.7363)\n",
            "Epoch: [121/290][499/782]\tLR: 0.008593\tTime 0.157 (0.151)\tData 0.000 (0.001)\tLoss 2.6081 (1.8343)\tTop 1-err 35.9375 (40.7828)\tTop 5-err 15.6250 (18.5016)\n",
            "Epoch: [121/290][599/782]\tLR: 0.008593\tTime 0.148 (0.151)\tData 0.000 (0.001)\tLoss 2.9253 (1.8298)\tTop 1-err 67.1875 (40.6927)\tTop 5-err 32.8125 (18.3867)\n",
            "Epoch: [121/290][699/782]\tLR: 0.008593\tTime 0.149 (0.151)\tData 0.000 (0.000)\tLoss 3.9976 (1.8293)\tTop 1-err 96.8750 (40.5167)\tTop 5-err 81.2500 (18.1038)\n",
            "* Epoch: [121/290]\t Top 1-err 40.710  Top 5-err 18.176\t Train Loss 1.837\n",
            "* Epoch: [121/290]\t Top 1-err 33.080  Top 5-err 9.250\t Test Loss 1.189\n",
            "Current best accuracy (top-1 and 5 error): 30.85 8.19\n",
            "Epoch: [122/290][99/782]\tLR: 0.008668\tTime 0.148 (0.153)\tData 0.000 (0.003)\tLoss 1.0851 (1.7759)\tTop 1-err 26.5625 (38.8203)\tTop 5-err 7.8125 (17.1328)\n",
            "Epoch: [122/290][199/782]\tLR: 0.008668\tTime 0.147 (0.152)\tData 0.000 (0.001)\tLoss 0.9154 (1.8269)\tTop 1-err 21.8750 (40.5000)\tTop 5-err 6.2500 (18.1953)\n",
            "Epoch: [122/290][299/782]\tLR: 0.008668\tTime 0.147 (0.151)\tData 0.000 (0.001)\tLoss 0.9291 (1.8055)\tTop 1-err 25.0000 (40.1198)\tTop 5-err 6.2500 (18.0234)\n",
            "Epoch: [122/290][399/782]\tLR: 0.008668\tTime 0.150 (0.151)\tData 0.000 (0.001)\tLoss 2.7366 (1.8011)\tTop 1-err 45.3125 (40.1406)\tTop 5-err 15.6250 (17.9688)\n",
            "Epoch: [122/290][499/782]\tLR: 0.008668\tTime 0.152 (0.151)\tData 0.000 (0.001)\tLoss 3.2035 (1.8191)\tTop 1-err 68.7500 (40.6844)\tTop 5-err 28.1250 (18.3094)\n",
            "Epoch: [122/290][599/782]\tLR: 0.008668\tTime 0.148 (0.151)\tData 0.000 (0.001)\tLoss 3.0914 (1.8238)\tTop 1-err 60.9375 (40.6667)\tTop 5-err 28.1250 (18.1328)\n",
            "Epoch: [122/290][699/782]\tLR: 0.008668\tTime 0.155 (0.151)\tData 0.000 (0.000)\tLoss 0.9108 (1.8078)\tTop 1-err 25.0000 (40.4554)\tTop 5-err 9.3750 (17.9699)\n",
            "* Epoch: [122/290]\t Top 1-err 40.410  Top 5-err 17.962\t Train Loss 1.804\n",
            "* Epoch: [122/290]\t Top 1-err 31.460  Top 5-err 8.350\t Test Loss 1.101\n",
            "Current best accuracy (top-1 and 5 error): 30.85 8.19\n",
            "Epoch: [123/290][99/782]\tLR: 0.008743\tTime 0.147 (0.154)\tData 0.000 (0.003)\tLoss 0.8353 (1.8366)\tTop 1-err 23.4375 (40.2812)\tTop 5-err 4.6875 (17.6328)\n",
            "Epoch: [123/290][199/782]\tLR: 0.008743\tTime 0.148 (0.152)\tData 0.000 (0.001)\tLoss 1.1981 (1.7761)\tTop 1-err 37.5000 (39.4688)\tTop 5-err 7.8125 (17.0977)\n",
            "Epoch: [123/290][299/782]\tLR: 0.008743\tTime 0.154 (0.151)\tData 0.000 (0.001)\tLoss 3.9834 (1.8014)\tTop 1-err 100.0000 (39.3411)\tTop 5-err 82.8125 (16.9036)\n",
            "Epoch: [123/290][399/782]\tLR: 0.008743\tTime 0.148 (0.152)\tData 0.000 (0.001)\tLoss 4.5301 (1.7842)\tTop 1-err 96.8750 (39.0410)\tTop 5-err 85.9375 (16.7422)\n",
            "Epoch: [123/290][499/782]\tLR: 0.008743\tTime 0.147 (0.151)\tData 0.000 (0.001)\tLoss 0.7819 (1.7791)\tTop 1-err 21.8750 (39.3031)\tTop 5-err 3.1250 (17.0078)\n",
            "Epoch: [123/290][599/782]\tLR: 0.008743\tTime 0.149 (0.151)\tData 0.000 (0.001)\tLoss 3.5584 (1.8215)\tTop 1-err 84.3750 (40.0924)\tTop 5-err 50.0000 (17.6042)\n",
            "Epoch: [123/290][699/782]\tLR: 0.008743\tTime 0.147 (0.151)\tData 0.000 (0.000)\tLoss 0.8972 (1.8380)\tTop 1-err 26.5625 (40.5681)\tTop 5-err 4.6875 (17.8594)\n",
            "* Epoch: [123/290]\t Top 1-err 40.571  Top 5-err 17.851\t Train Loss 1.835\n",
            "* Epoch: [123/290]\t Top 1-err 31.680  Top 5-err 8.450\t Test Loss 1.102\n",
            "Current best accuracy (top-1 and 5 error): 30.85 8.19\n",
            "Epoch: [124/290][99/782]\tLR: 0.008818\tTime 0.147 (0.153)\tData 0.000 (0.002)\tLoss 2.5926 (1.8143)\tTop 1-err 53.1250 (40.3125)\tTop 5-err 23.4375 (18.6406)\n",
            "Epoch: [124/290][199/782]\tLR: 0.008818\tTime 0.152 (0.152)\tData 0.000 (0.001)\tLoss 0.7543 (1.9185)\tTop 1-err 25.0000 (42.0508)\tTop 5-err 4.6875 (20.1367)\n",
            "Epoch: [124/290][299/782]\tLR: 0.008818\tTime 0.150 (0.152)\tData 0.000 (0.001)\tLoss 0.9128 (1.8619)\tTop 1-err 35.9375 (41.0677)\tTop 5-err 1.5625 (18.7917)\n",
            "Epoch: [124/290][399/782]\tLR: 0.008818\tTime 0.149 (0.151)\tData 0.000 (0.001)\tLoss 4.1185 (1.7984)\tTop 1-err 89.0625 (39.8789)\tTop 5-err 64.0625 (17.8535)\n",
            "Epoch: [124/290][499/782]\tLR: 0.008818\tTime 0.146 (0.151)\tData 0.000 (0.000)\tLoss 3.5213 (1.7764)\tTop 1-err 67.1875 (39.3719)\tTop 5-err 37.5000 (17.3094)\n",
            "Epoch: [124/290][599/782]\tLR: 0.008818\tTime 0.147 (0.151)\tData 0.000 (0.000)\tLoss 0.8312 (1.7766)\tTop 1-err 17.1875 (39.5521)\tTop 5-err 6.2500 (17.3737)\n",
            "Epoch: [124/290][699/782]\tLR: 0.008818\tTime 0.149 (0.151)\tData 0.000 (0.000)\tLoss 0.6552 (1.7655)\tTop 1-err 17.1875 (39.4855)\tTop 5-err 1.5625 (17.2199)\n",
            "* Epoch: [124/290]\t Top 1-err 39.680  Top 5-err 17.286\t Train Loss 1.776\n",
            "* Epoch: [124/290]\t Top 1-err 31.670  Top 5-err 9.160\t Test Loss 1.153\n",
            "Current best accuracy (top-1 and 5 error): 30.85 8.19\n",
            "Epoch: [125/290][99/782]\tLR: 0.008894\tTime 0.154 (0.153)\tData 0.000 (0.002)\tLoss 0.8697 (1.7087)\tTop 1-err 25.0000 (39.1328)\tTop 5-err 7.8125 (16.9453)\n",
            "Epoch: [125/290][199/782]\tLR: 0.008894\tTime 0.155 (0.152)\tData 0.000 (0.001)\tLoss 0.8393 (1.7635)\tTop 1-err 23.4375 (39.6328)\tTop 5-err 1.5625 (17.3398)\n",
            "Epoch: [125/290][299/782]\tLR: 0.008894\tTime 0.155 (0.152)\tData 0.000 (0.001)\tLoss 2.0208 (1.7670)\tTop 1-err 40.6250 (39.2318)\tTop 5-err 9.3750 (16.7943)\n",
            "Epoch: [125/290][399/782]\tLR: 0.008894\tTime 0.153 (0.151)\tData 0.000 (0.001)\tLoss 0.8844 (1.7860)\tTop 1-err 25.0000 (39.7734)\tTop 5-err 6.2500 (17.2637)\n",
            "Epoch: [125/290][499/782]\tLR: 0.008894\tTime 0.148 (0.151)\tData 0.000 (0.001)\tLoss 0.9482 (1.8274)\tTop 1-err 31.2500 (40.5359)\tTop 5-err 4.6875 (17.8766)\n",
            "Epoch: [125/290][599/782]\tLR: 0.008894\tTime 0.155 (0.151)\tData 0.000 (0.001)\tLoss 2.6136 (1.8573)\tTop 1-err 48.4375 (41.0195)\tTop 5-err 28.1250 (18.2734)\n",
            "Epoch: [125/290][699/782]\tLR: 0.008894\tTime 0.157 (0.151)\tData 0.000 (0.000)\tLoss 3.1891 (1.8378)\tTop 1-err 68.7500 (40.6763)\tTop 5-err 39.0625 (17.9922)\n",
            "* Epoch: [125/290]\t Top 1-err 40.545  Top 5-err 17.878\t Train Loss 1.830\n",
            "* Epoch: [125/290]\t Top 1-err 31.180  Top 5-err 8.630\t Test Loss 1.115\n",
            "Current best accuracy (top-1 and 5 error): 30.85 8.19\n",
            "Epoch: [126/290][99/782]\tLR: 0.008971\tTime 0.151 (0.154)\tData 0.000 (0.002)\tLoss 3.6628 (1.8056)\tTop 1-err 82.8125 (39.6797)\tTop 5-err 57.8125 (16.8984)\n",
            "Epoch: [126/290][199/782]\tLR: 0.008971\tTime 0.149 (0.153)\tData 0.000 (0.001)\tLoss 3.4718 (1.8393)\tTop 1-err 76.5625 (40.3320)\tTop 5-err 45.3125 (17.8125)\n",
            "Epoch: [126/290][299/782]\tLR: 0.008971\tTime 0.148 (0.152)\tData 0.000 (0.001)\tLoss 3.2349 (1.8253)\tTop 1-err 70.3125 (40.2995)\tTop 5-err 29.6875 (17.6771)\n",
            "Epoch: [126/290][399/782]\tLR: 0.008971\tTime 0.147 (0.152)\tData 0.000 (0.001)\tLoss 0.8349 (1.7957)\tTop 1-err 28.1250 (39.6348)\tTop 5-err 3.1250 (16.9160)\n",
            "Epoch: [126/290][499/782]\tLR: 0.008971\tTime 0.146 (0.152)\tData 0.000 (0.001)\tLoss 1.0050 (1.7800)\tTop 1-err 25.0000 (39.3328)\tTop 5-err 4.6875 (16.7453)\n",
            "Epoch: [126/290][599/782]\tLR: 0.008971\tTime 0.149 (0.152)\tData 0.000 (0.000)\tLoss 0.8791 (1.8070)\tTop 1-err 28.1250 (39.9362)\tTop 5-err 4.6875 (17.2826)\n",
            "Epoch: [126/290][699/782]\tLR: 0.008971\tTime 0.151 (0.151)\tData 0.000 (0.000)\tLoss 4.4904 (1.7911)\tTop 1-err 85.9375 (39.6674)\tTop 5-err 70.3125 (16.9509)\n",
            "* Epoch: [126/290]\t Top 1-err 40.157  Top 5-err 17.352\t Train Loss 1.810\n",
            "* Epoch: [126/290]\t Top 1-err 32.010  Top 5-err 8.620\t Test Loss 1.110\n",
            "Current best accuracy (top-1 and 5 error): 30.85 8.19\n",
            "Epoch: [127/290][99/782]\tLR: 0.009048\tTime 0.159 (0.152)\tData 0.000 (0.002)\tLoss 2.1394 (1.9392)\tTop 1-err 57.8125 (41.7422)\tTop 5-err 18.7500 (19.8047)\n",
            "Epoch: [127/290][199/782]\tLR: 0.009048\tTime 0.145 (0.152)\tData 0.000 (0.001)\tLoss 1.0405 (1.8596)\tTop 1-err 28.1250 (39.9219)\tTop 5-err 4.6875 (17.9805)\n",
            "Epoch: [127/290][299/782]\tLR: 0.009048\tTime 0.149 (0.151)\tData 0.000 (0.001)\tLoss 2.1120 (1.8662)\tTop 1-err 29.6875 (40.2760)\tTop 5-err 9.3750 (18.1901)\n",
            "Epoch: [127/290][399/782]\tLR: 0.009048\tTime 0.150 (0.151)\tData 0.000 (0.001)\tLoss 4.0786 (1.8850)\tTop 1-err 87.5000 (40.7812)\tTop 5-err 64.0625 (18.5645)\n",
            "Epoch: [127/290][499/782]\tLR: 0.009048\tTime 0.155 (0.151)\tData 0.000 (0.001)\tLoss 0.9797 (1.8894)\tTop 1-err 23.4375 (41.0500)\tTop 5-err 4.6875 (18.7312)\n",
            "Epoch: [127/290][599/782]\tLR: 0.009048\tTime 0.149 (0.151)\tData 0.000 (0.000)\tLoss 1.0513 (1.8958)\tTop 1-err 37.5000 (41.3646)\tTop 5-err 6.2500 (18.8880)\n",
            "Epoch: [127/290][699/782]\tLR: 0.009048\tTime 0.160 (0.151)\tData 0.000 (0.000)\tLoss 0.9435 (1.8581)\tTop 1-err 29.6875 (40.7991)\tTop 5-err 4.6875 (18.4152)\n",
            "* Epoch: [127/290]\t Top 1-err 41.057  Top 5-err 18.579\t Train Loss 1.881\n",
            "* Epoch: [127/290]\t Top 1-err 33.690  Top 5-err 9.690\t Test Loss 1.223\n",
            "Current best accuracy (top-1 and 5 error): 30.85 8.19\n",
            "Epoch: [128/290][99/782]\tLR: 0.009126\tTime 0.149 (0.153)\tData 0.000 (0.003)\tLoss 0.8507 (1.7002)\tTop 1-err 23.4375 (38.1406)\tTop 5-err 3.1250 (15.9453)\n",
            "Epoch: [128/290][199/782]\tLR: 0.009126\tTime 0.146 (0.152)\tData 0.000 (0.001)\tLoss 1.0501 (1.7850)\tTop 1-err 28.1250 (39.8945)\tTop 5-err 10.9375 (17.5547)\n",
            "Epoch: [128/290][299/782]\tLR: 0.009126\tTime 0.153 (0.152)\tData 0.000 (0.001)\tLoss 0.7760 (1.8189)\tTop 1-err 21.8750 (40.5286)\tTop 5-err 3.1250 (17.8880)\n",
            "Epoch: [128/290][399/782]\tLR: 0.009126\tTime 0.158 (0.152)\tData 0.000 (0.001)\tLoss 1.1650 (1.8596)\tTop 1-err 37.5000 (41.1660)\tTop 5-err 6.2500 (18.3867)\n",
            "Epoch: [128/290][499/782]\tLR: 0.009126\tTime 0.149 (0.152)\tData 0.000 (0.001)\tLoss 1.0123 (1.8509)\tTop 1-err 31.2500 (40.9250)\tTop 5-err 10.9375 (18.2453)\n",
            "Epoch: [128/290][599/782]\tLR: 0.009126\tTime 0.156 (0.152)\tData 0.000 (0.001)\tLoss 4.9302 (1.8489)\tTop 1-err 87.5000 (41.0365)\tTop 5-err 73.4375 (18.3932)\n",
            "Epoch: [128/290][699/782]\tLR: 0.009126\tTime 0.151 (0.151)\tData 0.000 (0.000)\tLoss 0.7070 (1.8448)\tTop 1-err 21.8750 (41.0558)\tTop 5-err 1.5625 (18.4542)\n",
            "* Epoch: [128/290]\t Top 1-err 41.465  Top 5-err 18.761\t Train Loss 1.867\n",
            "* Epoch: [128/290]\t Top 1-err 31.120  Top 5-err 8.120\t Test Loss 1.084\n",
            "Current best accuracy (top-1 and 5 error): 30.85 8.19\n",
            "Epoch: [129/290][99/782]\tLR: 0.009204\tTime 0.149 (0.153)\tData 0.000 (0.002)\tLoss 1.4828 (1.7715)\tTop 1-err 35.9375 (38.2812)\tTop 5-err 12.5000 (15.5938)\n",
            "Epoch: [129/290][199/782]\tLR: 0.009204\tTime 0.149 (0.152)\tData 0.000 (0.001)\tLoss 3.3942 (1.8558)\tTop 1-err 78.1250 (40.2188)\tTop 5-err 59.3750 (17.1953)\n",
            "Epoch: [129/290][299/782]\tLR: 0.009204\tTime 0.147 (0.151)\tData 0.000 (0.001)\tLoss 0.9395 (1.8388)\tTop 1-err 29.6875 (40.3516)\tTop 5-err 3.1250 (17.6276)\n",
            "Epoch: [129/290][399/782]\tLR: 0.009204\tTime 0.153 (0.151)\tData 0.000 (0.001)\tLoss 0.9179 (1.8656)\tTop 1-err 28.1250 (40.9102)\tTop 5-err 1.5625 (17.9961)\n",
            "Epoch: [129/290][499/782]\tLR: 0.009204\tTime 0.159 (0.151)\tData 0.000 (0.000)\tLoss 3.8788 (1.8646)\tTop 1-err 87.5000 (41.0891)\tTop 5-err 64.0625 (18.3172)\n",
            "Epoch: [129/290][599/782]\tLR: 0.009204\tTime 0.147 (0.151)\tData 0.000 (0.000)\tLoss 0.7558 (1.8500)\tTop 1-err 26.5625 (40.7839)\tTop 5-err 4.6875 (18.1471)\n",
            "Epoch: [129/290][699/782]\tLR: 0.009204\tTime 0.152 (0.151)\tData 0.000 (0.000)\tLoss 0.9799 (1.8406)\tTop 1-err 25.0000 (40.6261)\tTop 5-err 7.8125 (18.0067)\n",
            "* Epoch: [129/290]\t Top 1-err 40.927  Top 5-err 18.257\t Train Loss 1.853\n",
            "* Epoch: [129/290]\t Top 1-err 32.610  Top 5-err 9.180\t Test Loss 1.171\n",
            "Current best accuracy (top-1 and 5 error): 30.85 8.19\n",
            "Epoch: [130/290][99/782]\tLR: 0.009283\tTime 0.148 (0.154)\tData 0.000 (0.002)\tLoss 0.9378 (1.7516)\tTop 1-err 25.0000 (38.9062)\tTop 5-err 6.2500 (17.1875)\n",
            "Epoch: [130/290][199/782]\tLR: 0.009283\tTime 0.147 (0.152)\tData 0.000 (0.001)\tLoss 0.9067 (1.8994)\tTop 1-err 25.0000 (41.7500)\tTop 5-err 7.8125 (19.0781)\n",
            "Epoch: [130/290][299/782]\tLR: 0.009283\tTime 0.147 (0.152)\tData 0.000 (0.001)\tLoss 4.0909 (1.8484)\tTop 1-err 84.3750 (40.5755)\tTop 5-err 70.3125 (17.8750)\n",
            "Epoch: [130/290][399/782]\tLR: 0.009283\tTime 0.153 (0.152)\tData 0.000 (0.001)\tLoss 1.4581 (1.8498)\tTop 1-err 39.0625 (40.7715)\tTop 5-err 10.9375 (17.7793)\n",
            "Epoch: [130/290][499/782]\tLR: 0.009283\tTime 0.148 (0.152)\tData 0.000 (0.000)\tLoss 0.9769 (1.8757)\tTop 1-err 25.0000 (41.2141)\tTop 5-err 7.8125 (18.1859)\n",
            "Epoch: [130/290][599/782]\tLR: 0.009283\tTime 0.149 (0.152)\tData 0.000 (0.000)\tLoss 2.8664 (1.8748)\tTop 1-err 54.6875 (41.0768)\tTop 5-err 21.8750 (18.1120)\n",
            "Epoch: [130/290][699/782]\tLR: 0.009283\tTime 0.155 (0.151)\tData 0.000 (0.000)\tLoss 0.7399 (1.8766)\tTop 1-err 20.3125 (41.1116)\tTop 5-err 4.6875 (18.0413)\n",
            "* Epoch: [130/290]\t Top 1-err 40.629  Top 5-err 17.730\t Train Loss 1.845\n",
            "* Epoch: [130/290]\t Top 1-err 30.450  Top 5-err 7.700\t Test Loss 1.059\n",
            "Current best accuracy (top-1 and 5 error): 30.45 7.7\n",
            "Epoch: [131/290][99/782]\tLR: 0.009362\tTime 0.147 (0.153)\tData 0.000 (0.002)\tLoss 1.0971 (1.8391)\tTop 1-err 34.3750 (40.2656)\tTop 5-err 7.8125 (17.3672)\n",
            "Epoch: [131/290][199/782]\tLR: 0.009362\tTime 0.148 (0.152)\tData 0.000 (0.001)\tLoss 0.8747 (1.8799)\tTop 1-err 21.8750 (41.3867)\tTop 5-err 4.6875 (18.8164)\n",
            "Epoch: [131/290][299/782]\tLR: 0.009362\tTime 0.147 (0.151)\tData 0.000 (0.001)\tLoss 0.6676 (1.8968)\tTop 1-err 20.3125 (41.1302)\tTop 5-err 3.1250 (18.5964)\n",
            "Epoch: [131/290][399/782]\tLR: 0.009362\tTime 0.147 (0.151)\tData 0.000 (0.001)\tLoss 0.8334 (1.8756)\tTop 1-err 21.8750 (40.6152)\tTop 5-err 6.2500 (18.0684)\n",
            "Epoch: [131/290][499/782]\tLR: 0.009362\tTime 0.147 (0.151)\tData 0.000 (0.000)\tLoss 0.8528 (1.8645)\tTop 1-err 21.8750 (40.5969)\tTop 5-err 6.2500 (17.9781)\n",
            "Epoch: [131/290][599/782]\tLR: 0.009362\tTime 0.127 (0.151)\tData 0.000 (0.000)\tLoss 0.7678 (1.8670)\tTop 1-err 21.8750 (40.7786)\tTop 5-err 1.5625 (18.2174)\n",
            "Epoch: [131/290][699/782]\tLR: 0.009362\tTime 0.149 (0.151)\tData 0.000 (0.000)\tLoss 3.1762 (1.8464)\tTop 1-err 51.5625 (40.3795)\tTop 5-err 23.4375 (17.9096)\n",
            "* Epoch: [131/290]\t Top 1-err 40.188  Top 5-err 17.641\t Train Loss 1.834\n",
            "* Epoch: [131/290]\t Top 1-err 30.400  Top 5-err 7.790\t Test Loss 1.056\n",
            "Current best accuracy (top-1 and 5 error): 30.4 7.79\n",
            "Epoch: [132/290][99/782]\tLR: 0.009442\tTime 0.150 (0.155)\tData 0.000 (0.002)\tLoss 2.8545 (1.6203)\tTop 1-err 60.9375 (36.2812)\tTop 5-err 20.3125 (14.2500)\n",
            "Epoch: [132/290][199/782]\tLR: 0.009442\tTime 0.147 (0.152)\tData 0.000 (0.001)\tLoss 3.2014 (1.8410)\tTop 1-err 46.8750 (40.2266)\tTop 5-err 25.0000 (17.6914)\n",
            "Epoch: [132/290][299/782]\tLR: 0.009442\tTime 0.147 (0.152)\tData 0.000 (0.001)\tLoss 3.4178 (1.8532)\tTop 1-err 79.6875 (41.0521)\tTop 5-err 43.7500 (18.7474)\n",
            "Epoch: [132/290][399/782]\tLR: 0.009442\tTime 0.148 (0.152)\tData 0.000 (0.001)\tLoss 2.9232 (1.8576)\tTop 1-err 67.1875 (41.1348)\tTop 5-err 45.3125 (18.5742)\n",
            "Epoch: [132/290][499/782]\tLR: 0.009442\tTime 0.147 (0.152)\tData 0.000 (0.001)\tLoss 0.8175 (1.8588)\tTop 1-err 26.5625 (41.1000)\tTop 5-err 3.1250 (18.5781)\n",
            "Epoch: [132/290][599/782]\tLR: 0.009442\tTime 0.149 (0.152)\tData 0.000 (0.000)\tLoss 1.0773 (1.8524)\tTop 1-err 20.3125 (41.2070)\tTop 5-err 10.9375 (18.7292)\n",
            "Epoch: [132/290][699/782]\tLR: 0.009442\tTime 0.148 (0.151)\tData 0.000 (0.000)\tLoss 0.8156 (1.8509)\tTop 1-err 25.0000 (41.2277)\tTop 5-err 3.1250 (18.6853)\n",
            "* Epoch: [132/290]\t Top 1-err 41.800  Top 5-err 19.272\t Train Loss 1.877\n",
            "* Epoch: [132/290]\t Top 1-err 31.730  Top 5-err 8.780\t Test Loss 1.118\n",
            "Current best accuracy (top-1 and 5 error): 30.4 7.79\n",
            "Epoch: [133/290][99/782]\tLR: 0.009523\tTime 0.145 (0.152)\tData 0.000 (0.002)\tLoss 0.8121 (1.9370)\tTop 1-err 25.0000 (40.4766)\tTop 5-err 3.1250 (17.9531)\n",
            "Epoch: [133/290][199/782]\tLR: 0.009523\tTime 0.152 (0.150)\tData 0.000 (0.001)\tLoss 2.9142 (2.0119)\tTop 1-err 56.2500 (42.7031)\tTop 5-err 32.8125 (19.5234)\n",
            "Epoch: [133/290][299/782]\tLR: 0.009523\tTime 0.156 (0.150)\tData 0.000 (0.001)\tLoss 2.7747 (1.9518)\tTop 1-err 96.8750 (42.0365)\tTop 5-err 84.3750 (19.3490)\n",
            "Epoch: [133/290][399/782]\tLR: 0.009523\tTime 0.153 (0.150)\tData 0.000 (0.001)\tLoss 1.1485 (1.9419)\tTop 1-err 34.3750 (42.0059)\tTop 5-err 7.8125 (19.1992)\n",
            "Epoch: [133/290][499/782]\tLR: 0.009523\tTime 0.150 (0.150)\tData 0.000 (0.000)\tLoss 3.9871 (1.9324)\tTop 1-err 90.6250 (41.9688)\tTop 5-err 73.4375 (19.2516)\n",
            "Epoch: [133/290][599/782]\tLR: 0.009523\tTime 0.152 (0.150)\tData 0.000 (0.000)\tLoss 1.0216 (1.9411)\tTop 1-err 29.6875 (42.2852)\tTop 5-err 6.2500 (19.6276)\n",
            "Epoch: [133/290][699/782]\tLR: 0.009523\tTime 0.157 (0.150)\tData 0.000 (0.000)\tLoss 0.8509 (1.9434)\tTop 1-err 21.8750 (42.4118)\tTop 5-err 6.2500 (19.7824)\n",
            "* Epoch: [133/290]\t Top 1-err 42.702  Top 5-err 20.002\t Train Loss 1.947\n",
            "* Epoch: [133/290]\t Top 1-err 31.280  Top 5-err 8.510\t Test Loss 1.115\n",
            "Current best accuracy (top-1 and 5 error): 30.4 7.79\n",
            "Epoch: [134/290][99/782]\tLR: 0.009604\tTime 0.149 (0.154)\tData 0.000 (0.002)\tLoss 1.0410 (1.7344)\tTop 1-err 34.3750 (39.2188)\tTop 5-err 4.6875 (17.4219)\n",
            "Epoch: [134/290][199/782]\tLR: 0.009604\tTime 0.148 (0.152)\tData 0.000 (0.001)\tLoss 0.8196 (1.8702)\tTop 1-err 25.0000 (41.0312)\tTop 5-err 7.8125 (18.7773)\n",
            "Epoch: [134/290][299/782]\tLR: 0.009604\tTime 0.148 (0.152)\tData 0.000 (0.001)\tLoss 0.8149 (1.8437)\tTop 1-err 20.3125 (40.7318)\tTop 5-err 4.6875 (18.4089)\n",
            "Epoch: [134/290][399/782]\tLR: 0.009604\tTime 0.148 (0.151)\tData 0.000 (0.001)\tLoss 0.9668 (1.8315)\tTop 1-err 28.1250 (40.5488)\tTop 5-err 7.8125 (18.2402)\n",
            "Epoch: [134/290][499/782]\tLR: 0.009604\tTime 0.149 (0.151)\tData 0.000 (0.000)\tLoss 1.0061 (1.8315)\tTop 1-err 31.2500 (40.5297)\tTop 5-err 7.8125 (18.2500)\n",
            "Epoch: [134/290][599/782]\tLR: 0.009604\tTime 0.149 (0.151)\tData 0.000 (0.000)\tLoss 0.7287 (1.8174)\tTop 1-err 21.8750 (40.4193)\tTop 5-err 3.1250 (18.1042)\n",
            "Epoch: [134/290][699/782]\tLR: 0.009604\tTime 0.149 (0.151)\tData 0.000 (0.000)\tLoss 1.8835 (1.8417)\tTop 1-err 50.0000 (40.6886)\tTop 5-err 17.1875 (18.2087)\n",
            "* Epoch: [134/290]\t Top 1-err 40.674  Top 5-err 18.158\t Train Loss 1.838\n",
            "* Epoch: [134/290]\t Top 1-err 31.060  Top 5-err 8.030\t Test Loss 1.072\n",
            "Current best accuracy (top-1 and 5 error): 30.4 7.79\n",
            "Epoch: [135/290][99/782]\tLR: 0.009686\tTime 0.148 (0.154)\tData 0.000 (0.002)\tLoss 0.8997 (1.7747)\tTop 1-err 29.6875 (39.5781)\tTop 5-err 6.2500 (16.8516)\n",
            "Epoch: [135/290][199/782]\tLR: 0.009686\tTime 0.149 (0.152)\tData 0.000 (0.001)\tLoss 0.7182 (1.7690)\tTop 1-err 25.0000 (39.6523)\tTop 5-err 0.0000 (17.7500)\n",
            "Epoch: [135/290][299/782]\tLR: 0.009686\tTime 0.147 (0.152)\tData 0.000 (0.001)\tLoss 0.9782 (1.8187)\tTop 1-err 26.5625 (40.5078)\tTop 5-err 4.6875 (18.2370)\n",
            "Epoch: [135/290][399/782]\tLR: 0.009686\tTime 0.157 (0.152)\tData 0.000 (0.001)\tLoss 1.5870 (1.8172)\tTop 1-err 40.6250 (40.3125)\tTop 5-err 9.3750 (18.0020)\n",
            "Epoch: [135/290][499/782]\tLR: 0.009686\tTime 0.155 (0.152)\tData 0.000 (0.000)\tLoss 0.8288 (1.8337)\tTop 1-err 28.1250 (40.8766)\tTop 5-err 4.6875 (18.3000)\n",
            "Epoch: [135/290][599/782]\tLR: 0.009686\tTime 0.150 (0.152)\tData 0.000 (0.000)\tLoss 1.1441 (1.8177)\tTop 1-err 29.6875 (40.5091)\tTop 5-err 7.8125 (17.8906)\n",
            "Epoch: [135/290][699/782]\tLR: 0.009686\tTime 0.149 (0.152)\tData 0.000 (0.000)\tLoss 4.2275 (1.8266)\tTop 1-err 87.5000 (40.6317)\tTop 5-err 51.5625 (17.8739)\n",
            "* Epoch: [135/290]\t Top 1-err 40.739  Top 5-err 18.020\t Train Loss 1.841\n",
            "* Epoch: [135/290]\t Top 1-err 31.250  Top 5-err 8.330\t Test Loss 1.095\n",
            "Current best accuracy (top-1 and 5 error): 30.4 7.79\n",
            "Epoch: [136/290][99/782]\tLR: 0.009768\tTime 0.147 (0.153)\tData 0.000 (0.002)\tLoss 0.7840 (1.8633)\tTop 1-err 23.4375 (39.7812)\tTop 5-err 3.1250 (17.6953)\n",
            "Epoch: [136/290][199/782]\tLR: 0.009768\tTime 0.148 (0.152)\tData 0.000 (0.001)\tLoss 2.6815 (1.8290)\tTop 1-err 48.4375 (39.3281)\tTop 5-err 26.5625 (17.4570)\n",
            "Epoch: [136/290][299/782]\tLR: 0.009768\tTime 0.156 (0.152)\tData 0.000 (0.001)\tLoss 0.9591 (1.8423)\tTop 1-err 26.5625 (40.3047)\tTop 5-err 3.1250 (18.2135)\n",
            "Epoch: [136/290][399/782]\tLR: 0.009768\tTime 0.147 (0.151)\tData 0.000 (0.001)\tLoss 0.6989 (1.8360)\tTop 1-err 21.8750 (40.1250)\tTop 5-err 1.5625 (17.9707)\n",
            "Epoch: [136/290][499/782]\tLR: 0.009768\tTime 0.148 (0.151)\tData 0.000 (0.001)\tLoss 3.0389 (1.8404)\tTop 1-err 62.5000 (40.4359)\tTop 5-err 39.0625 (18.4203)\n",
            "Epoch: [136/290][599/782]\tLR: 0.009768\tTime 0.148 (0.151)\tData 0.000 (0.000)\tLoss 0.8240 (1.8209)\tTop 1-err 25.0000 (40.0677)\tTop 5-err 4.6875 (17.9479)\n",
            "Epoch: [136/290][699/782]\tLR: 0.009768\tTime 0.145 (0.151)\tData 0.000 (0.000)\tLoss 3.4498 (1.8426)\tTop 1-err 65.6250 (40.5904)\tTop 5-err 42.1875 (18.2634)\n",
            "* Epoch: [136/290]\t Top 1-err 40.673  Top 5-err 18.349\t Train Loss 1.844\n",
            "* Epoch: [136/290]\t Top 1-err 31.940  Top 5-err 8.630\t Test Loss 1.117\n",
            "Current best accuracy (top-1 and 5 error): 30.4 7.79\n",
            "Epoch: [137/290][99/782]\tLR: 0.009851\tTime 0.150 (0.153)\tData 0.000 (0.002)\tLoss 3.6506 (1.9251)\tTop 1-err 62.5000 (41.1641)\tTop 5-err 32.8125 (18.0156)\n",
            "Epoch: [137/290][199/782]\tLR: 0.009851\tTime 0.149 (0.152)\tData 0.000 (0.001)\tLoss 0.8746 (1.9310)\tTop 1-err 29.6875 (42.3984)\tTop 5-err 6.2500 (19.3906)\n",
            "Epoch: [137/290][299/782]\tLR: 0.009851\tTime 0.150 (0.152)\tData 0.000 (0.001)\tLoss 1.8513 (1.8832)\tTop 1-err 42.1875 (41.2161)\tTop 5-err 10.9375 (18.3568)\n",
            "Epoch: [137/290][399/782]\tLR: 0.009851\tTime 0.148 (0.151)\tData 0.000 (0.001)\tLoss 0.9547 (1.8857)\tTop 1-err 28.1250 (41.0391)\tTop 5-err 7.8125 (18.2969)\n",
            "Epoch: [137/290][499/782]\tLR: 0.009851\tTime 0.150 (0.151)\tData 0.000 (0.001)\tLoss 4.0306 (1.8838)\tTop 1-err 90.6250 (41.0969)\tTop 5-err 68.7500 (18.1469)\n",
            "Epoch: [137/290][599/782]\tLR: 0.009851\tTime 0.157 (0.151)\tData 0.000 (0.001)\tLoss 0.9560 (1.8788)\tTop 1-err 25.0000 (41.0690)\tTop 5-err 10.9375 (18.2786)\n",
            "Epoch: [137/290][699/782]\tLR: 0.009851\tTime 0.148 (0.151)\tData 0.000 (0.000)\tLoss 0.8118 (1.8621)\tTop 1-err 28.1250 (40.9967)\tTop 5-err 6.2500 (18.3984)\n",
            "* Epoch: [137/290]\t Top 1-err 41.444  Top 5-err 18.691\t Train Loss 1.881\n",
            "* Epoch: [137/290]\t Top 1-err 31.890  Top 5-err 8.570\t Test Loss 1.149\n",
            "Current best accuracy (top-1 and 5 error): 30.4 7.79\n",
            "Epoch: [138/290][99/782]\tLR: 0.009934\tTime 0.159 (0.153)\tData 0.000 (0.003)\tLoss 3.7928 (1.7883)\tTop 1-err 65.6250 (39.4453)\tTop 5-err 37.5000 (17.9141)\n",
            "Epoch: [138/290][199/782]\tLR: 0.009934\tTime 0.149 (0.152)\tData 0.000 (0.001)\tLoss 3.5696 (1.7503)\tTop 1-err 65.6250 (38.6250)\tTop 5-err 39.0625 (16.7461)\n",
            "Epoch: [138/290][299/782]\tLR: 0.009934\tTime 0.156 (0.152)\tData 0.000 (0.001)\tLoss 0.9055 (1.8351)\tTop 1-err 25.0000 (40.3281)\tTop 5-err 3.1250 (18.2188)\n",
            "Epoch: [138/290][399/782]\tLR: 0.009934\tTime 0.155 (0.152)\tData 0.000 (0.001)\tLoss 2.9064 (1.8287)\tTop 1-err 46.8750 (40.3398)\tTop 5-err 15.6250 (18.1719)\n",
            "Epoch: [138/290][499/782]\tLR: 0.009934\tTime 0.162 (0.152)\tData 0.000 (0.001)\tLoss 3.2179 (1.8380)\tTop 1-err 50.0000 (40.3469)\tTop 5-err 35.9375 (18.0891)\n",
            "Epoch: [138/290][599/782]\tLR: 0.009934\tTime 0.149 (0.152)\tData 0.000 (0.001)\tLoss 0.9240 (1.8350)\tTop 1-err 28.1250 (40.2487)\tTop 5-err 6.2500 (17.9388)\n",
            "Epoch: [138/290][699/782]\tLR: 0.009934\tTime 0.147 (0.152)\tData 0.000 (0.000)\tLoss 3.9132 (1.8438)\tTop 1-err 96.8750 (40.4799)\tTop 5-err 79.6875 (18.1518)\n",
            "* Epoch: [138/290]\t Top 1-err 40.595  Top 5-err 18.248\t Train Loss 1.843\n",
            "* Epoch: [138/290]\t Top 1-err 33.580  Top 5-err 9.090\t Test Loss 1.182\n",
            "Current best accuracy (top-1 and 5 error): 30.4 7.79\n",
            "Epoch: [139/290][99/782]\tLR: 0.010018\tTime 0.146 (0.153)\tData 0.000 (0.002)\tLoss 0.6231 (1.6228)\tTop 1-err 17.1875 (36.8281)\tTop 5-err 3.1250 (15.5312)\n",
            "Epoch: [139/290][199/782]\tLR: 0.010018\tTime 0.156 (0.152)\tData 0.000 (0.001)\tLoss 3.1159 (1.6527)\tTop 1-err 53.1250 (37.1602)\tTop 5-err 31.2500 (16.0000)\n",
            "Epoch: [139/290][299/782]\tLR: 0.010018\tTime 0.150 (0.152)\tData 0.000 (0.001)\tLoss 1.0802 (1.6768)\tTop 1-err 31.2500 (37.6146)\tTop 5-err 7.8125 (15.8932)\n",
            "Epoch: [139/290][399/782]\tLR: 0.010018\tTime 0.155 (0.152)\tData 0.000 (0.001)\tLoss 0.8433 (1.7170)\tTop 1-err 25.0000 (38.3965)\tTop 5-err 3.1250 (16.4082)\n",
            "Epoch: [139/290][499/782]\tLR: 0.010018\tTime 0.150 (0.151)\tData 0.000 (0.001)\tLoss 2.2437 (1.7549)\tTop 1-err 48.4375 (39.2500)\tTop 5-err 23.4375 (17.2125)\n",
            "Epoch: [139/290][599/782]\tLR: 0.010018\tTime 0.149 (0.151)\tData 0.000 (0.000)\tLoss 0.8585 (1.7481)\tTop 1-err 25.0000 (39.2305)\tTop 5-err 3.1250 (17.0651)\n",
            "Epoch: [139/290][699/782]\tLR: 0.010018\tTime 0.153 (0.151)\tData 0.000 (0.000)\tLoss 1.0799 (1.7614)\tTop 1-err 28.1250 (39.4196)\tTop 5-err 10.9375 (17.1038)\n",
            "* Epoch: [139/290]\t Top 1-err 39.227  Top 5-err 16.856\t Train Loss 1.756\n",
            "* Epoch: [139/290]\t Top 1-err 31.450  Top 5-err 8.300\t Test Loss 1.098\n",
            "Current best accuracy (top-1 and 5 error): 30.4 7.79\n",
            "Epoch: [140/290][99/782]\tLR: 0.010102\tTime 0.158 (0.154)\tData 0.000 (0.002)\tLoss 2.3509 (2.0805)\tTop 1-err 56.2500 (45.3828)\tTop 5-err 25.0000 (21.9453)\n",
            "Epoch: [140/290][199/782]\tLR: 0.010102\tTime 0.155 (0.153)\tData 0.000 (0.001)\tLoss 0.9077 (1.9399)\tTop 1-err 25.0000 (42.3008)\tTop 5-err 4.6875 (19.7266)\n",
            "Epoch: [140/290][299/782]\tLR: 0.010102\tTime 0.148 (0.152)\tData 0.000 (0.001)\tLoss 3.7380 (1.9092)\tTop 1-err 81.2500 (41.5234)\tTop 5-err 54.6875 (19.1276)\n",
            "Epoch: [140/290][399/782]\tLR: 0.010102\tTime 0.149 (0.152)\tData 0.000 (0.001)\tLoss 3.4192 (1.8917)\tTop 1-err 85.9375 (41.5879)\tTop 5-err 46.8750 (19.3516)\n",
            "Epoch: [140/290][499/782]\tLR: 0.010102\tTime 0.148 (0.151)\tData 0.000 (0.001)\tLoss 3.5133 (1.8611)\tTop 1-err 73.4375 (41.0344)\tTop 5-err 43.7500 (18.6594)\n",
            "Epoch: [140/290][599/782]\tLR: 0.010102\tTime 0.149 (0.151)\tData 0.000 (0.001)\tLoss 3.7814 (1.8773)\tTop 1-err 81.2500 (41.3451)\tTop 5-err 60.9375 (18.8945)\n",
            "Epoch: [140/290][699/782]\tLR: 0.010102\tTime 0.153 (0.151)\tData 0.000 (0.000)\tLoss 2.5310 (1.8701)\tTop 1-err 43.7500 (41.0167)\tTop 5-err 17.1875 (18.5000)\n",
            "* Epoch: [140/290]\t Top 1-err 40.916  Top 5-err 18.350\t Train Loss 1.855\n",
            "* Epoch: [140/290]\t Top 1-err 31.960  Top 5-err 8.960\t Test Loss 1.158\n",
            "Current best accuracy (top-1 and 5 error): 30.4 7.79\n",
            "Epoch: [141/290][99/782]\tLR: 0.010187\tTime 0.158 (0.154)\tData 0.000 (0.002)\tLoss 3.3105 (1.7793)\tTop 1-err 71.8750 (39.3594)\tTop 5-err 45.3125 (18.6406)\n",
            "Epoch: [141/290][199/782]\tLR: 0.010187\tTime 0.152 (0.153)\tData 0.000 (0.001)\tLoss 3.8460 (1.8462)\tTop 1-err 81.2500 (40.5000)\tTop 5-err 59.3750 (18.8438)\n",
            "Epoch: [141/290][299/782]\tLR: 0.010187\tTime 0.148 (0.152)\tData 0.000 (0.001)\tLoss 3.0005 (1.8391)\tTop 1-err 70.3125 (40.6667)\tTop 5-err 40.6250 (18.8984)\n",
            "Epoch: [141/290][399/782]\tLR: 0.010187\tTime 0.150 (0.152)\tData 0.000 (0.001)\tLoss 3.5912 (1.8480)\tTop 1-err 67.1875 (40.6582)\tTop 5-err 51.5625 (18.5703)\n",
            "Epoch: [141/290][499/782]\tLR: 0.010187\tTime 0.149 (0.152)\tData 0.000 (0.000)\tLoss 0.6998 (1.8484)\tTop 1-err 25.0000 (40.7594)\tTop 5-err 3.1250 (18.6313)\n",
            "Epoch: [141/290][599/782]\tLR: 0.010187\tTime 0.147 (0.152)\tData 0.000 (0.000)\tLoss 0.6416 (1.8871)\tTop 1-err 17.1875 (41.5859)\tTop 5-err 3.1250 (19.2565)\n",
            "Epoch: [141/290][699/782]\tLR: 0.010187\tTime 0.156 (0.152)\tData 0.000 (0.000)\tLoss 3.0803 (1.8720)\tTop 1-err 65.6250 (41.1295)\tTop 5-err 29.6875 (18.7433)\n",
            "* Epoch: [141/290]\t Top 1-err 41.178  Top 5-err 18.683\t Train Loss 1.876\n",
            "* Epoch: [141/290]\t Top 1-err 31.010  Top 5-err 8.190\t Test Loss 1.089\n",
            "Current best accuracy (top-1 and 5 error): 30.4 7.79\n",
            "Epoch: [142/290][99/782]\tLR: 0.010273\tTime 0.151 (0.154)\tData 0.000 (0.002)\tLoss 0.9596 (1.7191)\tTop 1-err 25.0000 (38.2109)\tTop 5-err 7.8125 (16.2578)\n",
            "Epoch: [142/290][199/782]\tLR: 0.010273\tTime 0.152 (0.152)\tData 0.000 (0.001)\tLoss 3.2266 (1.7622)\tTop 1-err 85.9375 (39.3438)\tTop 5-err 62.5000 (16.9570)\n",
            "Epoch: [142/290][299/782]\tLR: 0.010273\tTime 0.148 (0.152)\tData 0.000 (0.001)\tLoss 2.9043 (1.8271)\tTop 1-err 31.2500 (39.8073)\tTop 5-err 7.8125 (17.3698)\n",
            "Epoch: [142/290][399/782]\tLR: 0.010273\tTime 0.148 (0.152)\tData 0.000 (0.001)\tLoss 3.4607 (1.8446)\tTop 1-err 73.4375 (40.3945)\tTop 5-err 50.0000 (17.7324)\n",
            "Epoch: [142/290][499/782]\tLR: 0.010273\tTime 0.155 (0.152)\tData 0.000 (0.001)\tLoss 0.7701 (1.8591)\tTop 1-err 31.2500 (40.7203)\tTop 5-err 3.1250 (17.9297)\n",
            "Epoch: [142/290][599/782]\tLR: 0.010273\tTime 0.148 (0.151)\tData 0.000 (0.000)\tLoss 4.5979 (1.8546)\tTop 1-err 93.7500 (40.6914)\tTop 5-err 79.6875 (17.9232)\n",
            "Epoch: [142/290][699/782]\tLR: 0.010273\tTime 0.149 (0.151)\tData 0.000 (0.000)\tLoss 2.7635 (1.8778)\tTop 1-err 54.6875 (41.1496)\tTop 5-err 29.6875 (18.3672)\n",
            "* Epoch: [142/290]\t Top 1-err 41.069  Top 5-err 18.357\t Train Loss 1.876\n",
            "* Epoch: [142/290]\t Top 1-err 32.290  Top 5-err 8.680\t Test Loss 1.135\n",
            "Current best accuracy (top-1 and 5 error): 30.4 7.79\n",
            "Epoch: [143/290][99/782]\tLR: 0.010359\tTime 0.156 (0.154)\tData 0.000 (0.002)\tLoss 0.6498 (1.6707)\tTop 1-err 12.5000 (36.1484)\tTop 5-err 3.1250 (14.4219)\n",
            "Epoch: [143/290][199/782]\tLR: 0.010359\tTime 0.146 (0.152)\tData 0.000 (0.001)\tLoss 0.7578 (1.8265)\tTop 1-err 26.5625 (38.8984)\tTop 5-err 1.5625 (16.6406)\n",
            "Epoch: [143/290][299/782]\tLR: 0.010359\tTime 0.146 (0.151)\tData 0.000 (0.001)\tLoss 0.8551 (1.8089)\tTop 1-err 23.4375 (38.4609)\tTop 5-err 6.2500 (16.1875)\n",
            "Epoch: [143/290][399/782]\tLR: 0.010359\tTime 0.148 (0.151)\tData 0.000 (0.001)\tLoss 2.9111 (1.8187)\tTop 1-err 93.7500 (38.9609)\tTop 5-err 85.9375 (16.5332)\n",
            "Epoch: [143/290][499/782]\tLR: 0.010359\tTime 0.148 (0.151)\tData 0.000 (0.001)\tLoss 0.8954 (1.8362)\tTop 1-err 29.6875 (39.7266)\tTop 5-err 4.6875 (17.0484)\n",
            "Epoch: [143/290][599/782]\tLR: 0.010359\tTime 0.152 (0.151)\tData 0.000 (0.000)\tLoss 3.7811 (1.8670)\tTop 1-err 87.5000 (40.5312)\tTop 5-err 67.1875 (17.8776)\n",
            "Epoch: [143/290][699/782]\tLR: 0.010359\tTime 0.145 (0.150)\tData 0.000 (0.000)\tLoss 0.7422 (1.8291)\tTop 1-err 26.5625 (40.0301)\tTop 5-err 3.1250 (17.4643)\n",
            "* Epoch: [143/290]\t Top 1-err 39.968  Top 5-err 17.335\t Train Loss 1.819\n",
            "* Epoch: [143/290]\t Top 1-err 31.320  Top 5-err 8.560\t Test Loss 1.101\n",
            "Current best accuracy (top-1 and 5 error): 30.4 7.79\n",
            "Epoch: [144/290][99/782]\tLR: 0.010445\tTime 0.146 (0.151)\tData 0.000 (0.002)\tLoss 0.7953 (1.8261)\tTop 1-err 29.6875 (40.1406)\tTop 5-err 1.5625 (18.4844)\n",
            "Epoch: [144/290][199/782]\tLR: 0.010445\tTime 0.147 (0.150)\tData 0.000 (0.001)\tLoss 3.6328 (1.7296)\tTop 1-err 82.8125 (38.0273)\tTop 5-err 59.3750 (16.3203)\n",
            "Epoch: [144/290][299/782]\tLR: 0.010445\tTime 0.153 (0.150)\tData 0.000 (0.001)\tLoss 0.9048 (1.8075)\tTop 1-err 21.8750 (39.7630)\tTop 5-err 9.3750 (17.8229)\n",
            "Epoch: [144/290][399/782]\tLR: 0.010445\tTime 0.147 (0.150)\tData 0.000 (0.001)\tLoss 2.9019 (1.8130)\tTop 1-err 42.1875 (40.0039)\tTop 5-err 17.1875 (18.0137)\n",
            "Epoch: [144/290][499/782]\tLR: 0.010445\tTime 0.146 (0.150)\tData 0.000 (0.001)\tLoss 0.9319 (1.8075)\tTop 1-err 26.5625 (40.0031)\tTop 5-err 9.3750 (17.8922)\n",
            "Epoch: [144/290][599/782]\tLR: 0.010445\tTime 0.148 (0.149)\tData 0.000 (0.000)\tLoss 2.0648 (1.8159)\tTop 1-err 48.4375 (40.1120)\tTop 5-err 21.8750 (17.7279)\n",
            "Epoch: [144/290][699/782]\tLR: 0.010445\tTime 0.146 (0.149)\tData 0.000 (0.000)\tLoss 1.1827 (1.8169)\tTop 1-err 31.2500 (40.2310)\tTop 5-err 7.8125 (17.7924)\n",
            "* Epoch: [144/290]\t Top 1-err 40.085  Top 5-err 17.740\t Train Loss 1.804\n",
            "* Epoch: [144/290]\t Top 1-err 30.970  Top 5-err 8.380\t Test Loss 1.079\n",
            "Current best accuracy (top-1 and 5 error): 30.4 7.79\n",
            "Epoch: [145/290][99/782]\tLR: 0.010532\tTime 0.149 (0.151)\tData 0.000 (0.002)\tLoss 3.4512 (1.7109)\tTop 1-err 79.6875 (37.8750)\tTop 5-err 59.3750 (15.1953)\n",
            "Epoch: [145/290][199/782]\tLR: 0.010532\tTime 0.150 (0.150)\tData 0.000 (0.001)\tLoss 1.1976 (1.8382)\tTop 1-err 34.3750 (41.1211)\tTop 5-err 7.8125 (18.2852)\n",
            "Epoch: [145/290][299/782]\tLR: 0.010532\tTime 0.143 (0.150)\tData 0.000 (0.001)\tLoss 3.8260 (1.8390)\tTop 1-err 84.3750 (40.9401)\tTop 5-err 65.6250 (18.0573)\n",
            "Epoch: [145/290][399/782]\tLR: 0.010532\tTime 0.156 (0.150)\tData 0.000 (0.001)\tLoss 0.8109 (1.8668)\tTop 1-err 26.5625 (41.5625)\tTop 5-err 7.8125 (18.8672)\n",
            "Epoch: [145/290][499/782]\tLR: 0.010532\tTime 0.151 (0.150)\tData 0.000 (0.001)\tLoss 0.9327 (1.8621)\tTop 1-err 29.6875 (41.2719)\tTop 5-err 7.8125 (18.6547)\n",
            "Epoch: [145/290][599/782]\tLR: 0.010532\tTime 0.147 (0.150)\tData 0.000 (0.000)\tLoss 3.1234 (1.8489)\tTop 1-err 64.0625 (40.8164)\tTop 5-err 39.0625 (18.3841)\n",
            "Epoch: [145/290][699/782]\tLR: 0.010532\tTime 0.147 (0.150)\tData 0.000 (0.000)\tLoss 0.9291 (1.8557)\tTop 1-err 23.4375 (40.9900)\tTop 5-err 4.6875 (18.4866)\n",
            "* Epoch: [145/290]\t Top 1-err 41.368  Top 5-err 18.764\t Train Loss 1.873\n",
            "* Epoch: [145/290]\t Top 1-err 30.740  Top 5-err 8.200\t Test Loss 1.095\n",
            "Current best accuracy (top-1 and 5 error): 30.4 7.79\n",
            "Epoch: [146/290][99/782]\tLR: 0.010620\tTime 0.153 (0.153)\tData 0.000 (0.002)\tLoss 0.9735 (1.9450)\tTop 1-err 32.8125 (42.6953)\tTop 5-err 6.2500 (21.4219)\n",
            "Epoch: [146/290][199/782]\tLR: 0.010620\tTime 0.154 (0.151)\tData 0.000 (0.001)\tLoss 3.0206 (1.9890)\tTop 1-err 64.0625 (43.1406)\tTop 5-err 34.3750 (20.8711)\n",
            "Epoch: [146/290][299/782]\tLR: 0.010620\tTime 0.157 (0.151)\tData 0.000 (0.001)\tLoss 3.8020 (2.0094)\tTop 1-err 82.8125 (43.2500)\tTop 5-err 57.8125 (20.7161)\n",
            "Epoch: [146/290][399/782]\tLR: 0.010620\tTime 0.153 (0.150)\tData 0.000 (0.001)\tLoss 3.3279 (1.9564)\tTop 1-err 48.4375 (42.3809)\tTop 5-err 29.6875 (19.9629)\n",
            "Epoch: [146/290][499/782]\tLR: 0.010620\tTime 0.146 (0.150)\tData 0.000 (0.001)\tLoss 0.8919 (1.9476)\tTop 1-err 32.8125 (42.4750)\tTop 5-err 4.6875 (20.0000)\n",
            "Epoch: [146/290][599/782]\tLR: 0.010620\tTime 0.147 (0.150)\tData 0.000 (0.000)\tLoss 3.7819 (1.9030)\tTop 1-err 75.0000 (41.5482)\tTop 5-err 40.6250 (19.1393)\n",
            "Epoch: [146/290][699/782]\tLR: 0.010620\tTime 0.148 (0.150)\tData 0.000 (0.000)\tLoss 2.5008 (1.8753)\tTop 1-err 50.0000 (41.1741)\tTop 5-err 15.6250 (18.7589)\n",
            "* Epoch: [146/290]\t Top 1-err 41.193  Top 5-err 18.756\t Train Loss 1.876\n",
            "* Epoch: [146/290]\t Top 1-err 31.570  Top 5-err 8.390\t Test Loss 1.100\n",
            "Current best accuracy (top-1 and 5 error): 30.4 7.79\n",
            "Epoch: [147/290][99/782]\tLR: 0.010708\tTime 0.148 (0.153)\tData 0.000 (0.002)\tLoss 3.3178 (1.8791)\tTop 1-err 96.8750 (42.2422)\tTop 5-err 87.5000 (19.4453)\n",
            "Epoch: [147/290][199/782]\tLR: 0.010708\tTime 0.161 (0.152)\tData 0.000 (0.001)\tLoss 0.8016 (1.7939)\tTop 1-err 20.3125 (39.8789)\tTop 5-err 3.1250 (17.6523)\n",
            "Epoch: [147/290][299/782]\tLR: 0.010708\tTime 0.149 (0.151)\tData 0.000 (0.001)\tLoss 0.7384 (1.8313)\tTop 1-err 25.0000 (40.5859)\tTop 5-err 4.6875 (18.0026)\n",
            "Epoch: [147/290][399/782]\tLR: 0.010708\tTime 0.153 (0.150)\tData 0.000 (0.001)\tLoss 2.2407 (1.8389)\tTop 1-err 46.8750 (40.3691)\tTop 5-err 10.9375 (17.6934)\n",
            "Epoch: [147/290][499/782]\tLR: 0.010708\tTime 0.152 (0.150)\tData 0.000 (0.000)\tLoss 0.9735 (1.8208)\tTop 1-err 26.5625 (40.1719)\tTop 5-err 4.6875 (17.5078)\n",
            "Epoch: [147/290][599/782]\tLR: 0.010708\tTime 0.150 (0.150)\tData 0.000 (0.000)\tLoss 0.8635 (1.8087)\tTop 1-err 25.0000 (40.0911)\tTop 5-err 4.6875 (17.4557)\n",
            "Epoch: [147/290][699/782]\tLR: 0.010708\tTime 0.147 (0.150)\tData 0.000 (0.000)\tLoss 3.1659 (1.7964)\tTop 1-err 60.9375 (39.8951)\tTop 5-err 25.0000 (17.2969)\n",
            "* Epoch: [147/290]\t Top 1-err 39.872  Top 5-err 17.235\t Train Loss 1.801\n",
            "* Epoch: [147/290]\t Top 1-err 29.710  Top 5-err 7.600\t Test Loss 1.042\n",
            "Current best accuracy (top-1 and 5 error): 29.71 7.6\n",
            "Epoch: [148/290][99/782]\tLR: 0.010797\tTime 0.146 (0.152)\tData 0.000 (0.002)\tLoss 1.2712 (1.5727)\tTop 1-err 32.8125 (36.0625)\tTop 5-err 10.9375 (14.7734)\n",
            "Epoch: [148/290][199/782]\tLR: 0.010797\tTime 0.148 (0.150)\tData 0.000 (0.001)\tLoss 3.0781 (1.6946)\tTop 1-err 48.4375 (37.6797)\tTop 5-err 23.4375 (15.2969)\n",
            "Epoch: [148/290][299/782]\tLR: 0.010797\tTime 0.149 (0.150)\tData 0.000 (0.001)\tLoss 1.0686 (1.7062)\tTop 1-err 23.4375 (37.9062)\tTop 5-err 7.8125 (15.8151)\n",
            "Epoch: [148/290][399/782]\tLR: 0.010797\tTime 0.148 (0.150)\tData 0.000 (0.001)\tLoss 2.3125 (1.7269)\tTop 1-err 42.1875 (38.0723)\tTop 5-err 17.1875 (15.8340)\n",
            "Epoch: [148/290][499/782]\tLR: 0.010797\tTime 0.147 (0.150)\tData 0.000 (0.000)\tLoss 3.8531 (1.7731)\tTop 1-err 95.3125 (39.1297)\tTop 5-err 76.5625 (16.6484)\n",
            "Epoch: [148/290][599/782]\tLR: 0.010797\tTime 0.148 (0.149)\tData 0.000 (0.000)\tLoss 0.7461 (1.7992)\tTop 1-err 23.4375 (39.7188)\tTop 5-err 4.6875 (17.3607)\n",
            "Epoch: [148/290][699/782]\tLR: 0.010797\tTime 0.147 (0.149)\tData 0.000 (0.000)\tLoss 3.3186 (1.7998)\tTop 1-err 70.3125 (39.7522)\tTop 5-err 40.6250 (17.3371)\n",
            "* Epoch: [148/290]\t Top 1-err 40.049  Top 5-err 17.557\t Train Loss 1.810\n",
            "* Epoch: [148/290]\t Top 1-err 30.740  Top 5-err 7.780\t Test Loss 1.059\n",
            "Current best accuracy (top-1 and 5 error): 29.71 7.6\n",
            "Epoch: [149/290][99/782]\tLR: 0.010886\tTime 0.155 (0.152)\tData 0.000 (0.002)\tLoss 1.0242 (1.8736)\tTop 1-err 29.6875 (40.2344)\tTop 5-err 4.6875 (17.6250)\n",
            "Epoch: [149/290][199/782]\tLR: 0.010886\tTime 0.147 (0.150)\tData 0.000 (0.001)\tLoss 3.4578 (1.7665)\tTop 1-err 68.7500 (39.2578)\tTop 5-err 48.4375 (17.2148)\n",
            "Epoch: [149/290][299/782]\tLR: 0.010886\tTime 0.145 (0.150)\tData 0.000 (0.001)\tLoss 0.9409 (1.7683)\tTop 1-err 28.1250 (39.1745)\tTop 5-err 3.1250 (16.8333)\n",
            "Epoch: [149/290][399/782]\tLR: 0.010886\tTime 0.146 (0.149)\tData 0.000 (0.001)\tLoss 1.3024 (1.7748)\tTop 1-err 40.6250 (39.3730)\tTop 5-err 12.5000 (16.9414)\n",
            "Epoch: [149/290][499/782]\tLR: 0.010886\tTime 0.147 (0.149)\tData 0.000 (0.000)\tLoss 0.9476 (1.7970)\tTop 1-err 26.5625 (39.7828)\tTop 5-err 6.2500 (17.2063)\n",
            "Epoch: [149/290][599/782]\tLR: 0.010886\tTime 0.146 (0.149)\tData 0.000 (0.000)\tLoss 0.8309 (1.8045)\tTop 1-err 25.0000 (40.0091)\tTop 5-err 4.6875 (17.4688)\n",
            "Epoch: [149/290][699/782]\tLR: 0.010886\tTime 0.146 (0.149)\tData 0.000 (0.000)\tLoss 3.0726 (1.7798)\tTop 1-err 54.6875 (39.6507)\tTop 5-err 28.1250 (17.0770)\n",
            "* Epoch: [149/290]\t Top 1-err 39.697  Top 5-err 17.118\t Train Loss 1.783\n",
            "* Epoch: [149/290]\t Top 1-err 31.410  Top 5-err 8.300\t Test Loss 1.087\n",
            "Current best accuracy (top-1 and 5 error): 29.71 7.6\n",
            "Epoch: [150/290][99/782]\tLR: 0.010976\tTime 0.146 (0.152)\tData 0.000 (0.003)\tLoss 3.7253 (1.6770)\tTop 1-err 82.8125 (37.4219)\tTop 5-err 62.5000 (15.8281)\n",
            "Epoch: [150/290][199/782]\tLR: 0.010976\tTime 0.145 (0.150)\tData 0.000 (0.001)\tLoss 0.9785 (1.7712)\tTop 1-err 28.1250 (38.2930)\tTop 5-err 4.6875 (16.1055)\n",
            "Epoch: [150/290][299/782]\tLR: 0.010976\tTime 0.146 (0.150)\tData 0.000 (0.001)\tLoss 0.7862 (1.7711)\tTop 1-err 25.0000 (38.6224)\tTop 5-err 1.5625 (16.5104)\n",
            "Epoch: [150/290][399/782]\tLR: 0.010976\tTime 0.143 (0.149)\tData 0.000 (0.001)\tLoss 2.7459 (1.7555)\tTop 1-err 57.8125 (38.3750)\tTop 5-err 18.7500 (16.4043)\n",
            "Epoch: [150/290][499/782]\tLR: 0.010976\tTime 0.149 (0.149)\tData 0.000 (0.001)\tLoss 0.7627 (1.7537)\tTop 1-err 21.8750 (38.4328)\tTop 5-err 6.2500 (16.2969)\n",
            "Epoch: [150/290][599/782]\tLR: 0.010976\tTime 0.141 (0.148)\tData 0.000 (0.001)\tLoss 3.1639 (1.7346)\tTop 1-err 71.8750 (38.1003)\tTop 5-err 31.2500 (15.8906)\n",
            "Epoch: [150/290][699/782]\tLR: 0.010976\tTime 0.153 (0.148)\tData 0.000 (0.000)\tLoss 1.0548 (1.7400)\tTop 1-err 32.8125 (38.4654)\tTop 5-err 6.2500 (16.1752)\n",
            "* Epoch: [150/290]\t Top 1-err 39.042  Top 5-err 16.806\t Train Loss 1.768\n",
            "* Epoch: [150/290]\t Top 1-err 30.600  Top 5-err 8.260\t Test Loss 1.084\n",
            "Current best accuracy (top-1 and 5 error): 29.71 7.6\n",
            "Epoch: [151/290][99/782]\tLR: 0.011066\tTime 0.146 (0.151)\tData 0.000 (0.002)\tLoss 3.3149 (1.9305)\tTop 1-err 62.5000 (42.5078)\tTop 5-err 37.5000 (19.1250)\n",
            "Epoch: [151/290][199/782]\tLR: 0.011066\tTime 0.157 (0.149)\tData 0.000 (0.001)\tLoss 0.9206 (1.8557)\tTop 1-err 25.0000 (40.8438)\tTop 5-err 4.6875 (17.9688)\n",
            "Epoch: [151/290][299/782]\tLR: 0.011066\tTime 0.147 (0.148)\tData 0.000 (0.001)\tLoss 0.8157 (1.8145)\tTop 1-err 21.8750 (40.2891)\tTop 5-err 4.6875 (17.3021)\n",
            "Epoch: [151/290][399/782]\tLR: 0.011066\tTime 0.145 (0.148)\tData 0.000 (0.001)\tLoss 2.1908 (1.7973)\tTop 1-err 48.4375 (39.8027)\tTop 5-err 10.9375 (16.9785)\n",
            "Epoch: [151/290][499/782]\tLR: 0.011066\tTime 0.153 (0.148)\tData 0.000 (0.000)\tLoss 3.4683 (1.7982)\tTop 1-err 82.8125 (39.8703)\tTop 5-err 57.8125 (17.2547)\n",
            "Epoch: [151/290][599/782]\tLR: 0.011066\tTime 0.145 (0.148)\tData 0.000 (0.000)\tLoss 2.4927 (1.7883)\tTop 1-err 57.8125 (39.7201)\tTop 5-err 31.2500 (17.0703)\n",
            "Epoch: [151/290][699/782]\tLR: 0.011066\tTime 0.149 (0.148)\tData 0.000 (0.000)\tLoss 0.8611 (1.8248)\tTop 1-err 25.0000 (40.3002)\tTop 5-err 6.2500 (17.5525)\n",
            "* Epoch: [151/290]\t Top 1-err 40.231  Top 5-err 17.389\t Train Loss 1.823\n",
            "* Epoch: [151/290]\t Top 1-err 31.090  Top 5-err 8.010\t Test Loss 1.073\n",
            "Current best accuracy (top-1 and 5 error): 29.71 7.6\n",
            "Epoch: [152/290][99/782]\tLR: 0.011157\tTime 0.145 (0.149)\tData 0.000 (0.002)\tLoss 3.1274 (1.8117)\tTop 1-err 51.5625 (40.4062)\tTop 5-err 28.1250 (18.0859)\n",
            "Epoch: [152/290][199/782]\tLR: 0.011157\tTime 0.145 (0.148)\tData 0.000 (0.001)\tLoss 3.6146 (1.8227)\tTop 1-err 51.5625 (39.8242)\tTop 5-err 28.1250 (17.1133)\n",
            "Epoch: [152/290][299/782]\tLR: 0.011157\tTime 0.149 (0.148)\tData 0.000 (0.001)\tLoss 2.8734 (1.7941)\tTop 1-err 93.7500 (39.7135)\tTop 5-err 73.4375 (17.2943)\n",
            "Epoch: [152/290][399/782]\tLR: 0.011157\tTime 0.145 (0.148)\tData 0.000 (0.001)\tLoss 3.4616 (1.7960)\tTop 1-err 70.3125 (39.6602)\tTop 5-err 45.3125 (17.2832)\n",
            "Epoch: [152/290][499/782]\tLR: 0.011157\tTime 0.152 (0.148)\tData 0.000 (0.001)\tLoss 0.8609 (1.7863)\tTop 1-err 23.4375 (39.4047)\tTop 5-err 4.6875 (17.0453)\n",
            "Epoch: [152/290][599/782]\tLR: 0.011157\tTime 0.147 (0.148)\tData 0.000 (0.000)\tLoss 1.1925 (1.7749)\tTop 1-err 34.3750 (39.1367)\tTop 5-err 10.9375 (16.7721)\n",
            "Epoch: [152/290][699/782]\tLR: 0.011157\tTime 0.146 (0.147)\tData 0.000 (0.000)\tLoss 4.0044 (1.7793)\tTop 1-err 85.9375 (39.4877)\tTop 5-err 64.0625 (17.0848)\n",
            "* Epoch: [152/290]\t Top 1-err 39.170  Top 5-err 16.713\t Train Loss 1.762\n",
            "* Epoch: [152/290]\t Top 1-err 31.330  Top 5-err 8.220\t Test Loss 1.095\n",
            "Current best accuracy (top-1 and 5 error): 29.71 7.6\n",
            "Epoch: [153/290][99/782]\tLR: 0.011249\tTime 0.145 (0.152)\tData 0.000 (0.002)\tLoss 3.6166 (1.7807)\tTop 1-err 89.0625 (40.6094)\tTop 5-err 67.1875 (17.7578)\n",
            "Epoch: [153/290][199/782]\tLR: 0.011249\tTime 0.148 (0.150)\tData 0.000 (0.001)\tLoss 4.0320 (1.7395)\tTop 1-err 84.3750 (40.2383)\tTop 5-err 64.0625 (17.9258)\n",
            "Epoch: [153/290][299/782]\tLR: 0.011249\tTime 0.143 (0.149)\tData 0.000 (0.001)\tLoss 3.3294 (1.7903)\tTop 1-err 67.1875 (40.6146)\tTop 5-err 42.1875 (17.9531)\n",
            "Epoch: [153/290][399/782]\tLR: 0.011249\tTime 0.147 (0.149)\tData 0.000 (0.000)\tLoss 0.9893 (1.7577)\tTop 1-err 32.8125 (40.2832)\tTop 5-err 1.5625 (17.6133)\n",
            "Epoch: [153/290][499/782]\tLR: 0.011249\tTime 0.144 (0.149)\tData 0.000 (0.000)\tLoss 0.8630 (1.8122)\tTop 1-err 25.0000 (40.9703)\tTop 5-err 3.1250 (18.2156)\n",
            "Epoch: [153/290][599/782]\tLR: 0.011249\tTime 0.144 (0.148)\tData 0.000 (0.000)\tLoss 1.1219 (1.8191)\tTop 1-err 29.6875 (41.0534)\tTop 5-err 9.3750 (18.4622)\n",
            "Epoch: [153/290][699/782]\tLR: 0.011249\tTime 0.146 (0.148)\tData 0.000 (0.000)\tLoss 3.6600 (1.7854)\tTop 1-err 85.9375 (40.3694)\tTop 5-err 51.5625 (17.7868)\n",
            "* Epoch: [153/290]\t Top 1-err 39.717  Top 5-err 17.331\t Train Loss 1.750\n",
            "* Epoch: [153/290]\t Top 1-err 29.970  Top 5-err 8.190\t Test Loss 1.064\n",
            "Current best accuracy (top-1 and 5 error): 29.71 7.6\n",
            "Epoch: [154/290][99/782]\tLR: 0.011341\tTime 0.146 (0.152)\tData 0.000 (0.002)\tLoss 3.2307 (1.8606)\tTop 1-err 73.4375 (39.6953)\tTop 5-err 46.8750 (18.3203)\n",
            "Epoch: [154/290][199/782]\tLR: 0.011341\tTime 0.146 (0.150)\tData 0.000 (0.001)\tLoss 3.0464 (1.7367)\tTop 1-err 68.7500 (38.0859)\tTop 5-err 39.0625 (16.8086)\n",
            "Epoch: [154/290][299/782]\tLR: 0.011341\tTime 0.145 (0.149)\tData 0.000 (0.001)\tLoss 0.9527 (1.7328)\tTop 1-err 26.5625 (38.2057)\tTop 5-err 6.2500 (16.2812)\n",
            "Epoch: [154/290][399/782]\tLR: 0.011341\tTime 0.145 (0.149)\tData 0.000 (0.001)\tLoss 0.9221 (1.7246)\tTop 1-err 26.5625 (38.1270)\tTop 5-err 4.6875 (16.0156)\n",
            "Epoch: [154/290][499/782]\tLR: 0.011341\tTime 0.152 (0.149)\tData 0.000 (0.000)\tLoss 2.4331 (1.7540)\tTop 1-err 37.5000 (38.7750)\tTop 5-err 10.9375 (16.3656)\n",
            "Epoch: [154/290][599/782]\tLR: 0.011341\tTime 0.147 (0.149)\tData 0.000 (0.000)\tLoss 0.9959 (1.7962)\tTop 1-err 29.6875 (39.8203)\tTop 5-err 7.8125 (17.3151)\n",
            "Epoch: [154/290][699/782]\tLR: 0.011341\tTime 0.146 (0.149)\tData 0.000 (0.000)\tLoss 0.6963 (1.8155)\tTop 1-err 20.3125 (40.1317)\tTop 5-err 1.5625 (17.5033)\n",
            "* Epoch: [154/290]\t Top 1-err 39.827  Top 5-err 17.286\t Train Loss 1.801\n",
            "* Epoch: [154/290]\t Top 1-err 30.910  Top 5-err 8.050\t Test Loss 1.076\n",
            "Current best accuracy (top-1 and 5 error): 29.71 7.6\n",
            "Epoch: [155/290][99/782]\tLR: 0.011433\tTime 0.146 (0.151)\tData 0.000 (0.002)\tLoss 3.4698 (2.0023)\tTop 1-err 96.8750 (43.4219)\tTop 5-err 84.3750 (20.1875)\n",
            "Epoch: [155/290][199/782]\tLR: 0.011433\tTime 0.153 (0.150)\tData 0.000 (0.001)\tLoss 3.0102 (1.8229)\tTop 1-err 51.5625 (40.0664)\tTop 5-err 26.5625 (17.8164)\n",
            "Epoch: [155/290][299/782]\tLR: 0.011433\tTime 0.147 (0.149)\tData 0.000 (0.001)\tLoss 3.3833 (1.8490)\tTop 1-err 76.5625 (40.4010)\tTop 5-err 48.4375 (18.1693)\n",
            "Epoch: [155/290][399/782]\tLR: 0.011433\tTime 0.149 (0.149)\tData 0.000 (0.001)\tLoss 3.5567 (1.8551)\tTop 1-err 92.1875 (40.6250)\tTop 5-err 89.0625 (18.5840)\n",
            "Epoch: [155/290][499/782]\tLR: 0.011433\tTime 0.155 (0.149)\tData 0.000 (0.000)\tLoss 0.8287 (1.8604)\tTop 1-err 21.8750 (40.7203)\tTop 5-err 4.6875 (18.5203)\n",
            "Epoch: [155/290][599/782]\tLR: 0.011433\tTime 0.146 (0.149)\tData 0.000 (0.000)\tLoss 1.1543 (1.8510)\tTop 1-err 32.8125 (40.4922)\tTop 5-err 14.0625 (18.2096)\n",
            "Epoch: [155/290][699/782]\tLR: 0.011433\tTime 0.148 (0.149)\tData 0.000 (0.000)\tLoss 2.6723 (1.8492)\tTop 1-err 46.8750 (40.5949)\tTop 5-err 23.4375 (18.2087)\n",
            "* Epoch: [155/290]\t Top 1-err 40.161  Top 5-err 17.974\t Train Loss 1.818\n",
            "* Epoch: [155/290]\t Top 1-err 30.790  Top 5-err 8.520\t Test Loss 1.102\n",
            "Current best accuracy (top-1 and 5 error): 29.71 7.6\n",
            "Epoch: [156/290][99/782]\tLR: 0.011526\tTime 0.150 (0.150)\tData 0.000 (0.002)\tLoss 2.6221 (2.0382)\tTop 1-err 39.0625 (42.9062)\tTop 5-err 12.5000 (20.7266)\n",
            "Epoch: [156/290][199/782]\tLR: 0.011526\tTime 0.146 (0.149)\tData 0.000 (0.001)\tLoss 3.6440 (1.9056)\tTop 1-err 92.1875 (41.1523)\tTop 5-err 64.0625 (19.2070)\n",
            "Epoch: [156/290][299/782]\tLR: 0.011526\tTime 0.145 (0.149)\tData 0.000 (0.001)\tLoss 0.8009 (1.8651)\tTop 1-err 23.4375 (40.3984)\tTop 5-err 6.2500 (18.3542)\n",
            "Epoch: [156/290][399/782]\tLR: 0.011526\tTime 0.145 (0.149)\tData 0.000 (0.001)\tLoss 0.7117 (1.8710)\tTop 1-err 20.3125 (40.1836)\tTop 5-err 6.2500 (17.9746)\n",
            "Epoch: [156/290][499/782]\tLR: 0.011526\tTime 0.146 (0.149)\tData 0.000 (0.001)\tLoss 3.7802 (1.8684)\tTop 1-err 78.1250 (40.3797)\tTop 5-err 50.0000 (18.0906)\n",
            "Epoch: [156/290][599/782]\tLR: 0.011526\tTime 0.147 (0.148)\tData 0.000 (0.001)\tLoss 3.3055 (1.8427)\tTop 1-err 67.1875 (40.1745)\tTop 5-err 31.2500 (17.8229)\n",
            "Epoch: [156/290][699/782]\tLR: 0.011526\tTime 0.146 (0.148)\tData 0.000 (0.000)\tLoss 0.8538 (1.8153)\tTop 1-err 29.6875 (39.9319)\tTop 5-err 4.6875 (17.6194)\n",
            "* Epoch: [156/290]\t Top 1-err 39.907  Top 5-err 17.565\t Train Loss 1.813\n",
            "* Epoch: [156/290]\t Top 1-err 30.840  Top 5-err 8.580\t Test Loss 1.104\n",
            "Current best accuracy (top-1 and 5 error): 29.71 7.6\n",
            "Epoch: [157/290][99/782]\tLR: 0.011620\tTime 0.147 (0.151)\tData 0.000 (0.002)\tLoss 3.1739 (1.9027)\tTop 1-err 62.5000 (41.3281)\tTop 5-err 26.5625 (19.1016)\n",
            "Epoch: [157/290][199/782]\tLR: 0.011620\tTime 0.158 (0.150)\tData 0.000 (0.001)\tLoss 3.0539 (1.8022)\tTop 1-err 50.0000 (38.6367)\tTop 5-err 20.3125 (16.4062)\n",
            "Epoch: [157/290][299/782]\tLR: 0.011620\tTime 0.148 (0.150)\tData 0.000 (0.001)\tLoss 2.6826 (1.8511)\tTop 1-err 45.3125 (39.9714)\tTop 5-err 20.3125 (17.4609)\n",
            "Epoch: [157/290][399/782]\tLR: 0.011620\tTime 0.154 (0.149)\tData 0.000 (0.001)\tLoss 0.5888 (1.8535)\tTop 1-err 21.8750 (40.2305)\tTop 5-err 1.5625 (17.6719)\n",
            "Epoch: [157/290][499/782]\tLR: 0.011620\tTime 0.153 (0.149)\tData 0.000 (0.001)\tLoss 3.1847 (1.8612)\tTop 1-err 54.6875 (40.5047)\tTop 5-err 32.8125 (17.9375)\n",
            "Epoch: [157/290][599/782]\tLR: 0.011620\tTime 0.144 (0.149)\tData 0.000 (0.000)\tLoss 0.9401 (1.8740)\tTop 1-err 26.5625 (40.7682)\tTop 5-err 7.8125 (18.1289)\n",
            "Epoch: [157/290][699/782]\tLR: 0.011620\tTime 0.153 (0.149)\tData 0.000 (0.000)\tLoss 0.8237 (1.8932)\tTop 1-err 25.0000 (41.2377)\tTop 5-err 3.1250 (18.5547)\n",
            "* Epoch: [157/290]\t Top 1-err 41.390  Top 5-err 18.686\t Train Loss 1.899\n",
            "* Epoch: [157/290]\t Top 1-err 33.080  Top 5-err 9.240\t Test Loss 1.173\n",
            "Current best accuracy (top-1 and 5 error): 29.71 7.6\n",
            "Epoch: [158/290][99/782]\tLR: 0.011714\tTime 0.153 (0.153)\tData 0.000 (0.002)\tLoss 3.9470 (1.8516)\tTop 1-err 90.6250 (41.5078)\tTop 5-err 67.1875 (18.6484)\n",
            "Epoch: [158/290][199/782]\tLR: 0.011714\tTime 0.156 (0.151)\tData 0.000 (0.001)\tLoss 0.9868 (1.7120)\tTop 1-err 32.8125 (38.3008)\tTop 5-err 6.2500 (15.8086)\n",
            "Epoch: [158/290][299/782]\tLR: 0.011714\tTime 0.148 (0.151)\tData 0.000 (0.001)\tLoss 3.0556 (1.8243)\tTop 1-err 67.1875 (40.2031)\tTop 5-err 39.0625 (17.7682)\n",
            "Epoch: [158/290][399/782]\tLR: 0.011714\tTime 0.148 (0.150)\tData 0.000 (0.001)\tLoss 0.7419 (1.8126)\tTop 1-err 18.7500 (39.8164)\tTop 5-err 3.1250 (17.6875)\n",
            "Epoch: [158/290][499/782]\tLR: 0.011714\tTime 0.146 (0.150)\tData 0.000 (0.001)\tLoss 4.2478 (1.7958)\tTop 1-err 100.0000 (39.4937)\tTop 5-err 89.0625 (17.4531)\n",
            "Epoch: [158/290][599/782]\tLR: 0.011714\tTime 0.159 (0.150)\tData 0.000 (0.000)\tLoss 3.1201 (1.7918)\tTop 1-err 64.0625 (39.4440)\tTop 5-err 25.0000 (17.1849)\n",
            "Epoch: [158/290][699/782]\tLR: 0.011714\tTime 0.149 (0.150)\tData 0.000 (0.000)\tLoss 1.0636 (1.7983)\tTop 1-err 28.1250 (39.6663)\tTop 5-err 9.3750 (17.3538)\n",
            "* Epoch: [158/290]\t Top 1-err 39.649  Top 5-err 17.291\t Train Loss 1.793\n",
            "* Epoch: [158/290]\t Top 1-err 31.480  Top 5-err 8.640\t Test Loss 1.106\n",
            "Current best accuracy (top-1 and 5 error): 29.71 7.6\n",
            "Epoch: [159/290][99/782]\tLR: 0.011809\tTime 0.150 (0.152)\tData 0.000 (0.002)\tLoss 3.3545 (1.9374)\tTop 1-err 65.6250 (43.8750)\tTop 5-err 45.3125 (20.8281)\n",
            "Epoch: [159/290][199/782]\tLR: 0.011809\tTime 0.150 (0.151)\tData 0.000 (0.001)\tLoss 3.5323 (1.8354)\tTop 1-err 71.8750 (41.2891)\tTop 5-err 46.8750 (18.8633)\n",
            "Epoch: [159/290][299/782]\tLR: 0.011809\tTime 0.151 (0.150)\tData 0.000 (0.001)\tLoss 4.1461 (1.8847)\tTop 1-err 90.6250 (42.0964)\tTop 5-err 73.4375 (19.3125)\n",
            "Epoch: [159/290][399/782]\tLR: 0.011809\tTime 0.148 (0.150)\tData 0.000 (0.001)\tLoss 3.6515 (1.8331)\tTop 1-err 84.3750 (41.1777)\tTop 5-err 53.1250 (18.4551)\n",
            "Epoch: [159/290][499/782]\tLR: 0.011809\tTime 0.147 (0.150)\tData 0.000 (0.001)\tLoss 3.1347 (1.8087)\tTop 1-err 65.6250 (41.0125)\tTop 5-err 35.9375 (18.3781)\n",
            "Epoch: [159/290][599/782]\tLR: 0.011809\tTime 0.147 (0.150)\tData 0.000 (0.001)\tLoss 0.7581 (1.7954)\tTop 1-err 21.8750 (40.6172)\tTop 5-err 1.5625 (18.0833)\n",
            "Epoch: [159/290][699/782]\tLR: 0.011809\tTime 0.151 (0.150)\tData 0.000 (0.000)\tLoss 3.4627 (1.8045)\tTop 1-err 65.6250 (40.7522)\tTop 5-err 39.0625 (18.1908)\n",
            "* Epoch: [159/290]\t Top 1-err 40.653  Top 5-err 18.087\t Train Loss 1.798\n",
            "* Epoch: [159/290]\t Top 1-err 31.070  Top 5-err 8.070\t Test Loss 1.081\n",
            "Current best accuracy (top-1 and 5 error): 29.71 7.6\n",
            "Epoch: [160/290][99/782]\tLR: 0.011904\tTime 0.160 (0.151)\tData 0.000 (0.002)\tLoss 3.3509 (1.5684)\tTop 1-err 82.8125 (34.6328)\tTop 5-err 56.2500 (13.2109)\n",
            "Epoch: [160/290][199/782]\tLR: 0.011904\tTime 0.147 (0.150)\tData 0.000 (0.001)\tLoss 0.6931 (1.7186)\tTop 1-err 18.7500 (38.1406)\tTop 5-err 3.1250 (16.2070)\n",
            "Epoch: [160/290][299/782]\tLR: 0.011904\tTime 0.146 (0.150)\tData 0.000 (0.001)\tLoss 1.3067 (1.7560)\tTop 1-err 31.2500 (38.6953)\tTop 5-err 7.8125 (16.6667)\n",
            "Epoch: [160/290][399/782]\tLR: 0.011904\tTime 0.155 (0.150)\tData 0.000 (0.001)\tLoss 3.6813 (1.7990)\tTop 1-err 68.7500 (39.4766)\tTop 5-err 46.8750 (17.4102)\n",
            "Epoch: [160/290][499/782]\tLR: 0.011904\tTime 0.151 (0.150)\tData 0.000 (0.000)\tLoss 3.0561 (1.8247)\tTop 1-err 54.6875 (40.0359)\tTop 5-err 21.8750 (17.7937)\n",
            "Epoch: [160/290][599/782]\tLR: 0.011904\tTime 0.156 (0.149)\tData 0.000 (0.000)\tLoss 4.3226 (1.8427)\tTop 1-err 90.6250 (40.5599)\tTop 5-err 81.2500 (18.4115)\n",
            "Epoch: [160/290][699/782]\tLR: 0.011904\tTime 0.148 (0.149)\tData 0.000 (0.000)\tLoss 4.1761 (1.8435)\tTop 1-err 85.9375 (40.6060)\tTop 5-err 56.2500 (18.2991)\n",
            "* Epoch: [160/290]\t Top 1-err 40.416  Top 5-err 17.972\t Train Loss 1.834\n",
            "* Epoch: [160/290]\t Top 1-err 30.740  Top 5-err 8.280\t Test Loss 1.091\n",
            "Current best accuracy (top-1 and 5 error): 29.71 7.6\n",
            "Epoch: [161/290][99/782]\tLR: 0.011999\tTime 0.145 (0.153)\tData 0.000 (0.002)\tLoss 0.8716 (2.0374)\tTop 1-err 28.1250 (44.0312)\tTop 5-err 6.2500 (21.7109)\n",
            "Epoch: [161/290][199/782]\tLR: 0.011999\tTime 0.149 (0.151)\tData 0.000 (0.001)\tLoss 3.3307 (1.9117)\tTop 1-err 78.1250 (42.2383)\tTop 5-err 48.4375 (19.2891)\n",
            "Epoch: [161/290][299/782]\tLR: 0.011999\tTime 0.145 (0.150)\tData 0.000 (0.001)\tLoss 0.8416 (1.8394)\tTop 1-err 26.5625 (40.5964)\tTop 5-err 4.6875 (18.1458)\n",
            "Epoch: [161/290][399/782]\tLR: 0.011999\tTime 0.147 (0.150)\tData 0.000 (0.001)\tLoss 1.5234 (1.8198)\tTop 1-err 37.5000 (40.3262)\tTop 5-err 14.0625 (18.0020)\n",
            "Epoch: [161/290][499/782]\tLR: 0.011999\tTime 0.148 (0.150)\tData 0.000 (0.000)\tLoss 0.7964 (1.8210)\tTop 1-err 25.0000 (40.3016)\tTop 5-err 3.1250 (17.9641)\n",
            "Epoch: [161/290][599/782]\tLR: 0.011999\tTime 0.146 (0.150)\tData 0.000 (0.000)\tLoss 0.7013 (1.7894)\tTop 1-err 15.6250 (39.7513)\tTop 5-err 3.1250 (17.5078)\n",
            "Epoch: [161/290][699/782]\tLR: 0.011999\tTime 0.151 (0.150)\tData 0.000 (0.000)\tLoss 0.8337 (1.7961)\tTop 1-err 23.4375 (39.8594)\tTop 5-err 6.2500 (17.5714)\n",
            "* Epoch: [161/290]\t Top 1-err 39.708  Top 5-err 17.447\t Train Loss 1.794\n",
            "* Epoch: [161/290]\t Top 1-err 31.810  Top 5-err 8.810\t Test Loss 1.127\n",
            "Current best accuracy (top-1 and 5 error): 29.71 7.6\n",
            "Epoch: [162/290][99/782]\tLR: 0.012095\tTime 0.150 (0.152)\tData 0.000 (0.002)\tLoss 3.4124 (1.9106)\tTop 1-err 64.0625 (41.3984)\tTop 5-err 39.0625 (19.4844)\n",
            "Epoch: [162/290][199/782]\tLR: 0.012095\tTime 0.148 (0.151)\tData 0.000 (0.001)\tLoss 0.7752 (1.8589)\tTop 1-err 28.1250 (40.8320)\tTop 5-err 1.5625 (18.8555)\n",
            "Epoch: [162/290][299/782]\tLR: 0.012095\tTime 0.152 (0.150)\tData 0.000 (0.001)\tLoss 0.7956 (1.8557)\tTop 1-err 21.8750 (40.6615)\tTop 5-err 3.1250 (18.2760)\n",
            "Epoch: [162/290][399/782]\tLR: 0.012095\tTime 0.148 (0.151)\tData 0.000 (0.001)\tLoss 3.9888 (1.8612)\tTop 1-err 71.8750 (40.7656)\tTop 5-err 48.4375 (18.2832)\n",
            "Epoch: [162/290][499/782]\tLR: 0.012095\tTime 0.150 (0.151)\tData 0.000 (0.001)\tLoss 3.4194 (1.8759)\tTop 1-err 75.0000 (41.1391)\tTop 5-err 54.6875 (18.6766)\n",
            "Epoch: [162/290][599/782]\tLR: 0.012095\tTime 0.148 (0.150)\tData 0.000 (0.000)\tLoss 3.7232 (1.8598)\tTop 1-err 85.9375 (40.9505)\tTop 5-err 51.5625 (18.4388)\n",
            "Epoch: [162/290][699/782]\tLR: 0.012095\tTime 0.156 (0.150)\tData 0.000 (0.000)\tLoss 3.4963 (1.8560)\tTop 1-err 73.4375 (40.9598)\tTop 5-err 51.5625 (18.3538)\n",
            "* Epoch: [162/290]\t Top 1-err 41.196  Top 5-err 18.574\t Train Loss 1.874\n",
            "* Epoch: [162/290]\t Top 1-err 30.110  Top 5-err 7.780\t Test Loss 1.051\n",
            "Current best accuracy (top-1 and 5 error): 29.71 7.6\n",
            "Epoch: [163/290][99/782]\tLR: 0.012192\tTime 0.146 (0.153)\tData 0.000 (0.002)\tLoss 2.5362 (1.8050)\tTop 1-err 53.1250 (40.5781)\tTop 5-err 14.0625 (18.4375)\n",
            "Epoch: [163/290][199/782]\tLR: 0.012192\tTime 0.146 (0.152)\tData 0.000 (0.001)\tLoss 1.2665 (1.7802)\tTop 1-err 32.8125 (39.6250)\tTop 5-err 12.5000 (17.3359)\n",
            "Epoch: [163/290][299/782]\tLR: 0.012192\tTime 0.148 (0.151)\tData 0.000 (0.001)\tLoss 0.6805 (1.7730)\tTop 1-err 17.1875 (39.3750)\tTop 5-err 4.6875 (17.2448)\n",
            "Epoch: [163/290][399/782]\tLR: 0.012192\tTime 0.147 (0.151)\tData 0.000 (0.001)\tLoss 0.7720 (1.7811)\tTop 1-err 23.4375 (39.4707)\tTop 5-err 3.1250 (17.2188)\n",
            "Epoch: [163/290][499/782]\tLR: 0.012192\tTime 0.150 (0.151)\tData 0.000 (0.000)\tLoss 1.0172 (1.7899)\tTop 1-err 34.3750 (40.1141)\tTop 5-err 4.6875 (17.6562)\n",
            "Epoch: [163/290][599/782]\tLR: 0.012192\tTime 0.146 (0.151)\tData 0.000 (0.000)\tLoss 0.8305 (1.8231)\tTop 1-err 18.7500 (40.7057)\tTop 5-err 7.8125 (18.3841)\n",
            "Epoch: [163/290][699/782]\tLR: 0.012192\tTime 0.148 (0.150)\tData 0.000 (0.000)\tLoss 2.9929 (1.8246)\tTop 1-err 64.0625 (40.6350)\tTop 5-err 39.0625 (18.2333)\n",
            "* Epoch: [163/290]\t Top 1-err 40.633  Top 5-err 18.191\t Train Loss 1.833\n",
            "* Epoch: [163/290]\t Top 1-err 31.480  Top 5-err 8.420\t Test Loss 1.101\n",
            "Current best accuracy (top-1 and 5 error): 29.71 7.6\n",
            "Epoch: [164/290][99/782]\tLR: 0.012289\tTime 0.150 (0.152)\tData 0.000 (0.002)\tLoss 3.4576 (1.6953)\tTop 1-err 68.7500 (37.1484)\tTop 5-err 40.6250 (14.8750)\n",
            "Epoch: [164/290][199/782]\tLR: 0.012289\tTime 0.145 (0.151)\tData 0.000 (0.001)\tLoss 0.7302 (1.6857)\tTop 1-err 18.7500 (37.4297)\tTop 5-err 3.1250 (15.2305)\n",
            "Epoch: [164/290][299/782]\tLR: 0.012289\tTime 0.146 (0.150)\tData 0.000 (0.001)\tLoss 3.1716 (1.6919)\tTop 1-err 75.0000 (37.9505)\tTop 5-err 42.1875 (15.4740)\n",
            "Epoch: [164/290][399/782]\tLR: 0.012289\tTime 0.147 (0.150)\tData 0.000 (0.001)\tLoss 1.0889 (1.6960)\tTop 1-err 37.5000 (38.0938)\tTop 5-err 6.2500 (15.3203)\n",
            "Epoch: [164/290][499/782]\tLR: 0.012289\tTime 0.145 (0.150)\tData 0.000 (0.000)\tLoss 1.0528 (1.7425)\tTop 1-err 26.5625 (39.0063)\tTop 5-err 10.9375 (16.1172)\n",
            "Epoch: [164/290][599/782]\tLR: 0.012289\tTime 0.147 (0.150)\tData 0.000 (0.000)\tLoss 3.0529 (1.7812)\tTop 1-err 57.8125 (39.6068)\tTop 5-err 34.3750 (16.7760)\n",
            "Epoch: [164/290][699/782]\tLR: 0.012289\tTime 0.146 (0.150)\tData 0.000 (0.000)\tLoss 1.0554 (1.7699)\tTop 1-err 32.8125 (39.4944)\tTop 5-err 3.1250 (16.6830)\n",
            "* Epoch: [164/290]\t Top 1-err 39.463  Top 5-err 16.735\t Train Loss 1.770\n",
            "* Epoch: [164/290]\t Top 1-err 31.300  Top 5-err 8.620\t Test Loss 1.104\n",
            "Current best accuracy (top-1 and 5 error): 29.71 7.6\n",
            "Epoch: [165/290][99/782]\tLR: 0.012387\tTime 0.146 (0.151)\tData 0.000 (0.002)\tLoss 0.8079 (1.6489)\tTop 1-err 20.3125 (36.8047)\tTop 5-err 6.2500 (16.0703)\n",
            "Epoch: [165/290][199/782]\tLR: 0.012387\tTime 0.148 (0.150)\tData 0.000 (0.001)\tLoss 3.6326 (1.7103)\tTop 1-err 71.8750 (38.0000)\tTop 5-err 43.7500 (16.4062)\n",
            "Epoch: [165/290][299/782]\tLR: 0.012387\tTime 0.159 (0.149)\tData 0.000 (0.001)\tLoss 2.4549 (1.7249)\tTop 1-err 57.8125 (38.1224)\tTop 5-err 28.1250 (16.0052)\n",
            "Epoch: [165/290][399/782]\tLR: 0.012387\tTime 0.150 (0.149)\tData 0.000 (0.001)\tLoss 0.8259 (1.7456)\tTop 1-err 25.0000 (38.7598)\tTop 5-err 3.1250 (16.2695)\n",
            "Epoch: [165/290][499/782]\tLR: 0.012387\tTime 0.149 (0.149)\tData 0.000 (0.001)\tLoss 0.8384 (1.7285)\tTop 1-err 23.4375 (38.3859)\tTop 5-err 4.6875 (15.9094)\n",
            "Epoch: [165/290][599/782]\tLR: 0.012387\tTime 0.145 (0.149)\tData 0.000 (0.000)\tLoss 0.7353 (1.7191)\tTop 1-err 20.3125 (38.2305)\tTop 5-err 3.1250 (15.8503)\n",
            "Epoch: [165/290][699/782]\tLR: 0.012387\tTime 0.155 (0.149)\tData 0.000 (0.000)\tLoss 3.1019 (1.7489)\tTop 1-err 64.0625 (38.8359)\tTop 5-err 42.1875 (16.3951)\n",
            "* Epoch: [165/290]\t Top 1-err 38.880  Top 5-err 16.493\t Train Loss 1.746\n",
            "* Epoch: [165/290]\t Top 1-err 29.840  Top 5-err 7.520\t Test Loss 1.042\n",
            "Current best accuracy (top-1 and 5 error): 29.71 7.6\n",
            "Epoch: [166/290][99/782]\tLR: 0.012485\tTime 0.152 (0.150)\tData 0.000 (0.002)\tLoss 2.3186 (1.7814)\tTop 1-err 56.2500 (39.4375)\tTop 5-err 18.7500 (17.8906)\n",
            "Epoch: [166/290][199/782]\tLR: 0.012485\tTime 0.150 (0.150)\tData 0.000 (0.001)\tLoss 0.8904 (1.8517)\tTop 1-err 26.5625 (40.6953)\tTop 5-err 6.2500 (18.7109)\n",
            "Epoch: [166/290][299/782]\tLR: 0.012485\tTime 0.148 (0.149)\tData 0.000 (0.001)\tLoss 0.7255 (1.8236)\tTop 1-err 18.7500 (40.3594)\tTop 5-err 4.6875 (18.5339)\n",
            "Epoch: [166/290][399/782]\tLR: 0.012485\tTime 0.146 (0.149)\tData 0.000 (0.001)\tLoss 0.9917 (1.8457)\tTop 1-err 20.3125 (40.6367)\tTop 5-err 9.3750 (18.5938)\n",
            "Epoch: [166/290][499/782]\tLR: 0.012485\tTime 0.147 (0.149)\tData 0.000 (0.001)\tLoss 3.0783 (1.8272)\tTop 1-err 54.6875 (40.7594)\tTop 5-err 28.1250 (18.6359)\n",
            "Epoch: [166/290][599/782]\tLR: 0.012485\tTime 0.144 (0.149)\tData 0.000 (0.000)\tLoss 0.8187 (1.7985)\tTop 1-err 17.1875 (40.3750)\tTop 5-err 4.6875 (18.2812)\n",
            "Epoch: [166/290][699/782]\tLR: 0.012485\tTime 0.153 (0.149)\tData 0.000 (0.000)\tLoss 2.6980 (1.7981)\tTop 1-err 46.8750 (40.3326)\tTop 5-err 20.3125 (18.3069)\n",
            "* Epoch: [166/290]\t Top 1-err 40.671  Top 5-err 18.562\t Train Loss 1.816\n",
            "* Epoch: [166/290]\t Top 1-err 30.590  Top 5-err 8.230\t Test Loss 1.068\n",
            "Current best accuracy (top-1 and 5 error): 29.71 7.6\n",
            "Epoch: [167/290][99/782]\tLR: 0.012584\tTime 0.146 (0.150)\tData 0.000 (0.002)\tLoss 0.8714 (1.7788)\tTop 1-err 26.5625 (39.8047)\tTop 5-err 4.6875 (17.3984)\n",
            "Epoch: [167/290][199/782]\tLR: 0.012584\tTime 0.146 (0.150)\tData 0.000 (0.001)\tLoss 0.6676 (1.8006)\tTop 1-err 20.3125 (40.1602)\tTop 5-err 1.5625 (17.2305)\n",
            "Epoch: [167/290][299/782]\tLR: 0.012584\tTime 0.161 (0.149)\tData 0.000 (0.001)\tLoss 3.1090 (1.8072)\tTop 1-err 93.7500 (39.9401)\tTop 5-err 76.5625 (17.0729)\n",
            "Epoch: [167/290][399/782]\tLR: 0.012584\tTime 0.147 (0.149)\tData 0.000 (0.001)\tLoss 3.6549 (1.7918)\tTop 1-err 71.8750 (39.6719)\tTop 5-err 40.6250 (17.2070)\n",
            "Epoch: [167/290][499/782]\tLR: 0.012584\tTime 0.148 (0.149)\tData 0.000 (0.001)\tLoss 0.7499 (1.7863)\tTop 1-err 23.4375 (39.4641)\tTop 5-err 6.2500 (17.0672)\n",
            "Epoch: [167/290][599/782]\tLR: 0.012584\tTime 0.154 (0.149)\tData 0.000 (0.000)\tLoss 2.1663 (1.7569)\tTop 1-err 34.3750 (38.9570)\tTop 5-err 7.8125 (16.6836)\n",
            "Epoch: [167/290][699/782]\tLR: 0.012584\tTime 0.147 (0.149)\tData 0.000 (0.000)\tLoss 0.8871 (1.7564)\tTop 1-err 23.4375 (39.0748)\tTop 5-err 3.1250 (16.9219)\n",
            "* Epoch: [167/290]\t Top 1-err 39.095  Top 5-err 16.803\t Train Loss 1.750\n",
            "* Epoch: [167/290]\t Top 1-err 31.500  Top 5-err 7.960\t Test Loss 1.102\n",
            "Current best accuracy (top-1 and 5 error): 29.71 7.6\n",
            "Epoch: [168/290][99/782]\tLR: 0.012683\tTime 0.146 (0.150)\tData 0.000 (0.002)\tLoss 2.7595 (1.7328)\tTop 1-err 46.8750 (37.8438)\tTop 5-err 14.0625 (16.6016)\n",
            "Epoch: [168/290][199/782]\tLR: 0.012683\tTime 0.148 (0.149)\tData 0.000 (0.001)\tLoss 2.6311 (1.8023)\tTop 1-err 42.1875 (39.6445)\tTop 5-err 18.7500 (17.8672)\n",
            "Epoch: [168/290][299/782]\tLR: 0.012683\tTime 0.145 (0.149)\tData 0.000 (0.001)\tLoss 0.7365 (1.7775)\tTop 1-err 18.7500 (39.0052)\tTop 5-err 1.5625 (17.5964)\n",
            "Epoch: [168/290][399/782]\tLR: 0.012683\tTime 0.146 (0.148)\tData 0.000 (0.001)\tLoss 2.8747 (1.7398)\tTop 1-err 98.4375 (38.4492)\tTop 5-err 85.9375 (16.9590)\n",
            "Epoch: [168/290][499/782]\tLR: 0.012683\tTime 0.149 (0.148)\tData 0.000 (0.000)\tLoss 2.6265 (1.7654)\tTop 1-err 56.2500 (38.9859)\tTop 5-err 32.8125 (17.2609)\n",
            "Epoch: [168/290][599/782]\tLR: 0.012683\tTime 0.146 (0.148)\tData 0.000 (0.000)\tLoss 3.0752 (1.7860)\tTop 1-err 51.5625 (39.2904)\tTop 5-err 20.3125 (17.4479)\n",
            "Epoch: [168/290][699/782]\tLR: 0.012683\tTime 0.145 (0.148)\tData 0.000 (0.000)\tLoss 0.5786 (1.7968)\tTop 1-err 20.3125 (39.6942)\tTop 5-err 0.0000 (17.8058)\n",
            "* Epoch: [168/290]\t Top 1-err 40.164  Top 5-err 18.252\t Train Loss 1.822\n",
            "* Epoch: [168/290]\t Top 1-err 31.790  Top 5-err 8.200\t Test Loss 1.114\n",
            "Current best accuracy (top-1 and 5 error): 29.71 7.6\n",
            "Epoch: [169/290][99/782]\tLR: 0.012783\tTime 0.153 (0.151)\tData 0.000 (0.002)\tLoss 0.8797 (1.7577)\tTop 1-err 28.1250 (38.4062)\tTop 5-err 3.1250 (15.2969)\n",
            "Epoch: [169/290][199/782]\tLR: 0.012783\tTime 0.153 (0.150)\tData 0.000 (0.001)\tLoss 0.9465 (1.7842)\tTop 1-err 26.5625 (39.6562)\tTop 5-err 6.2500 (16.9492)\n",
            "Epoch: [169/290][299/782]\tLR: 0.012783\tTime 0.145 (0.150)\tData 0.000 (0.001)\tLoss 0.8945 (1.7809)\tTop 1-err 26.5625 (39.9349)\tTop 5-err 6.2500 (17.2109)\n",
            "Epoch: [169/290][399/782]\tLR: 0.012783\tTime 0.147 (0.149)\tData 0.000 (0.001)\tLoss 2.3188 (1.8381)\tTop 1-err 56.2500 (40.8906)\tTop 5-err 23.4375 (18.2148)\n",
            "Epoch: [169/290][499/782]\tLR: 0.012783\tTime 0.147 (0.149)\tData 0.000 (0.000)\tLoss 1.0861 (1.8087)\tTop 1-err 29.6875 (40.3016)\tTop 5-err 4.6875 (17.4406)\n",
            "Epoch: [169/290][599/782]\tLR: 0.012783\tTime 0.145 (0.149)\tData 0.000 (0.000)\tLoss 0.6808 (1.8101)\tTop 1-err 21.8750 (40.1172)\tTop 5-err 0.0000 (17.4310)\n",
            "Epoch: [169/290][699/782]\tLR: 0.012783\tTime 0.148 (0.148)\tData 0.000 (0.000)\tLoss 0.8674 (1.8091)\tTop 1-err 26.5625 (40.0781)\tTop 5-err 3.1250 (17.4475)\n",
            "* Epoch: [169/290]\t Top 1-err 40.370  Top 5-err 17.678\t Train Loss 1.827\n",
            "* Epoch: [169/290]\t Top 1-err 31.440  Top 5-err 8.680\t Test Loss 1.108\n",
            "Current best accuracy (top-1 and 5 error): 29.71 7.6\n",
            "Epoch: [170/290][99/782]\tLR: 0.012884\tTime 0.156 (0.150)\tData 0.000 (0.002)\tLoss 0.8568 (1.7597)\tTop 1-err 28.1250 (38.1953)\tTop 5-err 1.5625 (16.5625)\n",
            "Epoch: [170/290][199/782]\tLR: 0.012884\tTime 0.145 (0.149)\tData 0.000 (0.001)\tLoss 0.6914 (1.7875)\tTop 1-err 25.0000 (39.4766)\tTop 5-err 3.1250 (17.5977)\n",
            "Epoch: [170/290][299/782]\tLR: 0.012884\tTime 0.146 (0.149)\tData 0.000 (0.001)\tLoss 2.0936 (1.8313)\tTop 1-err 43.7500 (40.2370)\tTop 5-err 18.7500 (18.2318)\n",
            "Epoch: [170/290][399/782]\tLR: 0.012884\tTime 0.146 (0.149)\tData 0.000 (0.001)\tLoss 3.4836 (1.8400)\tTop 1-err 92.1875 (40.4043)\tTop 5-err 73.4375 (18.2031)\n",
            "Epoch: [170/290][499/782]\tLR: 0.012884\tTime 0.147 (0.149)\tData 0.000 (0.000)\tLoss 2.7344 (1.8005)\tTop 1-err 48.4375 (39.5641)\tTop 5-err 31.2500 (17.5484)\n",
            "Epoch: [170/290][599/782]\tLR: 0.012884\tTime 0.146 (0.149)\tData 0.000 (0.000)\tLoss 1.0516 (1.8006)\tTop 1-err 35.9375 (39.4609)\tTop 5-err 9.3750 (17.3372)\n",
            "Epoch: [170/290][699/782]\tLR: 0.012884\tTime 0.146 (0.149)\tData 0.000 (0.000)\tLoss 1.1671 (1.8146)\tTop 1-err 29.6875 (39.8147)\tTop 5-err 10.9375 (17.5480)\n",
            "* Epoch: [170/290]\t Top 1-err 40.029  Top 5-err 17.793\t Train Loss 1.821\n",
            "* Epoch: [170/290]\t Top 1-err 31.660  Top 5-err 8.550\t Test Loss 1.115\n",
            "Current best accuracy (top-1 and 5 error): 29.71 7.6\n",
            "Epoch: [171/290][99/782]\tLR: 0.012984\tTime 0.146 (0.152)\tData 0.000 (0.002)\tLoss 3.4456 (1.7606)\tTop 1-err 79.6875 (40.2422)\tTop 5-err 60.9375 (18.5625)\n",
            "Epoch: [171/290][199/782]\tLR: 0.012984\tTime 0.145 (0.150)\tData 0.000 (0.001)\tLoss 3.5153 (1.8130)\tTop 1-err 65.6250 (40.6328)\tTop 5-err 40.6250 (18.5117)\n",
            "Epoch: [171/290][299/782]\tLR: 0.012984\tTime 0.150 (0.149)\tData 0.000 (0.001)\tLoss 0.8772 (1.7354)\tTop 1-err 23.4375 (39.0755)\tTop 5-err 6.2500 (17.1276)\n",
            "Epoch: [171/290][399/782]\tLR: 0.012984\tTime 0.154 (0.149)\tData 0.000 (0.001)\tLoss 0.8934 (1.7597)\tTop 1-err 23.4375 (39.1621)\tTop 5-err 1.5625 (17.0703)\n",
            "Epoch: [171/290][499/782]\tLR: 0.012984\tTime 0.146 (0.149)\tData 0.000 (0.000)\tLoss 1.5893 (1.7820)\tTop 1-err 34.3750 (39.6281)\tTop 5-err 9.3750 (17.7469)\n",
            "Epoch: [171/290][599/782]\tLR: 0.012984\tTime 0.144 (0.149)\tData 0.000 (0.000)\tLoss 3.4135 (1.7928)\tTop 1-err 60.9375 (39.7135)\tTop 5-err 32.8125 (17.8346)\n",
            "Epoch: [171/290][699/782]\tLR: 0.012984\tTime 0.149 (0.148)\tData 0.000 (0.000)\tLoss 0.6878 (1.8069)\tTop 1-err 18.7500 (40.1138)\tTop 5-err 1.5625 (18.0179)\n",
            "* Epoch: [171/290]\t Top 1-err 40.227  Top 5-err 18.072\t Train Loss 1.812\n",
            "* Epoch: [171/290]\t Top 1-err 31.450  Top 5-err 8.400\t Test Loss 1.120\n",
            "Current best accuracy (top-1 and 5 error): 29.71 7.6\n",
            "Epoch: [172/290][99/782]\tLR: 0.013086\tTime 0.146 (0.151)\tData 0.000 (0.002)\tLoss 0.6999 (1.7009)\tTop 1-err 20.3125 (36.6406)\tTop 5-err 3.1250 (14.3359)\n",
            "Epoch: [172/290][199/782]\tLR: 0.013086\tTime 0.146 (0.149)\tData 0.000 (0.001)\tLoss 1.0486 (1.7781)\tTop 1-err 23.4375 (38.4844)\tTop 5-err 6.2500 (16.2383)\n",
            "Epoch: [172/290][299/782]\tLR: 0.013086\tTime 0.155 (0.149)\tData 0.000 (0.001)\tLoss 0.8560 (1.7719)\tTop 1-err 28.1250 (38.1302)\tTop 5-err 3.1250 (15.9141)\n",
            "Epoch: [172/290][399/782]\tLR: 0.013086\tTime 0.148 (0.149)\tData 0.000 (0.001)\tLoss 3.8383 (1.7673)\tTop 1-err 62.5000 (38.3613)\tTop 5-err 37.5000 (16.2148)\n",
            "Epoch: [172/290][499/782]\tLR: 0.013086\tTime 0.146 (0.149)\tData 0.000 (0.000)\tLoss 0.7650 (1.7895)\tTop 1-err 28.1250 (39.0219)\tTop 5-err 3.1250 (16.7594)\n",
            "Epoch: [172/290][599/782]\tLR: 0.013086\tTime 0.146 (0.148)\tData 0.000 (0.000)\tLoss 3.8486 (1.8183)\tTop 1-err 82.8125 (39.7044)\tTop 5-err 59.3750 (17.2643)\n",
            "Epoch: [172/290][699/782]\tLR: 0.013086\tTime 0.145 (0.148)\tData 0.000 (0.000)\tLoss 0.6151 (1.8027)\tTop 1-err 15.6250 (39.5815)\tTop 5-err 3.1250 (17.1038)\n",
            "* Epoch: [172/290]\t Top 1-err 39.930  Top 5-err 17.375\t Train Loss 1.811\n",
            "* Epoch: [172/290]\t Top 1-err 34.410  Top 5-err 9.840\t Test Loss 1.274\n",
            "Current best accuracy (top-1 and 5 error): 29.71 7.6\n",
            "Epoch: [173/290][99/782]\tLR: 0.013188\tTime 0.155 (0.151)\tData 0.000 (0.002)\tLoss 3.9545 (1.8233)\tTop 1-err 87.5000 (40.3828)\tTop 5-err 57.8125 (16.8125)\n",
            "Epoch: [173/290][199/782]\tLR: 0.013188\tTime 0.150 (0.151)\tData 0.000 (0.001)\tLoss 0.7944 (1.7619)\tTop 1-err 17.1875 (39.3359)\tTop 5-err 7.8125 (16.5312)\n",
            "Epoch: [173/290][299/782]\tLR: 0.013188\tTime 0.146 (0.150)\tData 0.000 (0.001)\tLoss 0.9985 (1.8359)\tTop 1-err 29.6875 (40.6667)\tTop 5-err 9.3750 (17.8021)\n",
            "Epoch: [173/290][399/782]\tLR: 0.013188\tTime 0.145 (0.149)\tData 0.000 (0.001)\tLoss 3.7046 (1.8330)\tTop 1-err 82.8125 (40.3691)\tTop 5-err 62.5000 (17.5625)\n",
            "Epoch: [173/290][499/782]\tLR: 0.013188\tTime 0.144 (0.149)\tData 0.000 (0.001)\tLoss 0.8678 (1.7950)\tTop 1-err 23.4375 (39.9844)\tTop 5-err 4.6875 (17.4797)\n",
            "Epoch: [173/290][599/782]\tLR: 0.013188\tTime 0.146 (0.148)\tData 0.000 (0.001)\tLoss 2.3768 (1.7736)\tTop 1-err 56.2500 (39.7826)\tTop 5-err 20.3125 (17.3268)\n",
            "Epoch: [173/290][699/782]\tLR: 0.013188\tTime 0.154 (0.148)\tData 0.000 (0.000)\tLoss 3.2112 (1.7906)\tTop 1-err 67.1875 (40.1228)\tTop 5-err 43.7500 (17.5636)\n",
            "* Epoch: [173/290]\t Top 1-err 40.037  Top 5-err 17.459\t Train Loss 1.792\n",
            "* Epoch: [173/290]\t Top 1-err 31.200  Top 5-err 8.150\t Test Loss 1.098\n",
            "Current best accuracy (top-1 and 5 error): 29.71 7.6\n",
            "Epoch: [174/290][99/782]\tLR: 0.013290\tTime 0.146 (0.150)\tData 0.000 (0.002)\tLoss 0.8516 (1.7874)\tTop 1-err 26.5625 (40.9609)\tTop 5-err 6.2500 (18.7266)\n",
            "Epoch: [174/290][199/782]\tLR: 0.013290\tTime 0.147 (0.149)\tData 0.000 (0.001)\tLoss 1.9279 (1.8138)\tTop 1-err 31.2500 (40.4219)\tTop 5-err 10.9375 (18.2734)\n",
            "Epoch: [174/290][299/782]\tLR: 0.013290\tTime 0.146 (0.148)\tData 0.000 (0.001)\tLoss 3.8387 (1.7884)\tTop 1-err 81.2500 (39.8646)\tTop 5-err 62.5000 (17.7266)\n",
            "Epoch: [174/290][399/782]\tLR: 0.013290\tTime 0.147 (0.148)\tData 0.000 (0.001)\tLoss 3.3249 (1.8266)\tTop 1-err 64.0625 (40.6914)\tTop 5-err 31.2500 (18.5742)\n",
            "Epoch: [174/290][499/782]\tLR: 0.013290\tTime 0.146 (0.148)\tData 0.000 (0.000)\tLoss 3.1922 (1.7907)\tTop 1-err 39.0625 (39.9594)\tTop 5-err 12.5000 (17.8828)\n",
            "Epoch: [174/290][599/782]\tLR: 0.013290\tTime 0.147 (0.148)\tData 0.000 (0.000)\tLoss 4.1550 (1.7947)\tTop 1-err 84.3750 (39.8841)\tTop 5-err 67.1875 (17.6237)\n",
            "Epoch: [174/290][699/782]\tLR: 0.013290\tTime 0.145 (0.148)\tData 0.000 (0.000)\tLoss 0.8828 (1.7964)\tTop 1-err 25.0000 (39.9208)\tTop 5-err 6.2500 (17.5312)\n",
            "* Epoch: [174/290]\t Top 1-err 40.260  Top 5-err 17.790\t Train Loss 1.808\n",
            "* Epoch: [174/290]\t Top 1-err 30.910  Top 5-err 8.370\t Test Loss 1.087\n",
            "Current best accuracy (top-1 and 5 error): 29.71 7.6\n",
            "Epoch: [175/290][99/782]\tLR: 0.013393\tTime 0.148 (0.151)\tData 0.000 (0.002)\tLoss 2.8481 (1.8742)\tTop 1-err 50.0000 (40.8906)\tTop 5-err 21.8750 (19.3125)\n",
            "Epoch: [175/290][199/782]\tLR: 0.013393\tTime 0.146 (0.149)\tData 0.000 (0.001)\tLoss 3.5499 (1.8067)\tTop 1-err 75.0000 (39.5586)\tTop 5-err 42.1875 (17.8516)\n",
            "Epoch: [175/290][299/782]\tLR: 0.013393\tTime 0.146 (0.149)\tData 0.000 (0.001)\tLoss 3.5720 (1.7421)\tTop 1-err 85.9375 (38.7839)\tTop 5-err 59.3750 (16.9714)\n",
            "Epoch: [175/290][399/782]\tLR: 0.013393\tTime 0.145 (0.148)\tData 0.000 (0.001)\tLoss 1.1631 (1.7387)\tTop 1-err 37.5000 (38.5703)\tTop 5-err 7.8125 (16.5117)\n",
            "Epoch: [175/290][499/782]\tLR: 0.013393\tTime 0.147 (0.148)\tData 0.000 (0.000)\tLoss 3.5522 (1.8035)\tTop 1-err 79.6875 (39.8406)\tTop 5-err 53.1250 (17.4828)\n",
            "Epoch: [175/290][599/782]\tLR: 0.013393\tTime 0.145 (0.148)\tData 0.000 (0.000)\tLoss 1.1098 (1.8097)\tTop 1-err 35.9375 (40.0000)\tTop 5-err 7.8125 (17.5182)\n",
            "Epoch: [175/290][699/782]\tLR: 0.013393\tTime 0.146 (0.148)\tData 0.000 (0.000)\tLoss 3.1812 (1.8396)\tTop 1-err 54.6875 (40.5268)\tTop 5-err 29.6875 (17.8225)\n",
            "* Epoch: [175/290]\t Top 1-err 40.190  Top 5-err 17.581\t Train Loss 1.821\n",
            "* Epoch: [175/290]\t Top 1-err 32.030  Top 5-err 8.930\t Test Loss 1.184\n",
            "Current best accuracy (top-1 and 5 error): 29.71 7.6\n",
            "Epoch: [176/290][99/782]\tLR: 0.013496\tTime 0.146 (0.151)\tData 0.000 (0.002)\tLoss 0.8454 (1.6742)\tTop 1-err 23.4375 (36.6094)\tTop 5-err 4.6875 (15.2266)\n",
            "Epoch: [176/290][199/782]\tLR: 0.013496\tTime 0.146 (0.150)\tData 0.000 (0.001)\tLoss 3.4335 (1.6443)\tTop 1-err 75.0000 (36.7266)\tTop 5-err 53.1250 (15.3633)\n",
            "Epoch: [176/290][299/782]\tLR: 0.013496\tTime 0.146 (0.149)\tData 0.000 (0.001)\tLoss 0.8089 (1.6753)\tTop 1-err 23.4375 (37.6146)\tTop 5-err 3.1250 (15.9453)\n",
            "Epoch: [176/290][399/782]\tLR: 0.013496\tTime 0.147 (0.149)\tData 0.000 (0.001)\tLoss 0.9383 (1.7267)\tTop 1-err 26.5625 (38.4531)\tTop 5-err 4.6875 (16.4805)\n",
            "Epoch: [176/290][499/782]\tLR: 0.013496\tTime 0.149 (0.148)\tData 0.000 (0.000)\tLoss 2.7371 (1.7545)\tTop 1-err 34.3750 (38.9578)\tTop 5-err 6.2500 (16.8328)\n",
            "Epoch: [176/290][599/782]\tLR: 0.013496\tTime 0.146 (0.148)\tData 0.000 (0.000)\tLoss 1.2364 (1.7668)\tTop 1-err 29.6875 (39.2357)\tTop 5-err 17.1875 (17.0677)\n",
            "Epoch: [176/290][699/782]\tLR: 0.013496\tTime 0.146 (0.148)\tData 0.000 (0.000)\tLoss 1.0500 (1.7527)\tTop 1-err 31.2500 (38.9788)\tTop 5-err 4.6875 (16.7355)\n",
            "* Epoch: [176/290]\t Top 1-err 38.663  Top 5-err 16.411\t Train Loss 1.733\n",
            "* Epoch: [176/290]\t Top 1-err 30.290  Top 5-err 7.780\t Test Loss 1.057\n",
            "Current best accuracy (top-1 and 5 error): 29.71 7.6\n",
            "Epoch: [177/290][99/782]\tLR: 0.013600\tTime 0.145 (0.150)\tData 0.000 (0.002)\tLoss 0.8337 (1.8657)\tTop 1-err 25.0000 (41.9688)\tTop 5-err 1.5625 (19.7656)\n",
            "Epoch: [177/290][199/782]\tLR: 0.013600\tTime 0.155 (0.149)\tData 0.000 (0.001)\tLoss 4.7961 (1.8451)\tTop 1-err 93.7500 (41.2656)\tTop 5-err 82.8125 (18.9062)\n",
            "Epoch: [177/290][299/782]\tLR: 0.013600\tTime 0.154 (0.149)\tData 0.000 (0.001)\tLoss 0.8515 (1.8530)\tTop 1-err 21.8750 (40.9036)\tTop 5-err 1.5625 (18.6589)\n",
            "Epoch: [177/290][399/782]\tLR: 0.013600\tTime 0.145 (0.148)\tData 0.000 (0.001)\tLoss 0.9846 (1.8411)\tTop 1-err 26.5625 (40.6445)\tTop 5-err 6.2500 (18.4121)\n",
            "Epoch: [177/290][499/782]\tLR: 0.013600\tTime 0.154 (0.148)\tData 0.000 (0.000)\tLoss 1.0251 (1.8403)\tTop 1-err 29.6875 (40.6109)\tTop 5-err 4.6875 (18.2453)\n",
            "Epoch: [177/290][599/782]\tLR: 0.013600\tTime 0.145 (0.148)\tData 0.000 (0.000)\tLoss 0.7263 (1.8418)\tTop 1-err 18.7500 (40.9206)\tTop 5-err 4.6875 (18.4635)\n",
            "Epoch: [177/290][699/782]\tLR: 0.013600\tTime 0.147 (0.148)\tData 0.000 (0.000)\tLoss 1.2976 (1.8518)\tTop 1-err 35.9375 (41.0033)\tTop 5-err 6.2500 (18.4475)\n",
            "* Epoch: [177/290]\t Top 1-err 41.248  Top 5-err 18.620\t Train Loss 1.863\n",
            "* Epoch: [177/290]\t Top 1-err 31.100  Top 5-err 8.060\t Test Loss 1.087\n",
            "Current best accuracy (top-1 and 5 error): 29.71 7.6\n",
            "Epoch: [178/290][99/782]\tLR: 0.013704\tTime 0.144 (0.151)\tData 0.000 (0.002)\tLoss 0.7474 (1.7585)\tTop 1-err 20.3125 (38.8672)\tTop 5-err 6.2500 (16.3438)\n",
            "Epoch: [178/290][199/782]\tLR: 0.013704\tTime 0.155 (0.150)\tData 0.000 (0.001)\tLoss 1.2430 (1.6715)\tTop 1-err 37.5000 (36.9883)\tTop 5-err 12.5000 (15.5625)\n",
            "Epoch: [178/290][299/782]\tLR: 0.013704\tTime 0.150 (0.149)\tData 0.000 (0.001)\tLoss 2.9163 (1.7319)\tTop 1-err 48.4375 (38.1745)\tTop 5-err 18.7500 (16.2604)\n",
            "Epoch: [178/290][399/782]\tLR: 0.013704\tTime 0.147 (0.149)\tData 0.000 (0.001)\tLoss 3.4494 (1.7806)\tTop 1-err 96.8750 (38.8750)\tTop 5-err 81.2500 (16.7969)\n",
            "Epoch: [178/290][499/782]\tLR: 0.013704\tTime 0.145 (0.148)\tData 0.000 (0.001)\tLoss 0.9114 (1.7925)\tTop 1-err 26.5625 (39.1125)\tTop 5-err 6.2500 (16.9250)\n",
            "Epoch: [178/290][599/782]\tLR: 0.013704\tTime 0.147 (0.148)\tData 0.000 (0.000)\tLoss 2.8634 (1.7826)\tTop 1-err 87.5000 (38.9557)\tTop 5-err 60.9375 (16.7096)\n",
            "Epoch: [178/290][699/782]\tLR: 0.013704\tTime 0.146 (0.148)\tData 0.000 (0.000)\tLoss 0.8491 (1.7855)\tTop 1-err 18.7500 (39.0212)\tTop 5-err 4.6875 (16.8292)\n",
            "* Epoch: [178/290]\t Top 1-err 39.434  Top 5-err 17.137\t Train Loss 1.798\n",
            "* Epoch: [178/290]\t Top 1-err 31.040  Top 5-err 8.330\t Test Loss 1.088\n",
            "Current best accuracy (top-1 and 5 error): 29.71 7.6\n",
            "Epoch: [179/290][99/782]\tLR: 0.013809\tTime 0.146 (0.149)\tData 0.000 (0.002)\tLoss 1.0356 (1.7075)\tTop 1-err 37.5000 (38.5078)\tTop 5-err 7.8125 (16.4219)\n",
            "Epoch: [179/290][199/782]\tLR: 0.013809\tTime 0.146 (0.149)\tData 0.000 (0.001)\tLoss 3.1406 (1.7098)\tTop 1-err 79.6875 (38.8555)\tTop 5-err 57.8125 (16.8516)\n",
            "Epoch: [179/290][299/782]\tLR: 0.013809\tTime 0.148 (0.149)\tData 0.000 (0.001)\tLoss 0.9547 (1.7305)\tTop 1-err 29.6875 (39.4089)\tTop 5-err 4.6875 (17.5234)\n",
            "Epoch: [179/290][399/782]\tLR: 0.013809\tTime 0.143 (0.148)\tData 0.000 (0.001)\tLoss 3.6407 (1.7714)\tTop 1-err 85.9375 (39.9746)\tTop 5-err 54.6875 (17.8262)\n",
            "Epoch: [179/290][499/782]\tLR: 0.013809\tTime 0.146 (0.148)\tData 0.000 (0.001)\tLoss 2.0485 (1.7918)\tTop 1-err 46.8750 (40.1484)\tTop 5-err 12.5000 (17.9594)\n",
            "Epoch: [179/290][599/782]\tLR: 0.013809\tTime 0.145 (0.148)\tData 0.000 (0.000)\tLoss 0.8195 (1.7945)\tTop 1-err 21.8750 (40.2682)\tTop 5-err 0.0000 (17.8971)\n",
            "Epoch: [179/290][699/782]\tLR: 0.013809\tTime 0.145 (0.148)\tData 0.000 (0.000)\tLoss 0.7447 (1.7964)\tTop 1-err 21.8750 (40.1551)\tTop 5-err 1.5625 (17.8917)\n",
            "* Epoch: [179/290]\t Top 1-err 40.632  Top 5-err 18.360\t Train Loss 1.819\n",
            "* Epoch: [179/290]\t Top 1-err 32.240  Top 5-err 8.800\t Test Loss 1.157\n",
            "Current best accuracy (top-1 and 5 error): 29.71 7.6\n",
            "Epoch: [180/290][99/782]\tLR: 0.013915\tTime 0.145 (0.151)\tData 0.000 (0.002)\tLoss 1.0109 (1.7713)\tTop 1-err 29.6875 (38.8828)\tTop 5-err 6.2500 (15.9531)\n",
            "Epoch: [180/290][199/782]\tLR: 0.013915\tTime 0.148 (0.149)\tData 0.000 (0.001)\tLoss 2.7067 (1.7486)\tTop 1-err 62.5000 (38.4336)\tTop 5-err 29.6875 (15.8398)\n",
            "Epoch: [180/290][299/782]\tLR: 0.013915\tTime 0.145 (0.148)\tData 0.000 (0.001)\tLoss 0.8786 (1.7317)\tTop 1-err 23.4375 (38.1380)\tTop 5-err 6.2500 (15.5391)\n",
            "Epoch: [180/290][399/782]\tLR: 0.013915\tTime 0.146 (0.148)\tData 0.000 (0.001)\tLoss 5.5971 (1.7845)\tTop 1-err 100.0000 (39.1680)\tTop 5-err 87.5000 (16.5938)\n",
            "Epoch: [180/290][499/782]\tLR: 0.013915\tTime 0.149 (0.148)\tData 0.000 (0.000)\tLoss 0.7785 (1.7827)\tTop 1-err 23.4375 (39.0422)\tTop 5-err 3.1250 (16.5703)\n",
            "Epoch: [180/290][599/782]\tLR: 0.013915\tTime 0.146 (0.148)\tData 0.000 (0.000)\tLoss 0.9024 (1.7976)\tTop 1-err 26.5625 (39.3698)\tTop 5-err 6.2500 (16.8281)\n",
            "Epoch: [180/290][699/782]\tLR: 0.013915\tTime 0.151 (0.148)\tData 0.000 (0.000)\tLoss 4.2210 (1.7873)\tTop 1-err 100.0000 (39.2042)\tTop 5-err 93.7500 (16.7567)\n",
            "* Epoch: [180/290]\t Top 1-err 39.192  Top 5-err 16.827\t Train Loss 1.779\n",
            "* Epoch: [180/290]\t Top 1-err 30.660  Top 5-err 8.180\t Test Loss 1.070\n",
            "Current best accuracy (top-1 and 5 error): 29.71 7.6\n",
            "Epoch: [181/290][99/782]\tLR: 0.014021\tTime 0.149 (0.149)\tData 0.000 (0.002)\tLoss 0.9017 (1.8508)\tTop 1-err 31.2500 (40.4453)\tTop 5-err 3.1250 (18.4219)\n",
            "Epoch: [181/290][199/782]\tLR: 0.014021\tTime 0.154 (0.149)\tData 0.000 (0.001)\tLoss 0.5227 (1.8352)\tTop 1-err 12.5000 (40.3633)\tTop 5-err 1.5625 (18.2422)\n",
            "Epoch: [181/290][299/782]\tLR: 0.014021\tTime 0.150 (0.149)\tData 0.000 (0.001)\tLoss 0.7847 (1.8681)\tTop 1-err 21.8750 (41.0677)\tTop 5-err 3.1250 (18.7240)\n",
            "Epoch: [181/290][399/782]\tLR: 0.014021\tTime 0.145 (0.148)\tData 0.000 (0.001)\tLoss 1.1298 (1.8763)\tTop 1-err 37.5000 (41.0781)\tTop 5-err 6.2500 (18.6797)\n",
            "Epoch: [181/290][499/782]\tLR: 0.014021\tTime 0.143 (0.148)\tData 0.000 (0.000)\tLoss 3.4973 (1.8846)\tTop 1-err 68.7500 (41.4391)\tTop 5-err 34.3750 (18.9828)\n",
            "Epoch: [181/290][599/782]\tLR: 0.014021\tTime 0.146 (0.148)\tData 0.000 (0.000)\tLoss 0.7471 (1.8861)\tTop 1-err 23.4375 (41.4154)\tTop 5-err 4.6875 (18.8242)\n",
            "Epoch: [181/290][699/782]\tLR: 0.014021\tTime 0.147 (0.148)\tData 0.000 (0.000)\tLoss 3.7920 (1.8920)\tTop 1-err 68.7500 (41.3125)\tTop 5-err 40.6250 (18.6250)\n",
            "* Epoch: [181/290]\t Top 1-err 41.400  Top 5-err 18.566\t Train Loss 1.894\n",
            "* Epoch: [181/290]\t Top 1-err 30.350  Top 5-err 7.980\t Test Loss 1.065\n",
            "Current best accuracy (top-1 and 5 error): 29.71 7.6\n",
            "Epoch: [182/290][99/782]\tLR: 0.014127\tTime 0.158 (0.150)\tData 0.000 (0.002)\tLoss 3.5584 (1.7869)\tTop 1-err 75.0000 (38.9609)\tTop 5-err 50.0000 (17.3828)\n",
            "Epoch: [182/290][199/782]\tLR: 0.014127\tTime 0.147 (0.149)\tData 0.000 (0.001)\tLoss 3.3639 (1.7891)\tTop 1-err 84.3750 (39.4180)\tTop 5-err 67.1875 (17.3906)\n",
            "Epoch: [182/290][299/782]\tLR: 0.014127\tTime 0.145 (0.148)\tData 0.000 (0.001)\tLoss 4.2197 (1.8233)\tTop 1-err 81.2500 (40.0755)\tTop 5-err 68.7500 (17.6641)\n",
            "Epoch: [182/290][399/782]\tLR: 0.014127\tTime 0.146 (0.148)\tData 0.000 (0.001)\tLoss 1.1469 (1.8530)\tTop 1-err 31.2500 (40.7266)\tTop 5-err 6.2500 (18.0938)\n",
            "Epoch: [182/290][499/782]\tLR: 0.014127\tTime 0.151 (0.148)\tData 0.000 (0.001)\tLoss 0.8187 (1.8417)\tTop 1-err 20.3125 (40.6375)\tTop 5-err 1.5625 (18.0953)\n",
            "Epoch: [182/290][599/782]\tLR: 0.014127\tTime 0.147 (0.148)\tData 0.000 (0.000)\tLoss 3.2102 (1.8210)\tTop 1-err 45.3125 (40.1458)\tTop 5-err 21.8750 (17.6836)\n",
            "Epoch: [182/290][699/782]\tLR: 0.014127\tTime 0.145 (0.148)\tData 0.000 (0.000)\tLoss 0.9199 (1.8392)\tTop 1-err 34.3750 (40.5748)\tTop 5-err 4.6875 (18.1551)\n",
            "* Epoch: [182/290]\t Top 1-err 40.390  Top 5-err 17.894\t Train Loss 1.832\n",
            "* Epoch: [182/290]\t Top 1-err 30.960  Top 5-err 8.190\t Test Loss 1.083\n",
            "Current best accuracy (top-1 and 5 error): 29.71 7.6\n",
            "Epoch: [183/290][99/782]\tLR: 0.014234\tTime 0.144 (0.151)\tData 0.000 (0.002)\tLoss 0.7985 (1.8838)\tTop 1-err 20.3125 (41.4609)\tTop 5-err 6.2500 (18.4453)\n",
            "Epoch: [183/290][199/782]\tLR: 0.014234\tTime 0.146 (0.149)\tData 0.000 (0.001)\tLoss 3.2191 (1.8290)\tTop 1-err 65.6250 (40.1328)\tTop 5-err 32.8125 (17.1641)\n",
            "Epoch: [183/290][299/782]\tLR: 0.014234\tTime 0.149 (0.149)\tData 0.000 (0.001)\tLoss 3.3149 (1.7801)\tTop 1-err 53.1250 (39.3906)\tTop 5-err 35.9375 (16.5651)\n",
            "Epoch: [183/290][399/782]\tLR: 0.014234\tTime 0.149 (0.149)\tData 0.000 (0.001)\tLoss 0.9763 (1.7456)\tTop 1-err 23.4375 (38.6914)\tTop 5-err 3.1250 (16.1738)\n",
            "Epoch: [183/290][499/782]\tLR: 0.014234\tTime 0.145 (0.148)\tData 0.000 (0.001)\tLoss 2.8173 (1.7670)\tTop 1-err 62.5000 (39.1172)\tTop 5-err 20.3125 (16.5281)\n",
            "Epoch: [183/290][599/782]\tLR: 0.014234\tTime 0.145 (0.148)\tData 0.000 (0.000)\tLoss 0.7653 (1.7661)\tTop 1-err 18.7500 (39.1953)\tTop 5-err 3.1250 (16.6497)\n",
            "Epoch: [183/290][699/782]\tLR: 0.014234\tTime 0.147 (0.148)\tData 0.000 (0.000)\tLoss 3.5800 (1.8172)\tTop 1-err 76.5625 (39.9833)\tTop 5-err 51.5625 (17.4018)\n",
            "* Epoch: [183/290]\t Top 1-err 39.752  Top 5-err 17.249\t Train Loss 1.808\n",
            "* Epoch: [183/290]\t Top 1-err 30.850  Top 5-err 8.340\t Test Loss 1.085\n",
            "Current best accuracy (top-1 and 5 error): 29.71 7.6\n",
            "Epoch: [184/290][99/782]\tLR: 0.014341\tTime 0.144 (0.149)\tData 0.000 (0.002)\tLoss 0.8618 (1.7767)\tTop 1-err 20.3125 (39.8750)\tTop 5-err 4.6875 (17.8984)\n",
            "Epoch: [184/290][199/782]\tLR: 0.014341\tTime 0.145 (0.149)\tData 0.000 (0.001)\tLoss 0.8037 (1.7290)\tTop 1-err 15.6250 (38.3711)\tTop 5-err 6.2500 (16.2539)\n",
            "Epoch: [184/290][299/782]\tLR: 0.014341\tTime 0.147 (0.149)\tData 0.000 (0.001)\tLoss 3.9045 (1.7346)\tTop 1-err 82.8125 (38.2786)\tTop 5-err 67.1875 (16.2474)\n",
            "Epoch: [184/290][399/782]\tLR: 0.014341\tTime 0.145 (0.148)\tData 0.000 (0.001)\tLoss 0.6654 (1.7398)\tTop 1-err 17.1875 (38.6367)\tTop 5-err 3.1250 (16.4512)\n",
            "Epoch: [184/290][499/782]\tLR: 0.014341\tTime 0.146 (0.148)\tData 0.000 (0.001)\tLoss 0.8633 (1.7537)\tTop 1-err 18.7500 (39.2812)\tTop 5-err 4.6875 (17.0109)\n",
            "Epoch: [184/290][599/782]\tLR: 0.014341\tTime 0.146 (0.148)\tData 0.000 (0.000)\tLoss 0.7698 (1.7638)\tTop 1-err 26.5625 (39.2630)\tTop 5-err 4.6875 (17.1289)\n",
            "Epoch: [184/290][699/782]\tLR: 0.014341\tTime 0.151 (0.148)\tData 0.000 (0.000)\tLoss 3.4657 (1.7650)\tTop 1-err 75.0000 (39.3147)\tTop 5-err 35.9375 (17.0167)\n",
            "* Epoch: [184/290]\t Top 1-err 39.554  Top 5-err 17.211\t Train Loss 1.780\n",
            "* Epoch: [184/290]\t Top 1-err 31.720  Top 5-err 8.550\t Test Loss 1.114\n",
            "Current best accuracy (top-1 and 5 error): 29.71 7.6\n",
            "Epoch: [185/290][99/782]\tLR: 0.014449\tTime 0.145 (0.149)\tData 0.000 (0.002)\tLoss 0.7846 (1.8168)\tTop 1-err 17.1875 (39.5391)\tTop 5-err 3.1250 (17.7578)\n",
            "Epoch: [185/290][199/782]\tLR: 0.014449\tTime 0.145 (0.148)\tData 0.000 (0.001)\tLoss 0.7502 (1.7634)\tTop 1-err 23.4375 (37.9727)\tTop 5-err 4.6875 (15.8242)\n",
            "Epoch: [185/290][299/782]\tLR: 0.014449\tTime 0.160 (0.148)\tData 0.000 (0.001)\tLoss 2.5694 (1.7800)\tTop 1-err 64.0625 (38.6172)\tTop 5-err 31.2500 (16.3099)\n",
            "Epoch: [185/290][399/782]\tLR: 0.014449\tTime 0.154 (0.148)\tData 0.000 (0.001)\tLoss 3.2339 (1.7889)\tTop 1-err 50.0000 (38.8711)\tTop 5-err 25.0000 (16.2598)\n",
            "Epoch: [185/290][499/782]\tLR: 0.014449\tTime 0.150 (0.148)\tData 0.000 (0.000)\tLoss 0.8793 (1.8174)\tTop 1-err 28.1250 (39.3438)\tTop 5-err 6.2500 (16.7766)\n",
            "Epoch: [185/290][599/782]\tLR: 0.014449\tTime 0.148 (0.148)\tData 0.000 (0.000)\tLoss 2.6883 (1.8145)\tTop 1-err 98.4375 (39.6615)\tTop 5-err 93.7500 (17.0404)\n",
            "Epoch: [185/290][699/782]\tLR: 0.014449\tTime 0.148 (0.148)\tData 0.000 (0.000)\tLoss 4.1157 (1.8142)\tTop 1-err 85.9375 (39.7478)\tTop 5-err 59.3750 (17.1942)\n",
            "* Epoch: [185/290]\t Top 1-err 39.331  Top 5-err 16.870\t Train Loss 1.790\n",
            "* Epoch: [185/290]\t Top 1-err 30.520  Top 5-err 7.950\t Test Loss 1.061\n",
            "Current best accuracy (top-1 and 5 error): 29.71 7.6\n",
            "Epoch: [186/290][99/782]\tLR: 0.014557\tTime 0.145 (0.150)\tData 0.000 (0.002)\tLoss 0.8602 (1.6389)\tTop 1-err 23.4375 (36.0703)\tTop 5-err 9.3750 (15.0781)\n",
            "Epoch: [186/290][199/782]\tLR: 0.014557\tTime 0.145 (0.149)\tData 0.000 (0.001)\tLoss 0.8171 (1.7228)\tTop 1-err 18.7500 (38.0586)\tTop 5-err 6.2500 (16.6445)\n",
            "Epoch: [186/290][299/782]\tLR: 0.014557\tTime 0.146 (0.148)\tData 0.000 (0.001)\tLoss 3.3086 (1.7225)\tTop 1-err 62.5000 (37.7943)\tTop 5-err 34.3750 (16.1510)\n",
            "Epoch: [186/290][399/782]\tLR: 0.014557\tTime 0.145 (0.148)\tData 0.000 (0.001)\tLoss 0.8013 (1.7303)\tTop 1-err 26.5625 (38.4141)\tTop 5-err 1.5625 (16.5801)\n",
            "Epoch: [186/290][499/782]\tLR: 0.014557\tTime 0.146 (0.148)\tData 0.000 (0.001)\tLoss 2.2045 (1.7560)\tTop 1-err 43.7500 (38.6047)\tTop 5-err 12.5000 (16.3266)\n",
            "Epoch: [186/290][599/782]\tLR: 0.014557\tTime 0.146 (0.148)\tData 0.000 (0.000)\tLoss 0.7159 (1.7735)\tTop 1-err 18.7500 (39.1953)\tTop 5-err 4.6875 (16.7982)\n",
            "Epoch: [186/290][699/782]\tLR: 0.014557\tTime 0.145 (0.148)\tData 0.000 (0.000)\tLoss 2.2769 (1.7834)\tTop 1-err 42.1875 (39.1518)\tTop 5-err 12.5000 (16.7210)\n",
            "* Epoch: [186/290]\t Top 1-err 39.786  Top 5-err 17.401\t Train Loss 1.812\n",
            "* Epoch: [186/290]\t Top 1-err 29.540  Top 5-err 7.720\t Test Loss 1.037\n",
            "Current best accuracy (top-1 and 5 error): 29.54 7.72\n",
            "Epoch: [187/290][99/782]\tLR: 0.014666\tTime 0.144 (0.150)\tData 0.000 (0.002)\tLoss 0.6741 (1.9022)\tTop 1-err 20.3125 (42.1484)\tTop 5-err 1.5625 (19.8203)\n",
            "Epoch: [187/290][199/782]\tLR: 0.014666\tTime 0.145 (0.149)\tData 0.000 (0.001)\tLoss 2.9210 (1.9685)\tTop 1-err 51.5625 (43.1680)\tTop 5-err 18.7500 (20.6094)\n",
            "Epoch: [187/290][299/782]\tLR: 0.014666\tTime 0.152 (0.149)\tData 0.000 (0.001)\tLoss 0.9844 (1.9667)\tTop 1-err 26.5625 (43.0182)\tTop 5-err 3.1250 (20.3281)\n",
            "Epoch: [187/290][399/782]\tLR: 0.014666\tTime 0.154 (0.149)\tData 0.000 (0.001)\tLoss 0.8756 (1.9024)\tTop 1-err 23.4375 (41.3789)\tTop 5-err 4.6875 (18.8848)\n",
            "Epoch: [187/290][499/782]\tLR: 0.014666\tTime 0.147 (0.148)\tData 0.000 (0.001)\tLoss 0.8809 (1.8886)\tTop 1-err 23.4375 (41.0422)\tTop 5-err 3.1250 (18.3953)\n",
            "Epoch: [187/290][599/782]\tLR: 0.014666\tTime 0.147 (0.148)\tData 0.000 (0.000)\tLoss 2.9757 (1.8982)\tTop 1-err 45.3125 (41.3177)\tTop 5-err 17.1875 (18.5195)\n",
            "Epoch: [187/290][699/782]\tLR: 0.014666\tTime 0.156 (0.148)\tData 0.000 (0.000)\tLoss 0.5177 (1.8605)\tTop 1-err 17.1875 (40.7768)\tTop 5-err 1.5625 (18.0681)\n",
            "* Epoch: [187/290]\t Top 1-err 41.664  Top 5-err 18.693\t Train Loss 1.900\n",
            "* Epoch: [187/290]\t Top 1-err 33.840  Top 5-err 9.630\t Test Loss 1.259\n",
            "Current best accuracy (top-1 and 5 error): 29.54 7.72\n",
            "Epoch: [188/290][99/782]\tLR: 0.014775\tTime 0.154 (0.149)\tData 0.000 (0.002)\tLoss 0.9941 (1.7314)\tTop 1-err 31.2500 (38.6406)\tTop 5-err 3.1250 (16.6875)\n",
            "Epoch: [188/290][199/782]\tLR: 0.014775\tTime 0.148 (0.149)\tData 0.000 (0.001)\tLoss 2.4303 (1.7776)\tTop 1-err 45.3125 (39.4336)\tTop 5-err 21.8750 (17.2930)\n",
            "Epoch: [188/290][299/782]\tLR: 0.014775\tTime 0.150 (0.149)\tData 0.000 (0.001)\tLoss 1.0337 (1.7053)\tTop 1-err 31.2500 (37.8177)\tTop 5-err 7.8125 (16.1953)\n",
            "Epoch: [188/290][399/782]\tLR: 0.014775\tTime 0.145 (0.148)\tData 0.000 (0.001)\tLoss 0.9554 (1.6850)\tTop 1-err 25.0000 (37.4258)\tTop 5-err 9.3750 (15.9941)\n",
            "Epoch: [188/290][499/782]\tLR: 0.014775\tTime 0.159 (0.148)\tData 0.000 (0.001)\tLoss 0.8093 (1.7012)\tTop 1-err 23.4375 (37.6734)\tTop 5-err 9.3750 (16.0781)\n",
            "Epoch: [188/290][599/782]\tLR: 0.014775\tTime 0.147 (0.148)\tData 0.000 (0.000)\tLoss 2.2798 (1.7227)\tTop 1-err 43.7500 (38.1250)\tTop 5-err 21.8750 (16.5312)\n",
            "Epoch: [188/290][699/782]\tLR: 0.014775\tTime 0.144 (0.148)\tData 0.000 (0.000)\tLoss 0.7100 (1.7426)\tTop 1-err 23.4375 (38.6819)\tTop 5-err 4.6875 (16.8493)\n",
            "* Epoch: [188/290]\t Top 1-err 39.344  Top 5-err 17.401\t Train Loss 1.778\n",
            "* Epoch: [188/290]\t Top 1-err 33.220  Top 5-err 9.590\t Test Loss 1.200\n",
            "Current best accuracy (top-1 and 5 error): 29.54 7.72\n",
            "Epoch: [189/290][99/782]\tLR: 0.014885\tTime 0.147 (0.149)\tData 0.000 (0.002)\tLoss 0.8666 (1.7555)\tTop 1-err 20.3125 (37.3750)\tTop 5-err 4.6875 (15.0469)\n",
            "Epoch: [189/290][199/782]\tLR: 0.014885\tTime 0.144 (0.148)\tData 0.000 (0.001)\tLoss 2.5264 (1.7789)\tTop 1-err 39.0625 (38.2227)\tTop 5-err 14.0625 (16.2188)\n",
            "Epoch: [189/290][299/782]\tLR: 0.014885\tTime 0.145 (0.148)\tData 0.000 (0.001)\tLoss 0.7741 (1.7622)\tTop 1-err 20.3125 (38.2839)\tTop 5-err 3.1250 (16.3411)\n",
            "Epoch: [189/290][399/782]\tLR: 0.014885\tTime 0.146 (0.148)\tData 0.000 (0.001)\tLoss 0.9437 (1.7531)\tTop 1-err 23.4375 (38.6309)\tTop 5-err 7.8125 (16.7988)\n",
            "Epoch: [189/290][499/782]\tLR: 0.014885\tTime 0.153 (0.148)\tData 0.000 (0.001)\tLoss 4.2125 (1.7553)\tTop 1-err 90.6250 (38.6875)\tTop 5-err 81.2500 (16.7516)\n",
            "Epoch: [189/290][599/782]\tLR: 0.014885\tTime 0.151 (0.148)\tData 0.000 (0.000)\tLoss 2.2879 (1.7833)\tTop 1-err 28.1250 (39.2161)\tTop 5-err 7.8125 (17.0651)\n",
            "Epoch: [189/290][699/782]\tLR: 0.014885\tTime 0.146 (0.148)\tData 0.000 (0.000)\tLoss 0.7107 (1.8169)\tTop 1-err 23.4375 (39.7946)\tTop 5-err 1.5625 (17.3984)\n",
            "* Epoch: [189/290]\t Top 1-err 40.048  Top 5-err 17.669\t Train Loss 1.832\n",
            "* Epoch: [189/290]\t Top 1-err 30.970  Top 5-err 7.930\t Test Loss 1.072\n",
            "Current best accuracy (top-1 and 5 error): 29.54 7.72\n",
            "Epoch: [190/290][99/782]\tLR: 0.014995\tTime 0.146 (0.149)\tData 0.000 (0.003)\tLoss 4.4891 (1.8639)\tTop 1-err 96.8750 (40.7812)\tTop 5-err 89.0625 (18.7969)\n",
            "Epoch: [190/290][199/782]\tLR: 0.014995\tTime 0.148 (0.149)\tData 0.000 (0.001)\tLoss 3.7420 (1.8197)\tTop 1-err 64.0625 (39.8906)\tTop 5-err 35.9375 (17.3359)\n",
            "Epoch: [190/290][299/782]\tLR: 0.014995\tTime 0.145 (0.148)\tData 0.000 (0.001)\tLoss 0.7967 (1.8319)\tTop 1-err 25.0000 (39.9818)\tTop 5-err 3.1250 (17.3802)\n",
            "Epoch: [190/290][399/782]\tLR: 0.014995\tTime 0.146 (0.148)\tData 0.000 (0.001)\tLoss 3.6868 (1.8411)\tTop 1-err 93.7500 (40.1797)\tTop 5-err 71.8750 (17.7598)\n",
            "Epoch: [190/290][499/782]\tLR: 0.014995\tTime 0.145 (0.148)\tData 0.000 (0.001)\tLoss 3.4021 (1.8481)\tTop 1-err 62.5000 (40.4656)\tTop 5-err 32.8125 (18.0734)\n",
            "Epoch: [190/290][599/782]\tLR: 0.014995\tTime 0.146 (0.148)\tData 0.000 (0.001)\tLoss 2.5690 (1.8730)\tTop 1-err 39.0625 (40.8138)\tTop 5-err 14.0625 (18.1862)\n",
            "Epoch: [190/290][699/782]\tLR: 0.014995\tTime 0.146 (0.148)\tData 0.000 (0.000)\tLoss 2.9411 (1.8647)\tTop 1-err 81.2500 (40.8906)\tTop 5-err 54.6875 (18.1496)\n",
            "* Epoch: [190/290]\t Top 1-err 41.315  Top 5-err 18.503\t Train Loss 1.879\n",
            "* Epoch: [190/290]\t Top 1-err 32.030  Top 5-err 8.670\t Test Loss 1.129\n",
            "Current best accuracy (top-1 and 5 error): 29.54 7.72\n",
            "Epoch: [191/290][99/782]\tLR: 0.015106\tTime 0.146 (0.150)\tData 0.000 (0.002)\tLoss 3.0224 (1.7495)\tTop 1-err 50.0000 (37.4531)\tTop 5-err 18.7500 (16.5312)\n",
            "Epoch: [191/290][199/782]\tLR: 0.015106\tTime 0.146 (0.149)\tData 0.000 (0.001)\tLoss 3.7905 (1.7547)\tTop 1-err 79.6875 (38.1758)\tTop 5-err 54.6875 (16.8516)\n",
            "Epoch: [191/290][299/782]\tLR: 0.015106\tTime 0.150 (0.149)\tData 0.000 (0.001)\tLoss 1.9041 (1.7485)\tTop 1-err 39.0625 (38.1927)\tTop 5-err 14.0625 (16.7500)\n",
            "Epoch: [191/290][399/782]\tLR: 0.015106\tTime 0.155 (0.149)\tData 0.000 (0.001)\tLoss 0.9199 (1.7917)\tTop 1-err 29.6875 (38.8008)\tTop 5-err 7.8125 (17.1465)\n",
            "Epoch: [191/290][499/782]\tLR: 0.015106\tTime 0.145 (0.148)\tData 0.000 (0.001)\tLoss 0.7282 (1.8003)\tTop 1-err 23.4375 (39.3953)\tTop 5-err 4.6875 (17.4922)\n",
            "Epoch: [191/290][599/782]\tLR: 0.015106\tTime 0.146 (0.148)\tData 0.000 (0.000)\tLoss 0.8404 (1.7949)\tTop 1-err 25.0000 (39.5417)\tTop 5-err 6.2500 (17.6016)\n",
            "Epoch: [191/290][699/782]\tLR: 0.015106\tTime 0.156 (0.148)\tData 0.000 (0.000)\tLoss 3.7459 (1.8010)\tTop 1-err 76.5625 (39.7277)\tTop 5-err 50.0000 (17.6596)\n",
            "* Epoch: [191/290]\t Top 1-err 39.258  Top 5-err 17.234\t Train Loss 1.778\n",
            "* Epoch: [191/290]\t Top 1-err 31.180  Top 5-err 7.900\t Test Loss 1.088\n",
            "Current best accuracy (top-1 and 5 error): 29.54 7.72\n",
            "Epoch: [192/290][99/782]\tLR: 0.015217\tTime 0.147 (0.150)\tData 0.000 (0.002)\tLoss 3.1327 (1.8118)\tTop 1-err 60.9375 (39.7812)\tTop 5-err 34.3750 (17.7734)\n",
            "Epoch: [192/290][199/782]\tLR: 0.015217\tTime 0.146 (0.149)\tData 0.000 (0.001)\tLoss 2.6216 (1.8470)\tTop 1-err 43.7500 (39.8477)\tTop 5-err 17.1875 (17.3086)\n",
            "Epoch: [192/290][299/782]\tLR: 0.015217\tTime 0.146 (0.148)\tData 0.000 (0.001)\tLoss 0.8110 (1.8142)\tTop 1-err 23.4375 (39.4661)\tTop 5-err 3.1250 (16.7552)\n",
            "Epoch: [192/290][399/782]\tLR: 0.015217\tTime 0.145 (0.148)\tData 0.000 (0.001)\tLoss 0.7050 (1.8206)\tTop 1-err 18.7500 (39.6680)\tTop 5-err 3.1250 (17.0176)\n",
            "Epoch: [192/290][499/782]\tLR: 0.015217\tTime 0.145 (0.148)\tData 0.000 (0.000)\tLoss 0.7697 (1.7899)\tTop 1-err 25.0000 (39.4391)\tTop 5-err 4.6875 (16.9609)\n",
            "Epoch: [192/290][599/782]\tLR: 0.015217\tTime 0.152 (0.148)\tData 0.000 (0.000)\tLoss 0.8175 (1.7906)\tTop 1-err 25.0000 (39.5221)\tTop 5-err 1.5625 (17.0703)\n",
            "Epoch: [192/290][699/782]\tLR: 0.015217\tTime 0.146 (0.148)\tData 0.000 (0.000)\tLoss 0.8228 (1.7898)\tTop 1-err 25.0000 (39.5792)\tTop 5-err 6.2500 (17.1786)\n",
            "* Epoch: [192/290]\t Top 1-err 39.515  Top 5-err 17.131\t Train Loss 1.782\n",
            "* Epoch: [192/290]\t Top 1-err 31.170  Top 5-err 8.000\t Test Loss 1.090\n",
            "Current best accuracy (top-1 and 5 error): 29.54 7.72\n",
            "Epoch: [193/290][99/782]\tLR: 0.015329\tTime 0.146 (0.149)\tData 0.000 (0.002)\tLoss 0.9037 (1.6739)\tTop 1-err 20.3125 (36.5859)\tTop 5-err 3.1250 (15.4922)\n",
            "Epoch: [193/290][199/782]\tLR: 0.015329\tTime 0.145 (0.148)\tData 0.000 (0.001)\tLoss 0.8569 (1.7505)\tTop 1-err 26.5625 (38.5312)\tTop 5-err 7.8125 (17.1055)\n",
            "Epoch: [193/290][299/782]\tLR: 0.015329\tTime 0.145 (0.148)\tData 0.000 (0.001)\tLoss 0.8747 (1.6957)\tTop 1-err 31.2500 (37.2630)\tTop 5-err 3.1250 (15.6719)\n",
            "Epoch: [193/290][399/782]\tLR: 0.015329\tTime 0.146 (0.148)\tData 0.000 (0.001)\tLoss 0.9300 (1.6590)\tTop 1-err 29.6875 (36.9004)\tTop 5-err 7.8125 (15.2773)\n",
            "Epoch: [193/290][499/782]\tLR: 0.015329\tTime 0.145 (0.148)\tData 0.000 (0.001)\tLoss 0.7589 (1.6896)\tTop 1-err 18.7500 (37.6453)\tTop 5-err 3.1250 (15.7203)\n",
            "Epoch: [193/290][599/782]\tLR: 0.015329\tTime 0.151 (0.148)\tData 0.000 (0.000)\tLoss 0.9574 (1.7198)\tTop 1-err 25.0000 (38.4440)\tTop 5-err 6.2500 (16.4727)\n",
            "Epoch: [193/290][699/782]\tLR: 0.015329\tTime 0.152 (0.148)\tData 0.000 (0.000)\tLoss 0.8207 (1.7361)\tTop 1-err 23.4375 (38.8516)\tTop 5-err 4.6875 (16.7176)\n",
            "* Epoch: [193/290]\t Top 1-err 38.935  Top 5-err 16.702\t Train Loss 1.741\n",
            "* Epoch: [193/290]\t Top 1-err 30.310  Top 5-err 7.760\t Test Loss 1.051\n",
            "Current best accuracy (top-1 and 5 error): 29.54 7.72\n",
            "Epoch: [194/290][99/782]\tLR: 0.015441\tTime 0.145 (0.151)\tData 0.000 (0.002)\tLoss 1.0871 (1.9510)\tTop 1-err 31.2500 (41.8828)\tTop 5-err 6.2500 (18.7656)\n",
            "Epoch: [194/290][199/782]\tLR: 0.015441\tTime 0.148 (0.149)\tData 0.000 (0.001)\tLoss 4.0434 (1.9013)\tTop 1-err 79.6875 (41.2305)\tTop 5-err 64.0625 (18.2383)\n",
            "Epoch: [194/290][299/782]\tLR: 0.015441\tTime 0.145 (0.149)\tData 0.000 (0.001)\tLoss 0.7381 (1.8839)\tTop 1-err 23.4375 (41.3151)\tTop 5-err 3.1250 (18.5339)\n",
            "Epoch: [194/290][399/782]\tLR: 0.015441\tTime 0.147 (0.149)\tData 0.000 (0.001)\tLoss 2.9580 (1.8528)\tTop 1-err 62.5000 (40.8105)\tTop 5-err 26.5625 (17.9473)\n",
            "Epoch: [194/290][499/782]\tLR: 0.015441\tTime 0.145 (0.148)\tData 0.000 (0.001)\tLoss 0.7942 (1.8499)\tTop 1-err 20.3125 (40.7062)\tTop 5-err 6.2500 (18.0328)\n",
            "Epoch: [194/290][599/782]\tLR: 0.015441\tTime 0.146 (0.148)\tData 0.000 (0.000)\tLoss 0.9026 (1.8602)\tTop 1-err 26.5625 (41.0299)\tTop 5-err 4.6875 (18.3411)\n",
            "Epoch: [194/290][699/782]\tLR: 0.015441\tTime 0.145 (0.148)\tData 0.000 (0.000)\tLoss 3.8781 (1.8751)\tTop 1-err 89.0625 (41.0826)\tTop 5-err 81.2500 (18.3080)\n",
            "* Epoch: [194/290]\t Top 1-err 40.716  Top 5-err 17.969\t Train Loss 1.853\n",
            "* Epoch: [194/290]\t Top 1-err 30.260  Top 5-err 8.010\t Test Loss 1.062\n",
            "Current best accuracy (top-1 and 5 error): 29.54 7.72\n",
            "Epoch: [195/290][99/782]\tLR: 0.015554\tTime 0.154 (0.149)\tData 0.000 (0.002)\tLoss 2.7012 (1.7616)\tTop 1-err 39.0625 (39.1094)\tTop 5-err 4.6875 (16.6562)\n",
            "Epoch: [195/290][199/782]\tLR: 0.015554\tTime 0.147 (0.149)\tData 0.000 (0.001)\tLoss 3.3706 (1.7003)\tTop 1-err 70.3125 (37.7812)\tTop 5-err 37.5000 (15.5625)\n",
            "Epoch: [195/290][299/782]\tLR: 0.015554\tTime 0.150 (0.148)\tData 0.000 (0.001)\tLoss 0.9569 (1.7039)\tTop 1-err 25.0000 (37.7891)\tTop 5-err 10.9375 (15.4271)\n",
            "Epoch: [195/290][399/782]\tLR: 0.015554\tTime 0.150 (0.148)\tData 0.000 (0.001)\tLoss 0.8652 (1.6642)\tTop 1-err 25.0000 (37.3086)\tTop 5-err 1.5625 (15.2422)\n",
            "Epoch: [195/290][499/782]\tLR: 0.015554\tTime 0.145 (0.148)\tData 0.000 (0.001)\tLoss 0.6973 (1.6753)\tTop 1-err 21.8750 (37.5656)\tTop 5-err 3.1250 (15.3859)\n",
            "Epoch: [195/290][599/782]\tLR: 0.015554\tTime 0.148 (0.148)\tData 0.000 (0.000)\tLoss 2.8977 (1.6970)\tTop 1-err 70.3125 (37.9544)\tTop 5-err 28.1250 (15.5990)\n",
            "Epoch: [195/290][699/782]\tLR: 0.015554\tTime 0.146 (0.148)\tData 0.000 (0.000)\tLoss 2.2331 (1.6950)\tTop 1-err 32.8125 (37.8750)\tTop 5-err 9.3750 (15.5714)\n",
            "* Epoch: [195/290]\t Top 1-err 38.024  Top 5-err 15.694\t Train Loss 1.705\n",
            "* Epoch: [195/290]\t Top 1-err 31.220  Top 5-err 8.380\t Test Loss 1.092\n",
            "Current best accuracy (top-1 and 5 error): 29.54 7.72\n",
            "Epoch: [196/290][99/782]\tLR: 0.015667\tTime 0.151 (0.150)\tData 0.000 (0.002)\tLoss 2.2617 (1.9169)\tTop 1-err 35.9375 (39.9688)\tTop 5-err 10.9375 (17.1719)\n",
            "Epoch: [196/290][199/782]\tLR: 0.015667\tTime 0.148 (0.149)\tData 0.000 (0.001)\tLoss 4.2820 (1.8147)\tTop 1-err 95.3125 (39.1250)\tTop 5-err 71.8750 (17.3906)\n",
            "Epoch: [196/290][299/782]\tLR: 0.015667\tTime 0.147 (0.149)\tData 0.000 (0.001)\tLoss 3.6216 (1.8284)\tTop 1-err 59.3750 (39.7839)\tTop 5-err 37.5000 (17.4427)\n",
            "Epoch: [196/290][399/782]\tLR: 0.015667\tTime 0.147 (0.149)\tData 0.000 (0.001)\tLoss 0.7901 (1.8425)\tTop 1-err 20.3125 (40.1758)\tTop 5-err 1.5625 (17.9395)\n",
            "Epoch: [196/290][499/782]\tLR: 0.015667\tTime 0.146 (0.148)\tData 0.000 (0.000)\tLoss 1.1923 (1.8227)\tTop 1-err 26.5625 (40.1297)\tTop 5-err 3.1250 (17.8484)\n",
            "Epoch: [196/290][599/782]\tLR: 0.015667\tTime 0.148 (0.148)\tData 0.000 (0.000)\tLoss 0.6958 (1.8164)\tTop 1-err 21.8750 (39.8411)\tTop 5-err 4.6875 (17.5768)\n",
            "Epoch: [196/290][699/782]\tLR: 0.015667\tTime 0.148 (0.148)\tData 0.000 (0.000)\tLoss 2.9090 (1.8550)\tTop 1-err 45.3125 (40.5234)\tTop 5-err 15.6250 (17.9509)\n",
            "* Epoch: [196/290]\t Top 1-err 40.472  Top 5-err 17.961\t Train Loss 1.852\n",
            "* Epoch: [196/290]\t Top 1-err 32.230  Top 5-err 8.780\t Test Loss 1.154\n",
            "Current best accuracy (top-1 and 5 error): 29.54 7.72\n",
            "Epoch: [197/290][99/782]\tLR: 0.015781\tTime 0.146 (0.150)\tData 0.000 (0.003)\tLoss 3.0476 (1.8091)\tTop 1-err 82.8125 (40.7109)\tTop 5-err 53.1250 (18.6797)\n",
            "Epoch: [197/290][199/782]\tLR: 0.015781\tTime 0.144 (0.149)\tData 0.000 (0.001)\tLoss 0.7075 (1.7162)\tTop 1-err 18.7500 (38.5977)\tTop 5-err 1.5625 (16.8320)\n",
            "Epoch: [197/290][299/782]\tLR: 0.015781\tTime 0.152 (0.148)\tData 0.000 (0.001)\tLoss 2.6787 (1.7357)\tTop 1-err 43.7500 (38.6562)\tTop 5-err 15.6250 (16.6146)\n",
            "Epoch: [197/290][399/782]\tLR: 0.015781\tTime 0.146 (0.148)\tData 0.000 (0.001)\tLoss 0.8621 (1.7622)\tTop 1-err 23.4375 (39.1094)\tTop 5-err 3.1250 (16.7930)\n",
            "Epoch: [197/290][499/782]\tLR: 0.015781\tTime 0.149 (0.148)\tData 0.000 (0.001)\tLoss 3.1364 (1.7678)\tTop 1-err 62.5000 (39.2922)\tTop 5-err 29.6875 (16.9234)\n",
            "Epoch: [197/290][599/782]\tLR: 0.015781\tTime 0.152 (0.148)\tData 0.000 (0.001)\tLoss 3.3398 (1.8073)\tTop 1-err 59.3750 (40.0599)\tTop 5-err 31.2500 (17.5911)\n",
            "Epoch: [197/290][699/782]\tLR: 0.015781\tTime 0.146 (0.148)\tData 0.000 (0.000)\tLoss 3.1370 (1.7957)\tTop 1-err 64.0625 (39.7600)\tTop 5-err 28.1250 (17.2946)\n",
            "* Epoch: [197/290]\t Top 1-err 40.191  Top 5-err 17.644\t Train Loss 1.811\n",
            "* Epoch: [197/290]\t Top 1-err 30.990  Top 5-err 7.800\t Test Loss 1.078\n",
            "Current best accuracy (top-1 and 5 error): 29.54 7.72\n",
            "Epoch: [198/290][99/782]\tLR: 0.015895\tTime 0.147 (0.150)\tData 0.000 (0.002)\tLoss 4.8529 (2.0658)\tTop 1-err 90.6250 (44.4922)\tTop 5-err 84.3750 (20.5781)\n",
            "Epoch: [198/290][199/782]\tLR: 0.015895\tTime 0.149 (0.150)\tData 0.000 (0.001)\tLoss 4.2776 (1.9546)\tTop 1-err 90.6250 (42.1602)\tTop 5-err 73.4375 (19.2383)\n",
            "Epoch: [198/290][299/782]\tLR: 0.015895\tTime 0.150 (0.149)\tData 0.000 (0.001)\tLoss 3.9345 (1.8933)\tTop 1-err 90.6250 (41.2656)\tTop 5-err 71.8750 (18.7552)\n",
            "Epoch: [198/290][399/782]\tLR: 0.015895\tTime 0.145 (0.148)\tData 0.000 (0.001)\tLoss 4.1013 (1.8720)\tTop 1-err 85.9375 (41.3867)\tTop 5-err 75.0000 (18.6719)\n",
            "Epoch: [198/290][499/782]\tLR: 0.015895\tTime 0.146 (0.148)\tData 0.000 (0.000)\tLoss 2.9870 (1.8687)\tTop 1-err 57.8125 (41.2281)\tTop 5-err 35.9375 (18.6531)\n",
            "Epoch: [198/290][599/782]\tLR: 0.015895\tTime 0.145 (0.148)\tData 0.000 (0.000)\tLoss 0.8922 (1.8525)\tTop 1-err 21.8750 (40.9010)\tTop 5-err 6.2500 (18.2018)\n",
            "Epoch: [198/290][699/782]\tLR: 0.015895\tTime 0.150 (0.148)\tData 0.000 (0.000)\tLoss 3.5258 (1.8632)\tTop 1-err 92.1875 (41.1083)\tTop 5-err 75.0000 (18.5045)\n",
            "* Epoch: [198/290]\t Top 1-err 40.917  Top 5-err 18.305\t Train Loss 1.848\n",
            "* Epoch: [198/290]\t Top 1-err 32.300  Top 5-err 9.100\t Test Loss 1.133\n",
            "Current best accuracy (top-1 and 5 error): 29.54 7.72\n",
            "Epoch: [199/290][99/782]\tLR: 0.016010\tTime 0.144 (0.150)\tData 0.000 (0.002)\tLoss 0.7143 (1.8923)\tTop 1-err 20.3125 (42.4375)\tTop 5-err 4.6875 (19.8828)\n",
            "Epoch: [199/290][199/782]\tLR: 0.016010\tTime 0.146 (0.149)\tData 0.000 (0.001)\tLoss 0.6829 (1.9343)\tTop 1-err 18.7500 (43.0156)\tTop 5-err 1.5625 (20.5938)\n",
            "Epoch: [199/290][299/782]\tLR: 0.016010\tTime 0.149 (0.149)\tData 0.000 (0.001)\tLoss 0.7482 (1.9159)\tTop 1-err 21.8750 (42.7656)\tTop 5-err 1.5625 (20.2552)\n",
            "Epoch: [199/290][399/782]\tLR: 0.016010\tTime 0.145 (0.148)\tData 0.000 (0.001)\tLoss 0.7317 (1.8881)\tTop 1-err 21.8750 (41.8066)\tTop 5-err 4.6875 (19.3496)\n",
            "Epoch: [199/290][499/782]\tLR: 0.016010\tTime 0.145 (0.148)\tData 0.000 (0.001)\tLoss 1.0577 (1.9308)\tTop 1-err 26.5625 (42.2969)\tTop 5-err 6.2500 (19.8844)\n",
            "Epoch: [199/290][599/782]\tLR: 0.016010\tTime 0.151 (0.148)\tData 0.000 (0.000)\tLoss 3.0369 (1.9196)\tTop 1-err 68.7500 (42.0625)\tTop 5-err 42.1875 (19.6875)\n",
            "Epoch: [199/290][699/782]\tLR: 0.016010\tTime 0.145 (0.148)\tData 0.000 (0.000)\tLoss 3.0091 (1.8992)\tTop 1-err 48.4375 (41.7176)\tTop 5-err 31.2500 (19.4230)\n",
            "* Epoch: [199/290]\t Top 1-err 41.743  Top 5-err 19.343\t Train Loss 1.905\n",
            "* Epoch: [199/290]\t Top 1-err 31.460  Top 5-err 8.380\t Test Loss 1.101\n",
            "Current best accuracy (top-1 and 5 error): 29.54 7.72\n",
            "Epoch: [200/290][99/782]\tLR: 0.016125\tTime 0.147 (0.150)\tData 0.000 (0.002)\tLoss 0.8107 (1.6690)\tTop 1-err 29.6875 (37.4062)\tTop 5-err 1.5625 (14.9766)\n",
            "Epoch: [200/290][199/782]\tLR: 0.016125\tTime 0.150 (0.149)\tData 0.000 (0.001)\tLoss 2.8412 (1.7942)\tTop 1-err 42.1875 (38.4727)\tTop 5-err 10.9375 (16.0391)\n",
            "Epoch: [200/290][299/782]\tLR: 0.016125\tTime 0.144 (0.149)\tData 0.000 (0.001)\tLoss 0.7392 (1.7706)\tTop 1-err 25.0000 (38.2760)\tTop 5-err 4.6875 (15.8177)\n",
            "Epoch: [200/290][399/782]\tLR: 0.016125\tTime 0.146 (0.148)\tData 0.000 (0.001)\tLoss 3.0992 (1.7896)\tTop 1-err 54.6875 (38.9238)\tTop 5-err 31.2500 (16.4570)\n",
            "Epoch: [200/290][499/782]\tLR: 0.016125\tTime 0.149 (0.148)\tData 0.000 (0.000)\tLoss 0.8828 (1.7951)\tTop 1-err 23.4375 (39.3422)\tTop 5-err 3.1250 (16.8375)\n",
            "Epoch: [200/290][599/782]\tLR: 0.016125\tTime 0.156 (0.148)\tData 0.000 (0.000)\tLoss 0.7169 (1.7818)\tTop 1-err 20.3125 (39.2812)\tTop 5-err 3.1250 (16.7773)\n",
            "Epoch: [200/290][699/782]\tLR: 0.016125\tTime 0.146 (0.148)\tData 0.000 (0.000)\tLoss 3.8580 (1.7913)\tTop 1-err 82.8125 (39.5179)\tTop 5-err 53.1250 (16.9208)\n",
            "* Epoch: [200/290]\t Top 1-err 39.627  Top 5-err 17.119\t Train Loss 1.795\n",
            "* Epoch: [200/290]\t Top 1-err 30.360  Top 5-err 8.010\t Test Loss 1.069\n",
            "Current best accuracy (top-1 and 5 error): 29.54 7.72\n",
            "Epoch: [201/290][99/782]\tLR: 0.016240\tTime 0.145 (0.150)\tData 0.000 (0.002)\tLoss 1.0983 (1.9786)\tTop 1-err 28.1250 (43.8516)\tTop 5-err 7.8125 (21.4922)\n",
            "Epoch: [201/290][199/782]\tLR: 0.016240\tTime 0.146 (0.149)\tData 0.000 (0.001)\tLoss 3.4714 (1.8468)\tTop 1-err 56.2500 (40.8633)\tTop 5-err 32.8125 (18.8555)\n",
            "Epoch: [201/290][299/782]\tLR: 0.016240\tTime 0.152 (0.148)\tData 0.000 (0.001)\tLoss 3.9838 (1.8159)\tTop 1-err 93.7500 (40.6068)\tTop 5-err 71.8750 (18.4010)\n",
            "Epoch: [201/290][399/782]\tLR: 0.016240\tTime 0.144 (0.148)\tData 0.000 (0.001)\tLoss 0.7716 (1.8062)\tTop 1-err 21.8750 (40.4492)\tTop 5-err 3.1250 (18.0977)\n",
            "Epoch: [201/290][499/782]\tLR: 0.016240\tTime 0.149 (0.148)\tData 0.000 (0.001)\tLoss 0.7697 (1.8039)\tTop 1-err 25.0000 (40.3281)\tTop 5-err 4.6875 (18.0656)\n",
            "Epoch: [201/290][599/782]\tLR: 0.016240\tTime 0.145 (0.148)\tData 0.000 (0.001)\tLoss 0.9694 (1.7861)\tTop 1-err 37.5000 (39.9466)\tTop 5-err 3.1250 (17.8320)\n",
            "Epoch: [201/290][699/782]\tLR: 0.016240\tTime 0.147 (0.148)\tData 0.000 (0.000)\tLoss 0.7470 (1.7712)\tTop 1-err 25.0000 (39.6897)\tTop 5-err 3.1250 (17.5491)\n",
            "* Epoch: [201/290]\t Top 1-err 40.056  Top 5-err 17.783\t Train Loss 1.789\n",
            "* Epoch: [201/290]\t Top 1-err 32.660  Top 5-err 9.080\t Test Loss 1.168\n",
            "Current best accuracy (top-1 and 5 error): 29.54 7.72\n",
            "Epoch: [202/290][99/782]\tLR: 0.016356\tTime 0.146 (0.151)\tData 0.000 (0.003)\tLoss 0.8929 (1.6705)\tTop 1-err 28.1250 (37.2109)\tTop 5-err 7.8125 (14.9609)\n",
            "Epoch: [202/290][199/782]\tLR: 0.016356\tTime 0.145 (0.150)\tData 0.000 (0.001)\tLoss 0.7436 (1.6362)\tTop 1-err 18.7500 (37.0352)\tTop 5-err 0.0000 (15.4141)\n",
            "Epoch: [202/290][299/782]\tLR: 0.016356\tTime 0.146 (0.149)\tData 0.000 (0.001)\tLoss 0.9257 (1.7008)\tTop 1-err 32.8125 (38.4297)\tTop 5-err 3.1250 (16.3307)\n",
            "Epoch: [202/290][399/782]\tLR: 0.016356\tTime 0.154 (0.149)\tData 0.000 (0.001)\tLoss 0.9904 (1.7243)\tTop 1-err 25.0000 (38.7305)\tTop 5-err 6.2500 (16.6172)\n",
            "Epoch: [202/290][499/782]\tLR: 0.016356\tTime 0.145 (0.148)\tData 0.000 (0.001)\tLoss 0.9023 (1.7227)\tTop 1-err 31.2500 (38.6719)\tTop 5-err 3.1250 (16.6156)\n",
            "Epoch: [202/290][599/782]\tLR: 0.016356\tTime 0.145 (0.148)\tData 0.000 (0.001)\tLoss 1.0266 (1.7300)\tTop 1-err 28.1250 (38.9544)\tTop 5-err 6.2500 (16.9453)\n",
            "Epoch: [202/290][699/782]\tLR: 0.016356\tTime 0.154 (0.148)\tData 0.000 (0.000)\tLoss 3.1799 (1.7386)\tTop 1-err 53.1250 (39.0692)\tTop 5-err 20.3125 (17.0234)\n",
            "* Epoch: [202/290]\t Top 1-err 39.331  Top 5-err 17.231\t Train Loss 1.748\n",
            "* Epoch: [202/290]\t Top 1-err 31.510  Top 5-err 8.750\t Test Loss 1.118\n",
            "Current best accuracy (top-1 and 5 error): 29.54 7.72\n",
            "Epoch: [203/290][99/782]\tLR: 0.016473\tTime 0.150 (0.151)\tData 0.000 (0.002)\tLoss 1.0044 (1.8660)\tTop 1-err 31.2500 (39.1797)\tTop 5-err 7.8125 (17.6797)\n",
            "Epoch: [203/290][199/782]\tLR: 0.016473\tTime 0.147 (0.149)\tData 0.000 (0.001)\tLoss 3.8609 (1.9499)\tTop 1-err 89.0625 (40.8320)\tTop 5-err 65.6250 (18.0977)\n",
            "Epoch: [203/290][299/782]\tLR: 0.016473\tTime 0.150 (0.148)\tData 0.000 (0.001)\tLoss 2.4428 (1.9128)\tTop 1-err 40.6250 (40.2865)\tTop 5-err 20.3125 (17.9714)\n",
            "Epoch: [203/290][399/782]\tLR: 0.016473\tTime 0.153 (0.148)\tData 0.000 (0.001)\tLoss 2.7716 (1.8991)\tTop 1-err 48.4375 (40.6602)\tTop 5-err 21.8750 (18.3574)\n",
            "Epoch: [203/290][499/782]\tLR: 0.016473\tTime 0.145 (0.148)\tData 0.000 (0.001)\tLoss 0.8283 (1.8832)\tTop 1-err 23.4375 (40.5828)\tTop 5-err 6.2500 (18.2922)\n",
            "Epoch: [203/290][599/782]\tLR: 0.016473\tTime 0.146 (0.148)\tData 0.000 (0.000)\tLoss 2.5359 (1.8589)\tTop 1-err 60.9375 (40.1289)\tTop 5-err 32.8125 (17.7526)\n",
            "Epoch: [203/290][699/782]\tLR: 0.016473\tTime 0.146 (0.148)\tData 0.000 (0.000)\tLoss 1.8714 (1.8723)\tTop 1-err 37.5000 (40.3739)\tTop 5-err 12.5000 (17.9598)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yiIMf621ZXAw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovnnkG69ZRqk"
      },
      "source": [
        "### 기존"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "oI4JSkVb3rPF",
        "outputId": "0d1cac87-335c-488e-8cbd-11f42e71d260"
      },
      "source": [
        "#parser.set_defaults(bottleneck=True)\n",
        "#parser.set_defaults(verbose=True)\n",
        "\n",
        "best_err1 = 100\n",
        "best_err5 = 100\n",
        "\n",
        "\n",
        "def main():\n",
        "    global args, best_err1, best_err5\n",
        "    \n",
        "\n",
        "    if args.dataset.startswith('cifar'):\n",
        "        normalize = transforms.Normalize(mean=[x / 255.0 for x in [125.3, 123.0, 113.9]],\n",
        "                                         std=[x / 255.0 for x in [63.0, 62.1, 66.7]])\n",
        "\n",
        "        transform_train = transforms.Compose([\n",
        "            transforms.RandomCrop(32, padding=4),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.ToTensor(),\n",
        "            normalize,\n",
        "        ])\n",
        "\n",
        "        transform_test = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            normalize\n",
        "        ])\n",
        "\n",
        "        if args.dataset == 'cifar100':\n",
        "            train_loader = torch.utils.data.DataLoader(\n",
        "                datasets.CIFAR100('../data', train=True, download=True, transform=transform_train),\n",
        "                batch_size=args.batch_size, shuffle=True, num_workers=args.workers, pin_memory=True)\n",
        "            val_loader = torch.utils.data.DataLoader(\n",
        "                datasets.CIFAR100('../data', train=False, transform=transform_test),\n",
        "                batch_size=args.batch_size, shuffle=True, num_workers=args.workers, pin_memory=True)\n",
        "            numberofclass = 100\n",
        "        elif args.dataset == 'cifar10':\n",
        "            train_loader = torch.utils.data.DataLoader(\n",
        "                datasets.CIFAR10('../data', train=True, download=True, transform=transform_train),\n",
        "                batch_size=args.batch_size, shuffle=True, num_workers=args.workers, pin_memory=True)\n",
        "            val_loader = torch.utils.data.DataLoader(\n",
        "                datasets.CIFAR10('../data', train=False, transform=transform_test),\n",
        "                batch_size=args.batch_size, shuffle=True, num_workers=args.workers, pin_memory=True)\n",
        "            numberofclass = 10\n",
        "        else:\n",
        "            raise Exception('unknown dataset: {}'.format(args.dataset))\n",
        "\n",
        "    else:\n",
        "        raise Exception('unknown dataset: {}'.format(args.dataset))\n",
        "\n",
        "    print(\"=> creating model '{}'\".format(args.net_type))\n",
        "    if args.net_type == 'resnet':\n",
        "        model = bsconv.pytorch.get_model('cifar_wrn28_4_bsconvs_p1d8', num_classes=100)\n",
        "    else:\n",
        "        raise Exception('unknown network architecture: {}'.format(args.net_type))\n",
        "\n",
        "    model = torch.nn.DataParallel(model).cuda()\n",
        "\n",
        "    print(model)\n",
        "    print('the number of model parameters: {}'.format(sum([p.data.nelement() for p in model.parameters()])))\n",
        "\n",
        "    # define loss function (criterion) and optimizer\n",
        "    criterion = nn.CrossEntropyLoss().cuda()\n",
        "\n",
        "    optimizer = SAMSGD(model.parameters(), lr=args.lr, momentum=args.momentum,\n",
        "                       weight_decay=args.weight_decay, nesterov=True, rho=0.05)\n",
        "\n",
        "#          optimizer = torch.optim.SGD(model.parameters(), args.lr,\n",
        "#                                      momentum=args.momentum,\n",
        "#                                      weight_decay=args.weight_decay, nesterov=True)\n",
        "\n",
        "    cudnn.benchmark = True\n",
        "    if os.path.exists(f'/content/drive/MyDrive/ajoudeep/contest/bsconv_sam/checkpoint.pth.tar'):\n",
        "        checkpoint = torch.load(f'/content/drive/MyDrive/ajoudeep/contest/bsconv_sam/checkpoint.pth.tar')\n",
        "        model.load_state_dict(checkpoint['state_dict'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "        start_epoch = checkpoint['epoch']\n",
        "        best_err1 = checkpoint['best_err1']\n",
        "        best_err5 = checkpoint['best_err5']\n",
        "        \n",
        "    else:\n",
        "        start_epoch = 0\n",
        "\n",
        "\n",
        "    for epoch in range(start_epoch, args.epochs):\n",
        "\n",
        "        adjust_learning_rate(optimizer, epoch)\n",
        "\n",
        "        # train for one epoch\n",
        "        train_loss = train(train_loader, model, criterion, optimizer, epoch)\n",
        "\n",
        "        # evaluate on validation set\n",
        "        err1, err5, val_loss = validate(val_loader, model, criterion, epoch)\n",
        "\n",
        "        # remember best prec@1 and save checkpoint\n",
        "        is_best = err1 <= best_err1\n",
        "        best_err1 = min(err1, best_err1)\n",
        "        if is_best:\n",
        "            best_err5 = err5\n",
        "\n",
        "        print('Current best accuracy (top-1 and 5 error):', best_err1, best_err5)\n",
        "        save_checkpoint({\n",
        "            'epoch': epoch,\n",
        "            'arch': args.net_type,\n",
        "            'state_dict': model.state_dict(),\n",
        "            'best_err1': best_err1,\n",
        "            'best_err5': best_err5,\n",
        "            'optimizer': optimizer.state_dict(),\n",
        "        }, is_best)\n",
        "\n",
        "    print('Best accuracy (top-1 and 5 error):', best_err1, best_err5)\n",
        "\n",
        "\n",
        "def train(train_loader, model, criterion, optimizer, epoch):\n",
        "    batch_time = AverageMeter()\n",
        "    data_time = AverageMeter()\n",
        "    losses = AverageMeter()\n",
        "    top1 = AverageMeter()\n",
        "    top5 = AverageMeter()\n",
        "\n",
        "    # switch to train mode\n",
        "    model.train()\n",
        "\n",
        "    end = time.time()\n",
        "    current_LR = get_learning_rate(optimizer)[0]\n",
        "    for i, (input, target) in enumerate(train_loader):\n",
        "        # measure data loading time\n",
        "        data_time.update(time.time() - end)\n",
        "\n",
        "        input = input.cuda()\n",
        "        target = target.cuda()\n",
        "\n",
        "        r = np.random.rand(1)\n",
        "        \n",
        "        def closure():\n",
        "            optimizer.zero_grad()\n",
        "            if args.beta > 0 and r < args.cutmix_prob:\n",
        "            # generate mixed sample\n",
        "                lam = np.random.beta(args.beta, args.beta)\n",
        "                rand_index = torch.randperm(input.size()[0]).cuda()\n",
        "                target_a = target\n",
        "                target_b = target[rand_index]\n",
        "                bbx1, bby1, bbx2, bby2 = rand_bbox(input.size(), lam)\n",
        "                input[:, :, bbx1:bbx2, bby1:bby2] = input[rand_index, :, bbx1:bbx2, bby1:bby2]\n",
        "            # adjust lambda to exactly match pixel ratio\n",
        "                lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (input.size()[-1] * input.size()[-2]))\n",
        "            # compute output\n",
        "                output = model(input)\n",
        "                loss = criterion(output, target_a) * lam + criterion(output, target_b) * (1. - lam)\n",
        "            else:\n",
        "            # compute output\n",
        "                output = model(input)\n",
        "                loss = criterion(output, target)\n",
        "\n",
        "                    # measure accuracy and record loss\n",
        "            err1, err5 = accuracy(output.data, target, topk=(1, 5))\n",
        "\n",
        "            losses.update(loss.item(), input.size(0))\n",
        "            top1.update(err1.item(), input.size(0))\n",
        "            top5.update(err5.item(), input.size(0))\n",
        "            \n",
        "            loss.backward()\n",
        "            return loss\n",
        "\n",
        "\n",
        "\n",
        "        # compute gradient and do SGD step\n",
        "        #optimizer.zero_grad()\n",
        "        #loss.backward()\n",
        "        #optimizer.step()\n",
        "\n",
        "        loss = optimizer.step(closure)\n",
        "\n",
        "\n",
        "        # measure elapsed time\n",
        "        batch_time.update(time.time() - end)\n",
        "        end = time.time()\n",
        "\n",
        "        show_period = 100\n",
        "\n",
        "        if i % show_period == show_period-1:        \n",
        "#        if i % args.print_freq == 0 and args.verbose == True:\n",
        "            print('Epoch: [{0}/{1}][{2}/{3}]\\t'\n",
        "                  'LR: {LR:.6f}\\t'\n",
        "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
        "                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
        "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
        "                  'Top 1-err {top1.val:.4f} ({top1.avg:.4f})\\t'\n",
        "                  'Top 5-err {top5.val:.4f} ({top5.avg:.4f})'.format(\n",
        "                epoch, args.epochs, i, len(train_loader), LR=current_LR, batch_time=batch_time,\n",
        "                data_time=data_time, loss=losses, top1=top1, top5=top5))\n",
        "\n",
        "    print('* Epoch: [{0}/{1}]\\t Top 1-err {top1.avg:.3f}  Top 5-err {top5.avg:.3f}\\t Train Loss {loss.avg:.3f}'.format(\n",
        "        epoch, args.epochs, top1=top1, top5=top5, loss=losses))\n",
        "\n",
        "    return losses.avg\n",
        "\n",
        "\n",
        "def rand_bbox(size, lam):\n",
        "    W = size[2]\n",
        "    H = size[3]\n",
        "    cut_rat = np.sqrt(1. - lam)\n",
        "    cut_w = np.int(W * cut_rat)\n",
        "    cut_h = np.int(H * cut_rat)\n",
        "\n",
        "    # uniform\n",
        "    cx = np.random.randint(W)\n",
        "    cy = np.random.randint(H)\n",
        "\n",
        "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
        "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
        "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
        "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
        "\n",
        "    return bbx1, bby1, bbx2, bby2\n",
        "\n",
        "\n",
        "def validate(val_loader, model, criterion, epoch):\n",
        "    batch_time = AverageMeter()\n",
        "    losses = AverageMeter()\n",
        "    top1 = AverageMeter()\n",
        "    top5 = AverageMeter()\n",
        "\n",
        "    # switch to evaluate mode\n",
        "    model.eval()\n",
        "\n",
        "    end = time.time()\n",
        "    for i, (input, target) in enumerate(val_loader):\n",
        "        target = target.cuda()\n",
        "\n",
        "        output = model(input)\n",
        "        loss = criterion(output, target)\n",
        "\n",
        "        # measure accuracy and record loss\n",
        "        err1, err5 = accuracy(output.data, target, topk=(1, 5))\n",
        "\n",
        "        losses.update(loss.item(), input.size(0))\n",
        "\n",
        "        top1.update(err1.item(), input.size(0))\n",
        "        top5.update(err5.item(), input.size(0))\n",
        "\n",
        "        # measure elapsed time\n",
        "        batch_time.update(time.time() - end)\n",
        "        end = time.time()\n",
        "\n",
        "    print('* Epoch: [{0}/{1}]\\t Top 1-err {top1.avg:.3f}  Top 5-err {top5.avg:.3f}\\t Test Loss {loss.avg:.3f}'.format(\n",
        "        epoch, args.epochs, top1=top1, top5=top5, loss=losses))\n",
        "    return top1.avg, top5.avg, losses.avg\n",
        "\n",
        "\n",
        "def save_checkpoint(state, is_best, filename='checkpoint.pth.tar'):\n",
        "    directory = os.path.join(\"/content/drive/MyDrive/ajoudeep/contest/\",\"bsconv_sam/\")\n",
        "    if not os.path.exists(directory):\n",
        "        os.makedirs(directory)\n",
        "    filename = os.path.join(directory, filename)\n",
        "    torch.save(state, filename)\n",
        "    if is_best:\n",
        "        shutil.copyfile(filename, os.path.join('/content/drive/MyDrive/ajoudeep/contest/bsconv_sam/','model_best.pth.tar'))\n",
        "\n",
        "\n",
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "\n",
        "def adjust_learning_rate(optimizer, epoch):\n",
        "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\"\n",
        "    lr = args.lr * (0.1 ** (epoch // (args.epochs * 0.3))) * (0.1 ** (epoch // (args.epochs * 0.75)))\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "\n",
        "def get_learning_rate(optimizer):\n",
        "    lr = []\n",
        "    for param_group in optimizer.param_groups:\n",
        "        lr += [param_group['lr']]\n",
        "    return lr\n",
        "\n",
        "def accuracy(output, target, topk=(1,)):\n",
        "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
        "    maxk = max(topk)\n",
        "    batch_size = target.size(0)\n",
        "\n",
        "    _, pred = output.topk(maxk, 1, True, True)\n",
        "    pred = pred.t()\n",
        "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "\n",
        "    res = []\n",
        "    for k in topk:\n",
        "        correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
        "        wrong_k = batch_size - correct_k\n",
        "        res.append(wrong_k.mul_(100.0 / batch_size))\n",
        "\n",
        "    return res\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "=> creating model 'resnet'\n",
            "DataParallel(\n",
            "  (module): ResNet(\n",
            "    (backbone): Sequential(\n",
            "      (data_bn): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (init_unit): InitUnitSmall(\n",
            "        (conv): ConvBlock(\n",
            "          (conv): BSConvS(\n",
            "            (pw1): Conv2d(3, 3, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (bn1): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (pw2): Conv2d(3, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (dw): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (stage1): Sequential(\n",
            "        (unit1): PreactUnit(\n",
            "          (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "          (conv1): ConvBlock(\n",
            "            (conv): BSConvS(\n",
            "              (pw1): Conv2d(16, 4, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (pw2): Conv2d(4, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (dw): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
            "            )\n",
            "            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (activation): ReLU(inplace=True)\n",
            "          )\n",
            "          (conv2): ConvBlock(\n",
            "            (conv): BSConvS(\n",
            "              (pw1): Conv2d(64, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (pw2): Conv2d(8, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (dw): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
            "            )\n",
            "          )\n",
            "          (projection): ConvBlock(\n",
            "            (conv): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          )\n",
            "        )\n",
            "        (unit2): PreactUnit(\n",
            "          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "          (conv1): ConvBlock(\n",
            "            (conv): BSConvS(\n",
            "              (pw1): Conv2d(64, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (pw2): Conv2d(8, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (dw): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
            "            )\n",
            "            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (activation): ReLU(inplace=True)\n",
            "          )\n",
            "          (conv2): ConvBlock(\n",
            "            (conv): BSConvS(\n",
            "              (pw1): Conv2d(64, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (pw2): Conv2d(8, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (dw): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (unit3): PreactUnit(\n",
            "          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "          (conv1): ConvBlock(\n",
            "            (conv): BSConvS(\n",
            "              (pw1): Conv2d(64, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (pw2): Conv2d(8, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (dw): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
            "            )\n",
            "            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (activation): ReLU(inplace=True)\n",
            "          )\n",
            "          (conv2): ConvBlock(\n",
            "            (conv): BSConvS(\n",
            "              (pw1): Conv2d(64, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (pw2): Conv2d(8, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (dw): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (unit4): PreactUnit(\n",
            "          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "          (conv1): ConvBlock(\n",
            "            (conv): BSConvS(\n",
            "              (pw1): Conv2d(64, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (pw2): Conv2d(8, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (dw): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
            "            )\n",
            "            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (activation): ReLU(inplace=True)\n",
            "          )\n",
            "          (conv2): ConvBlock(\n",
            "            (conv): BSConvS(\n",
            "              (pw1): Conv2d(64, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (pw2): Conv2d(8, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (dw): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (stage2): Sequential(\n",
            "        (unit1): PreactUnit(\n",
            "          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "          (conv1): ConvBlock(\n",
            "            (conv): BSConvS(\n",
            "              (pw1): Conv2d(64, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (pw2): Conv2d(8, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (dw): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)\n",
            "            )\n",
            "            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (activation): ReLU(inplace=True)\n",
            "          )\n",
            "          (conv2): ConvBlock(\n",
            "            (conv): BSConvS(\n",
            "              (pw1): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (pw2): Conv2d(16, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (dw): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
            "            )\n",
            "          )\n",
            "          (projection): ConvBlock(\n",
            "            (conv): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          )\n",
            "        )\n",
            "        (unit2): PreactUnit(\n",
            "          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "          (conv1): ConvBlock(\n",
            "            (conv): BSConvS(\n",
            "              (pw1): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (pw2): Conv2d(16, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (dw): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
            "            )\n",
            "            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (activation): ReLU(inplace=True)\n",
            "          )\n",
            "          (conv2): ConvBlock(\n",
            "            (conv): BSConvS(\n",
            "              (pw1): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (pw2): Conv2d(16, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (dw): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (unit3): PreactUnit(\n",
            "          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "          (conv1): ConvBlock(\n",
            "            (conv): BSConvS(\n",
            "              (pw1): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (pw2): Conv2d(16, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (dw): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
            "            )\n",
            "            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (activation): ReLU(inplace=True)\n",
            "          )\n",
            "          (conv2): ConvBlock(\n",
            "            (conv): BSConvS(\n",
            "              (pw1): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (pw2): Conv2d(16, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (dw): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (unit4): PreactUnit(\n",
            "          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "          (conv1): ConvBlock(\n",
            "            (conv): BSConvS(\n",
            "              (pw1): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (pw2): Conv2d(16, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (dw): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
            "            )\n",
            "            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (activation): ReLU(inplace=True)\n",
            "          )\n",
            "          (conv2): ConvBlock(\n",
            "            (conv): BSConvS(\n",
            "              (pw1): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (pw2): Conv2d(16, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (dw): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (stage3): Sequential(\n",
            "        (unit1): PreactUnit(\n",
            "          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "          (conv1): ConvBlock(\n",
            "            (conv): BSConvS(\n",
            "              (pw1): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (pw2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (dw): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)\n",
            "            )\n",
            "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (activation): ReLU(inplace=True)\n",
            "          )\n",
            "          (conv2): ConvBlock(\n",
            "            (conv): BSConvS(\n",
            "              (pw1): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (pw2): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (dw): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
            "            )\n",
            "          )\n",
            "          (projection): ConvBlock(\n",
            "            (conv): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          )\n",
            "        )\n",
            "        (unit2): PreactUnit(\n",
            "          (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "          (conv1): ConvBlock(\n",
            "            (conv): BSConvS(\n",
            "              (pw1): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (pw2): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (dw): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
            "            )\n",
            "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (activation): ReLU(inplace=True)\n",
            "          )\n",
            "          (conv2): ConvBlock(\n",
            "            (conv): BSConvS(\n",
            "              (pw1): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (pw2): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (dw): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (unit3): PreactUnit(\n",
            "          (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "          (conv1): ConvBlock(\n",
            "            (conv): BSConvS(\n",
            "              (pw1): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (pw2): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (dw): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
            "            )\n",
            "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (activation): ReLU(inplace=True)\n",
            "          )\n",
            "          (conv2): ConvBlock(\n",
            "            (conv): BSConvS(\n",
            "              (pw1): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (pw2): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (dw): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (unit4): PreactUnit(\n",
            "          (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "          (conv1): ConvBlock(\n",
            "            (conv): BSConvS(\n",
            "              (pw1): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (pw2): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (dw): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
            "            )\n",
            "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (activation): ReLU(inplace=True)\n",
            "          )\n",
            "          (conv2): ConvBlock(\n",
            "            (conv): BSConvS(\n",
            "              (pw1): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (pw2): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (dw): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (final_activation): PostActivation(\n",
            "        (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (global_pool): AdaptiveAvgPool2d(output_size=1)\n",
            "    )\n",
            "    (classifier): Classifier(\n",
            "      (conv): Conv2d(256, 100, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "  )\n",
            ")\n",
            "the number of model parameters: 273921\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-109390f300f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 310\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-16-109390f300f0>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;31m# train for one epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;31m# evaluate on validation set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-109390f300f0>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_loader, model, criterion, optimizer, epoch)\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;31m#optimizer.step()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-f46ea576eeac>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_foreach_add_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams_with_grads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0;31m# compute g=\\nabla_w L_B(w)|_{w+\\hat{\\epsilon}}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0mclosure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m             \u001b[0;31m# virtual step back to the original point\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_foreach_sub_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams_with_grads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-109390f300f0>\u001b[0m in \u001b[0;36mclosure\u001b[0;34m()\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeta\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mr\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcutmix_prob\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0;31m# generate mixed sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mzero_grad\u001b[0;34m(self, set_to_none)\u001b[0m\n\u001b[1;32m    190\u001b[0m                         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m                             \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m                         \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TeA_DFi2AiQ9"
      },
      "source": [
        "normalize = transforms.Normalize(mean=[x / 255.0 for x in [125.3, 123.0, 113.9]],std=[x / 255.0 for x in [63.0, 62.1, 66.7]])\n",
        "\n",
        "_train = transforms.Compose([\n",
        "            transforms.RandomCrop(32, padding=4),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.ToTensor(),\n",
        "            normalize,\n",
        "        ])\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84,
          "referenced_widgets": [
            "237d4385557648849031c6800ddbad2a",
            "5d4d56d8b8ed42f89ed1048288de67e9",
            "a3f5f5ae1da6468d8f1a9d6ff9fac92d",
            "d8c78c2e900b4b8f82dbb34e89119e16",
            "0c8c892120874f279b009768e93889d5",
            "24309690548c400581142321a27e62bb",
            "32a581040ed54c2db79fbdda935540c7",
            "2b6a6acfbad243a8acbffac97b7d3f1d"
          ]
        },
        "id": "3u9IxvhNASsI",
        "outputId": "f5a2f225-510e-45d4-cab1-9990c45a1997"
      },
      "source": [
        "\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "                datasets.CIFAR100('../data', train=True, download=True, transform=_train),\n",
        "                batch_size=args.batch_size, shuffle=True, num_workers=args.workers, pin_memory=True)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ../data/cifar-100-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "237d4385557648849031c6800ddbad2a",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ../data/cifar-100-python.tar.gz to ../data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dsStVh3y3ud5",
        "outputId": "32e03277-c138-4763-f763-cc4028866fe7"
      },
      "source": [
        "def main():\n",
        "    global args, best_err1, best_err5\n",
        "    \n",
        "\n",
        "    if args.dataset.startswith('cifar'):\n",
        "        normalize = transforms.Normalize(mean=[x / 255.0 for x in [125.3, 123.0, 113.9]],\n",
        "                                         std=[x / 255.0 for x in [63.0, 62.1, 66.7]])\n",
        "\n",
        "        transform_train = transforms.Compose([\n",
        "            transforms.RandomCrop(32, padding=4),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.ToTensor(),\n",
        "            normalize,\n",
        "        ])\n",
        "\n",
        "        transform_test = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            normalize\n",
        "        ])\n",
        "\n",
        "        if args.dataset == 'cifar100':\n",
        "            val_loader = torch.utils.data.DataLoader(\n",
        "                datasets.CIFAR100('../data', train=False, transform=transform_test),\n",
        "                batch_size=args.batch_size, shuffle=False, num_workers=args.workers, pin_memory=True)\n",
        "            numberofclass = 100\n",
        "        elif args.dataset == 'cifar10':\n",
        "            val_loader = torch.utils.data.DataLoader(\n",
        "                datasets.CIFAR10('../data', train=False, transform=transform_test),\n",
        "                batch_size=args.batch_size, shuffle=False, num_workers=args.workers, pin_memory=True)\n",
        "            numberofclass = 10\n",
        "        else:\n",
        "            raise Exception('unknown dataset: {}'.format(args.dataset))\n",
        "\n",
        "    elif args.dataset == 'imagenet':\n",
        "\n",
        "        valdir = os.path.join('/home/data/ILSVRC/val')\n",
        "        normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                         std=[0.229, 0.224, 0.225])\n",
        "\n",
        "        val_loader = torch.utils.data.DataLoader(\n",
        "            datasets.ImageFolder(valdir, transforms.Compose([\n",
        "                transforms.Resize(256),\n",
        "                transforms.CenterCrop(224),\n",
        "                transforms.ToTensor(),\n",
        "                normalize,\n",
        "            ])),\n",
        "            batch_size=args.batch_size, shuffle=False,\n",
        "            num_workers=args.workers, pin_memory=True)\n",
        "        numberofclass = 1000\n",
        "\n",
        "    else:\n",
        "        raise Exception('unknown dataset: {}'.format(args.dataset))\n",
        "\n",
        "    print(\"=> creating model '{}'\".format(args.net_type))\n",
        "    if args.net_type == 'resnet':\n",
        "         model = bsconv.pytorch.get_model('cifar_wrn28_4_bsconvs_p1d8', num_classes=100)\n",
        "    elif args.net_type == 'pyramidnet':\n",
        "        model = PyramidNet(args.dataset, args.depth, args.alpha, numberofclass,\n",
        "                                args.bottleneck)\n",
        "    else:\n",
        "        raise Exception('unknown network architecture: {}'.format(args.net_type))\n",
        "    model = torch.nn.DataParallel(model).cuda()\n",
        "    model.load_state_dict(torch.load('/content/drive/MyDrive/ajoudeep/contest/bsconv_sam/model_best.pth.tar')['state_dict'])\n",
        "\n",
        "    print(model)\n",
        "    print('the number of model parameters: {}'.format(sum([p.data.nelement() for p in model.parameters()])))\n",
        "\n",
        "    # define loss function (criterion) and optimizer\n",
        "    criterion = nn.CrossEntropyLoss().cuda()\n",
        "\n",
        "    cudnn.benchmark = True\n",
        "\n",
        "    # evaluate on validation set\n",
        "    err1, err5, val_loss = validate(val_loader, model, criterion)\n",
        "\n",
        "    print('Accuracy (top-1 and 5 error):', err1, err5)\n",
        "\n",
        "\n",
        "def validate(val_loader, model, criterion):\n",
        "    ans_txt = open('/content/drive/MyDrive/ajoudeep/contest/bsconv_sam/answer.txt','w')\n",
        "    batch_time = AverageMeter()\n",
        "    losses = AverageMeter()\n",
        "    top1 = AverageMeter()\n",
        "    top5 = AverageMeter()\n",
        "\n",
        "    # switch to evaluate mode\n",
        "    model.eval()\n",
        "\n",
        "    end = time.time()\n",
        "    for i, (input, target) in enumerate(val_loader):\n",
        "        target = target.cuda()\n",
        "        \n",
        "        output = model(input)\n",
        "        output = output.cuda()\n",
        "        loss = criterion(output, target)\n",
        "\n",
        "        # measure accuracy and record loss\n",
        "        (err1, err5),pred = accuracy(output.data, target, topk=(1, 5))\n",
        "\n",
        "\n",
        "        losses.update(loss.item(), input.size(0))\n",
        "\n",
        "        top1.update(err1.item(), input.size(0))\n",
        "        top5.update(err5.item(), input.size(0))\n",
        "\n",
        "        for a in pred[0]:\n",
        "            ans_txt.write(str(a.item()) + '\\n')\n",
        "\n",
        "\n",
        "        # measure elapsed time\n",
        "        batch_time.update(time.time() - end)\n",
        "        end = time.time()\n",
        "\n",
        "        if i % args.print_freq == 0 and args.verbose == True:\n",
        "            print('Test (on val set): [{0}/{1}]\\t'\n",
        "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
        "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
        "                  'Top 1-err {top1.val:.4f} ({top1.avg:.4f})\\t'\n",
        "                  'Top 5-err {top5.val:.4f} ({top5.avg:.4f})'.format(\n",
        "                i, len(val_loader), batch_time=batch_time, loss=losses,\n",
        "                top1=top1, top5=top5))\n",
        "\n",
        "    return top1.avg, top5.avg, losses.avg\n",
        "\n",
        "\n",
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "\n",
        "def accuracy(output, target, topk=(1,)):\n",
        "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
        "    maxk = max(topk)\n",
        "    batch_size = target.size(0)\n",
        "\n",
        "    _, pred = output.topk(maxk, 1, True, True)\n",
        "    pred = pred.t()\n",
        "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "\n",
        "    res = []\n",
        "    for k in topk:\n",
        "        correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
        "        wrong_k = batch_size - correct_k\n",
        "        res.append(wrong_k.mul_(100.0 / batch_size))\n",
        "\n",
        "    return res, pred[:1]\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "=> creating model 'resnet'\n",
            "DataParallel(\n",
            "  (module): ResNet(\n",
            "    (backbone): Sequential(\n",
            "      (data_bn): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (init_unit): InitUnitSmall(\n",
            "        (conv): ConvBlock(\n",
            "          (conv): BSConvS(\n",
            "            (pw1): Conv2d(3, 3, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (bn1): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (pw2): Conv2d(3, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (dw): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (stage1): Sequential(\n",
            "        (unit1): PreactUnit(\n",
            "          (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "          (conv1): ConvBlock(\n",
            "            (conv): BSConvS(\n",
            "              (pw1): Conv2d(16, 4, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (pw2): Conv2d(4, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (dw): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
            "            )\n",
            "            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (activation): ReLU(inplace=True)\n",
            "          )\n",
            "          (conv2): ConvBlock(\n",
            "            (conv): BSConvS(\n",
            "              (pw1): Conv2d(64, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (pw2): Conv2d(8, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (dw): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
            "            )\n",
            "          )\n",
            "          (projection): ConvBlock(\n",
            "            (conv): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          )\n",
            "        )\n",
            "        (unit2): PreactUnit(\n",
            "          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "          (conv1): ConvBlock(\n",
            "            (conv): BSConvS(\n",
            "              (pw1): Conv2d(64, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (pw2): Conv2d(8, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (dw): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
            "            )\n",
            "            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (activation): ReLU(inplace=True)\n",
            "          )\n",
            "          (conv2): ConvBlock(\n",
            "            (conv): BSConvS(\n",
            "              (pw1): Conv2d(64, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (pw2): Conv2d(8, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (dw): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (unit3): PreactUnit(\n",
            "          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "          (conv1): ConvBlock(\n",
            "            (conv): BSConvS(\n",
            "              (pw1): Conv2d(64, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (pw2): Conv2d(8, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (dw): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
            "            )\n",
            "            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (activation): ReLU(inplace=True)\n",
            "          )\n",
            "          (conv2): ConvBlock(\n",
            "            (conv): BSConvS(\n",
            "              (pw1): Conv2d(64, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (pw2): Conv2d(8, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (dw): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (unit4): PreactUnit(\n",
            "          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "          (conv1): ConvBlock(\n",
            "            (conv): BSConvS(\n",
            "              (pw1): Conv2d(64, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (pw2): Conv2d(8, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (dw): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
            "            )\n",
            "            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (activation): ReLU(inplace=True)\n",
            "          )\n",
            "          (conv2): ConvBlock(\n",
            "            (conv): BSConvS(\n",
            "              (pw1): Conv2d(64, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (pw2): Conv2d(8, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (dw): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (stage2): Sequential(\n",
            "        (unit1): PreactUnit(\n",
            "          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "          (conv1): ConvBlock(\n",
            "            (conv): BSConvS(\n",
            "              (pw1): Conv2d(64, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (pw2): Conv2d(8, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (dw): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)\n",
            "            )\n",
            "            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (activation): ReLU(inplace=True)\n",
            "          )\n",
            "          (conv2): ConvBlock(\n",
            "            (conv): BSConvS(\n",
            "              (pw1): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (pw2): Conv2d(16, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (dw): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
            "            )\n",
            "          )\n",
            "          (projection): ConvBlock(\n",
            "            (conv): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          )\n",
            "        )\n",
            "        (unit2): PreactUnit(\n",
            "          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "          (conv1): ConvBlock(\n",
            "            (conv): BSConvS(\n",
            "              (pw1): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (pw2): Conv2d(16, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (dw): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
            "            )\n",
            "            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (activation): ReLU(inplace=True)\n",
            "          )\n",
            "          (conv2): ConvBlock(\n",
            "            (conv): BSConvS(\n",
            "              (pw1): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (pw2): Conv2d(16, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (dw): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (unit3): PreactUnit(\n",
            "          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "          (conv1): ConvBlock(\n",
            "            (conv): BSConvS(\n",
            "              (pw1): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (pw2): Conv2d(16, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (dw): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
            "            )\n",
            "            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (activation): ReLU(inplace=True)\n",
            "          )\n",
            "          (conv2): ConvBlock(\n",
            "            (conv): BSConvS(\n",
            "              (pw1): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (pw2): Conv2d(16, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (dw): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (unit4): PreactUnit(\n",
            "          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "          (conv1): ConvBlock(\n",
            "            (conv): BSConvS(\n",
            "              (pw1): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (pw2): Conv2d(16, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (dw): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
            "            )\n",
            "            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (activation): ReLU(inplace=True)\n",
            "          )\n",
            "          (conv2): ConvBlock(\n",
            "            (conv): BSConvS(\n",
            "              (pw1): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (pw2): Conv2d(16, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (dw): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (stage3): Sequential(\n",
            "        (unit1): PreactUnit(\n",
            "          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "          (conv1): ConvBlock(\n",
            "            (conv): BSConvS(\n",
            "              (pw1): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (pw2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (dw): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)\n",
            "            )\n",
            "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (activation): ReLU(inplace=True)\n",
            "          )\n",
            "          (conv2): ConvBlock(\n",
            "            (conv): BSConvS(\n",
            "              (pw1): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (pw2): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (dw): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
            "            )\n",
            "          )\n",
            "          (projection): ConvBlock(\n",
            "            (conv): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          )\n",
            "        )\n",
            "        (unit2): PreactUnit(\n",
            "          (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "          (conv1): ConvBlock(\n",
            "            (conv): BSConvS(\n",
            "              (pw1): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (pw2): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (dw): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
            "            )\n",
            "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (activation): ReLU(inplace=True)\n",
            "          )\n",
            "          (conv2): ConvBlock(\n",
            "            (conv): BSConvS(\n",
            "              (pw1): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (pw2): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (dw): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (unit3): PreactUnit(\n",
            "          (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "          (conv1): ConvBlock(\n",
            "            (conv): BSConvS(\n",
            "              (pw1): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (pw2): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (dw): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
            "            )\n",
            "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (activation): ReLU(inplace=True)\n",
            "          )\n",
            "          (conv2): ConvBlock(\n",
            "            (conv): BSConvS(\n",
            "              (pw1): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (pw2): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (dw): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (unit4): PreactUnit(\n",
            "          (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "          (conv1): ConvBlock(\n",
            "            (conv): BSConvS(\n",
            "              (pw1): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (pw2): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (dw): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
            "            )\n",
            "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (activation): ReLU(inplace=True)\n",
            "          )\n",
            "          (conv2): ConvBlock(\n",
            "            (conv): BSConvS(\n",
            "              (pw1): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (pw2): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (dw): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (final_activation): PostActivation(\n",
            "        (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (global_pool): AdaptiveAvgPool2d(output_size=1)\n",
            "    )\n",
            "    (classifier): Classifier(\n",
            "      (conv): Conv2d(256, 100, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "  )\n",
            ")\n",
            "the number of model parameters: 273921\n",
            "Test (on val set): [0/157]\tTime 0.466 (0.466)\tLoss 0.7869 (0.7869)\tTop 1-err 20.3125 (20.3125)\tTop 5-err 4.6875 (4.6875)\n",
            "Test (on val set): [1/157]\tTime 0.037 (0.252)\tLoss 0.6317 (0.7093)\tTop 1-err 20.3125 (20.3125)\tTop 5-err 3.1250 (3.9062)\n",
            "Test (on val set): [2/157]\tTime 0.030 (0.178)\tLoss 1.2451 (0.8879)\tTop 1-err 26.5625 (22.3958)\tTop 5-err 12.5000 (6.7708)\n",
            "Test (on val set): [3/157]\tTime 0.030 (0.141)\tLoss 0.7736 (0.8593)\tTop 1-err 20.3125 (21.8750)\tTop 5-err 6.2500 (6.6406)\n",
            "Test (on val set): [4/157]\tTime 0.027 (0.118)\tLoss 0.9488 (0.8772)\tTop 1-err 26.5625 (22.8125)\tTop 5-err 7.8125 (6.8750)\n",
            "Test (on val set): [5/157]\tTime 0.024 (0.102)\tLoss 1.0311 (0.9029)\tTop 1-err 29.6875 (23.9583)\tTop 5-err 7.8125 (7.0312)\n",
            "Test (on val set): [6/157]\tTime 0.027 (0.092)\tLoss 0.9776 (0.9135)\tTop 1-err 28.1250 (24.5536)\tTop 5-err 7.8125 (7.1429)\n",
            "Test (on val set): [7/157]\tTime 0.029 (0.084)\tLoss 0.7016 (0.8870)\tTop 1-err 20.3125 (24.0234)\tTop 5-err 3.1250 (6.6406)\n",
            "Test (on val set): [8/157]\tTime 0.026 (0.077)\tLoss 1.3764 (0.9414)\tTop 1-err 39.0625 (25.6944)\tTop 5-err 9.3750 (6.9444)\n",
            "Test (on val set): [9/157]\tTime 0.027 (0.072)\tLoss 0.9369 (0.9410)\tTop 1-err 26.5625 (25.7812)\tTop 5-err 7.8125 (7.0312)\n",
            "Test (on val set): [10/157]\tTime 0.027 (0.068)\tLoss 0.9339 (0.9403)\tTop 1-err 18.7500 (25.1420)\tTop 5-err 9.3750 (7.2443)\n",
            "Test (on val set): [11/157]\tTime 0.027 (0.065)\tLoss 0.4988 (0.9035)\tTop 1-err 21.8750 (24.8698)\tTop 5-err 1.5625 (6.7708)\n",
            "Test (on val set): [12/157]\tTime 0.027 (0.062)\tLoss 0.8752 (0.9014)\tTop 1-err 20.3125 (24.5192)\tTop 5-err 9.3750 (6.9712)\n",
            "Test (on val set): [13/157]\tTime 0.026 (0.059)\tLoss 0.9716 (0.9064)\tTop 1-err 34.3750 (25.2232)\tTop 5-err 4.6875 (6.8080)\n",
            "Test (on val set): [14/157]\tTime 0.022 (0.057)\tLoss 1.1172 (0.9204)\tTop 1-err 29.6875 (25.5208)\tTop 5-err 6.2500 (6.7708)\n",
            "Test (on val set): [15/157]\tTime 0.028 (0.055)\tLoss 0.7685 (0.9109)\tTop 1-err 23.4375 (25.3906)\tTop 5-err 4.6875 (6.6406)\n",
            "Test (on val set): [16/157]\tTime 0.030 (0.054)\tLoss 0.8088 (0.9049)\tTop 1-err 23.4375 (25.2757)\tTop 5-err 3.1250 (6.4338)\n",
            "Test (on val set): [17/157]\tTime 0.027 (0.052)\tLoss 0.6946 (0.8932)\tTop 1-err 18.7500 (24.9132)\tTop 5-err 3.1250 (6.2500)\n",
            "Test (on val set): [18/157]\tTime 0.028 (0.051)\tLoss 0.7169 (0.8840)\tTop 1-err 18.7500 (24.5888)\tTop 5-err 3.1250 (6.0855)\n",
            "Test (on val set): [19/157]\tTime 0.028 (0.050)\tLoss 0.5770 (0.8686)\tTop 1-err 18.7500 (24.2969)\tTop 5-err 1.5625 (5.8594)\n",
            "Test (on val set): [20/157]\tTime 0.023 (0.048)\tLoss 0.8351 (0.8670)\tTop 1-err 18.7500 (24.0327)\tTop 5-err 6.2500 (5.8780)\n",
            "Test (on val set): [21/157]\tTime 0.035 (0.048)\tLoss 0.7222 (0.8604)\tTop 1-err 23.4375 (24.0057)\tTop 5-err 1.5625 (5.6818)\n",
            "Test (on val set): [22/157]\tTime 0.027 (0.047)\tLoss 0.7089 (0.8538)\tTop 1-err 20.3125 (23.8451)\tTop 5-err 3.1250 (5.5707)\n",
            "Test (on val set): [23/157]\tTime 0.025 (0.046)\tLoss 0.7598 (0.8499)\tTop 1-err 23.4375 (23.8281)\tTop 5-err 3.1250 (5.4688)\n",
            "Test (on val set): [24/157]\tTime 0.034 (0.045)\tLoss 0.8433 (0.8497)\tTop 1-err 23.4375 (23.8125)\tTop 5-err 3.1250 (5.3750)\n",
            "Test (on val set): [25/157]\tTime 0.027 (0.045)\tLoss 0.9618 (0.8540)\tTop 1-err 32.8125 (24.1587)\tTop 5-err 3.1250 (5.2885)\n",
            "Test (on val set): [26/157]\tTime 0.028 (0.044)\tLoss 1.1620 (0.8654)\tTop 1-err 31.2500 (24.4213)\tTop 5-err 7.8125 (5.3819)\n",
            "Test (on val set): [27/157]\tTime 0.026 (0.044)\tLoss 0.8550 (0.8650)\tTop 1-err 25.0000 (24.4420)\tTop 5-err 3.1250 (5.3013)\n",
            "Test (on val set): [28/157]\tTime 0.034 (0.043)\tLoss 0.7702 (0.8617)\tTop 1-err 23.4375 (24.4073)\tTop 5-err 6.2500 (5.3341)\n",
            "Test (on val set): [29/157]\tTime 0.027 (0.043)\tLoss 0.6284 (0.8540)\tTop 1-err 21.8750 (24.3229)\tTop 5-err 3.1250 (5.2604)\n",
            "Test (on val set): [30/157]\tTime 0.024 (0.042)\tLoss 0.6110 (0.8461)\tTop 1-err 17.1875 (24.0927)\tTop 5-err 3.1250 (5.1915)\n",
            "Test (on val set): [31/157]\tTime 0.032 (0.042)\tLoss 0.7453 (0.8430)\tTop 1-err 20.3125 (23.9746)\tTop 5-err 3.1250 (5.1270)\n",
            "Test (on val set): [32/157]\tTime 0.025 (0.041)\tLoss 0.9637 (0.8466)\tTop 1-err 29.6875 (24.1477)\tTop 5-err 4.6875 (5.1136)\n",
            "Test (on val set): [33/157]\tTime 0.032 (0.041)\tLoss 0.8676 (0.8472)\tTop 1-err 15.6250 (23.8971)\tTop 5-err 7.8125 (5.1930)\n",
            "Test (on val set): [34/157]\tTime 0.026 (0.041)\tLoss 1.0869 (0.8541)\tTop 1-err 28.1250 (24.0179)\tTop 5-err 10.9375 (5.3571)\n",
            "Test (on val set): [35/157]\tTime 0.023 (0.040)\tLoss 0.8732 (0.8546)\tTop 1-err 21.8750 (23.9583)\tTop 5-err 6.2500 (5.3819)\n",
            "Test (on val set): [36/157]\tTime 0.027 (0.040)\tLoss 0.8189 (0.8537)\tTop 1-err 25.0000 (23.9865)\tTop 5-err 3.1250 (5.3209)\n",
            "Test (on val set): [37/157]\tTime 0.025 (0.039)\tLoss 1.0176 (0.8580)\tTop 1-err 29.6875 (24.1365)\tTop 5-err 9.3750 (5.4276)\n",
            "Test (on val set): [38/157]\tTime 0.026 (0.039)\tLoss 0.6856 (0.8536)\tTop 1-err 18.7500 (23.9984)\tTop 5-err 3.1250 (5.3686)\n",
            "Test (on val set): [39/157]\tTime 0.030 (0.039)\tLoss 0.6633 (0.8488)\tTop 1-err 20.3125 (23.9062)\tTop 5-err 3.1250 (5.3125)\n",
            "Test (on val set): [40/157]\tTime 0.027 (0.038)\tLoss 0.8710 (0.8493)\tTop 1-err 20.3125 (23.8186)\tTop 5-err 6.2500 (5.3354)\n",
            "Test (on val set): [41/157]\tTime 0.025 (0.038)\tLoss 0.7090 (0.8460)\tTop 1-err 25.0000 (23.8467)\tTop 5-err 4.6875 (5.3199)\n",
            "Test (on val set): [42/157]\tTime 0.027 (0.038)\tLoss 0.7552 (0.8439)\tTop 1-err 21.8750 (23.8009)\tTop 5-err 3.1250 (5.2689)\n",
            "Test (on val set): [43/157]\tTime 0.024 (0.038)\tLoss 1.0022 (0.8475)\tTop 1-err 20.3125 (23.7216)\tTop 5-err 10.9375 (5.3977)\n",
            "Test (on val set): [44/157]\tTime 0.029 (0.037)\tLoss 1.1824 (0.8549)\tTop 1-err 35.9375 (23.9931)\tTop 5-err 7.8125 (5.4514)\n",
            "Test (on val set): [45/157]\tTime 0.026 (0.037)\tLoss 1.0215 (0.8585)\tTop 1-err 28.1250 (24.0829)\tTop 5-err 6.2500 (5.4688)\n",
            "Test (on val set): [46/157]\tTime 0.024 (0.037)\tLoss 0.7989 (0.8573)\tTop 1-err 28.1250 (24.1689)\tTop 5-err 1.5625 (5.3856)\n",
            "Test (on val set): [47/157]\tTime 0.031 (0.037)\tLoss 1.0670 (0.8616)\tTop 1-err 31.2500 (24.3164)\tTop 5-err 6.2500 (5.4036)\n",
            "Test (on val set): [48/157]\tTime 0.022 (0.036)\tLoss 0.9915 (0.8643)\tTop 1-err 28.1250 (24.3941)\tTop 5-err 4.6875 (5.3890)\n",
            "Test (on val set): [49/157]\tTime 0.029 (0.036)\tLoss 0.6517 (0.8600)\tTop 1-err 21.8750 (24.3438)\tTop 5-err 1.5625 (5.3125)\n",
            "Test (on val set): [50/157]\tTime 0.025 (0.036)\tLoss 0.9660 (0.8621)\tTop 1-err 28.1250 (24.4179)\tTop 5-err 9.3750 (5.3922)\n",
            "Test (on val set): [51/157]\tTime 0.024 (0.036)\tLoss 1.1141 (0.8670)\tTop 1-err 23.4375 (24.3990)\tTop 5-err 7.8125 (5.4387)\n",
            "Test (on val set): [52/157]\tTime 0.029 (0.036)\tLoss 0.9488 (0.8685)\tTop 1-err 26.5625 (24.4399)\tTop 5-err 6.2500 (5.4540)\n",
            "Test (on val set): [53/157]\tTime 0.026 (0.036)\tLoss 0.8849 (0.8688)\tTop 1-err 26.5625 (24.4792)\tTop 5-err 6.2500 (5.4688)\n",
            "Test (on val set): [54/157]\tTime 0.030 (0.035)\tLoss 1.3148 (0.8769)\tTop 1-err 32.8125 (24.6307)\tTop 5-err 10.9375 (5.5682)\n",
            "Test (on val set): [55/157]\tTime 0.027 (0.035)\tLoss 0.8572 (0.8766)\tTop 1-err 28.1250 (24.6931)\tTop 5-err 6.2500 (5.5804)\n",
            "Test (on val set): [56/157]\tTime 0.024 (0.035)\tLoss 0.7298 (0.8740)\tTop 1-err 26.5625 (24.7259)\tTop 5-err 0.0000 (5.4825)\n",
            "Test (on val set): [57/157]\tTime 0.027 (0.035)\tLoss 0.6887 (0.8708)\tTop 1-err 20.3125 (24.6498)\tTop 5-err 3.1250 (5.4418)\n",
            "Test (on val set): [58/157]\tTime 0.030 (0.035)\tLoss 0.4960 (0.8645)\tTop 1-err 15.6250 (24.4968)\tTop 5-err 3.1250 (5.4025)\n",
            "Test (on val set): [59/157]\tTime 0.026 (0.035)\tLoss 0.7902 (0.8632)\tTop 1-err 14.0625 (24.3229)\tTop 5-err 4.6875 (5.3906)\n",
            "Test (on val set): [60/157]\tTime 0.025 (0.035)\tLoss 0.9215 (0.8642)\tTop 1-err 28.1250 (24.3852)\tTop 5-err 1.5625 (5.3279)\n",
            "Test (on val set): [61/157]\tTime 0.025 (0.034)\tLoss 0.7011 (0.8615)\tTop 1-err 21.8750 (24.3448)\tTop 5-err 3.1250 (5.2923)\n",
            "Test (on val set): [62/157]\tTime 0.037 (0.034)\tLoss 0.6184 (0.8577)\tTop 1-err 18.7500 (24.2560)\tTop 5-err 1.5625 (5.2331)\n",
            "Test (on val set): [63/157]\tTime 0.027 (0.034)\tLoss 1.0988 (0.8614)\tTop 1-err 32.8125 (24.3896)\tTop 5-err 3.1250 (5.2002)\n",
            "Test (on val set): [64/157]\tTime 0.025 (0.034)\tLoss 0.8853 (0.8618)\tTop 1-err 20.3125 (24.3269)\tTop 5-err 4.6875 (5.1923)\n",
            "Test (on val set): [65/157]\tTime 0.026 (0.034)\tLoss 0.9728 (0.8635)\tTop 1-err 28.1250 (24.3845)\tTop 5-err 9.3750 (5.2557)\n",
            "Test (on val set): [66/157]\tTime 0.038 (0.034)\tLoss 0.8934 (0.8639)\tTop 1-err 23.4375 (24.3703)\tTop 5-err 1.5625 (5.2006)\n",
            "Test (on val set): [67/157]\tTime 0.024 (0.034)\tLoss 0.9025 (0.8645)\tTop 1-err 25.0000 (24.3796)\tTop 5-err 6.2500 (5.2160)\n",
            "Test (on val set): [68/157]\tTime 0.024 (0.034)\tLoss 0.8199 (0.8639)\tTop 1-err 18.7500 (24.2980)\tTop 5-err 3.1250 (5.1857)\n",
            "Test (on val set): [69/157]\tTime 0.024 (0.034)\tLoss 0.6551 (0.8609)\tTop 1-err 20.3125 (24.2411)\tTop 5-err 3.1250 (5.1562)\n",
            "Test (on val set): [70/157]\tTime 0.028 (0.034)\tLoss 0.7505 (0.8593)\tTop 1-err 25.0000 (24.2518)\tTop 5-err 6.2500 (5.1717)\n",
            "Test (on val set): [71/157]\tTime 0.036 (0.034)\tLoss 0.9867 (0.8611)\tTop 1-err 28.1250 (24.3056)\tTop 5-err 10.9375 (5.2517)\n",
            "Test (on val set): [72/157]\tTime 0.025 (0.033)\tLoss 0.8859 (0.8614)\tTop 1-err 28.1250 (24.3579)\tTop 5-err 3.1250 (5.2226)\n",
            "Test (on val set): [73/157]\tTime 0.023 (0.033)\tLoss 0.8069 (0.8607)\tTop 1-err 21.8750 (24.3243)\tTop 5-err 6.2500 (5.2365)\n",
            "Test (on val set): [74/157]\tTime 0.027 (0.033)\tLoss 1.0213 (0.8628)\tTop 1-err 31.2500 (24.4167)\tTop 5-err 9.3750 (5.2917)\n",
            "Test (on val set): [75/157]\tTime 0.029 (0.033)\tLoss 0.9927 (0.8645)\tTop 1-err 32.8125 (24.5271)\tTop 5-err 9.3750 (5.3454)\n",
            "Test (on val set): [76/157]\tTime 0.023 (0.033)\tLoss 0.8777 (0.8647)\tTop 1-err 26.5625 (24.5536)\tTop 5-err 3.1250 (5.3166)\n",
            "Test (on val set): [77/157]\tTime 0.033 (0.033)\tLoss 0.6327 (0.8617)\tTop 1-err 18.7500 (24.4792)\tTop 5-err 6.2500 (5.3285)\n",
            "Test (on val set): [78/157]\tTime 0.027 (0.033)\tLoss 0.9521 (0.8629)\tTop 1-err 25.0000 (24.4858)\tTop 5-err 7.8125 (5.3600)\n",
            "Test (on val set): [79/157]\tTime 0.026 (0.033)\tLoss 0.8966 (0.8633)\tTop 1-err 28.1250 (24.5312)\tTop 5-err 7.8125 (5.3906)\n",
            "Test (on val set): [80/157]\tTime 0.028 (0.033)\tLoss 0.7342 (0.8617)\tTop 1-err 21.8750 (24.4985)\tTop 5-err 3.1250 (5.3627)\n",
            "Test (on val set): [81/157]\tTime 0.026 (0.033)\tLoss 0.6170 (0.8587)\tTop 1-err 17.1875 (24.4093)\tTop 5-err 3.1250 (5.3354)\n",
            "Test (on val set): [82/157]\tTime 0.022 (0.033)\tLoss 1.1490 (0.8622)\tTop 1-err 34.3750 (24.5294)\tTop 5-err 3.1250 (5.3087)\n",
            "Test (on val set): [83/157]\tTime 0.029 (0.033)\tLoss 0.7610 (0.8610)\tTop 1-err 21.8750 (24.4978)\tTop 5-err 6.2500 (5.3199)\n",
            "Test (on val set): [84/157]\tTime 0.022 (0.032)\tLoss 0.8348 (0.8607)\tTop 1-err 20.3125 (24.4485)\tTop 5-err 4.6875 (5.3125)\n",
            "Test (on val set): [85/157]\tTime 0.022 (0.032)\tLoss 1.2477 (0.8652)\tTop 1-err 28.1250 (24.4913)\tTop 5-err 12.5000 (5.3961)\n",
            "Test (on val set): [86/157]\tTime 0.054 (0.033)\tLoss 0.6917 (0.8632)\tTop 1-err 23.4375 (24.4792)\tTop 5-err 0.0000 (5.3341)\n",
            "Test (on val set): [87/157]\tTime 0.042 (0.033)\tLoss 0.5935 (0.8602)\tTop 1-err 23.4375 (24.4673)\tTop 5-err 1.5625 (5.2912)\n",
            "Test (on val set): [88/157]\tTime 0.027 (0.033)\tLoss 1.0160 (0.8619)\tTop 1-err 23.4375 (24.4558)\tTop 5-err 7.8125 (5.3195)\n",
            "Test (on val set): [89/157]\tTime 0.025 (0.033)\tLoss 0.9677 (0.8631)\tTop 1-err 29.6875 (24.5139)\tTop 5-err 6.2500 (5.3299)\n",
            "Test (on val set): [90/157]\tTime 0.030 (0.033)\tLoss 0.5330 (0.8595)\tTop 1-err 10.9375 (24.3647)\tTop 5-err 3.1250 (5.3056)\n",
            "Test (on val set): [91/157]\tTime 0.024 (0.032)\tLoss 0.7039 (0.8578)\tTop 1-err 25.0000 (24.3716)\tTop 5-err 3.1250 (5.2819)\n",
            "Test (on val set): [92/157]\tTime 0.031 (0.032)\tLoss 0.7351 (0.8564)\tTop 1-err 20.3125 (24.3280)\tTop 5-err 3.1250 (5.2587)\n",
            "Test (on val set): [93/157]\tTime 0.029 (0.032)\tLoss 0.9286 (0.8572)\tTop 1-err 28.1250 (24.3684)\tTop 5-err 6.2500 (5.2693)\n",
            "Test (on val set): [94/157]\tTime 0.027 (0.032)\tLoss 0.8611 (0.8573)\tTop 1-err 35.9375 (24.4901)\tTop 5-err 0.0000 (5.2138)\n",
            "Test (on val set): [95/157]\tTime 0.033 (0.032)\tLoss 1.0831 (0.8596)\tTop 1-err 23.4375 (24.4792)\tTop 5-err 9.3750 (5.2572)\n",
            "Test (on val set): [96/157]\tTime 0.025 (0.032)\tLoss 1.1614 (0.8627)\tTop 1-err 26.5625 (24.5006)\tTop 5-err 7.8125 (5.2835)\n",
            "Test (on val set): [97/157]\tTime 0.025 (0.032)\tLoss 0.7490 (0.8616)\tTop 1-err 20.3125 (24.4579)\tTop 5-err 3.1250 (5.2615)\n",
            "Test (on val set): [98/157]\tTime 0.030 (0.032)\tLoss 0.6120 (0.8590)\tTop 1-err 15.6250 (24.3687)\tTop 5-err 3.1250 (5.2399)\n",
            "Test (on val set): [99/157]\tTime 0.025 (0.032)\tLoss 0.6588 (0.8570)\tTop 1-err 20.3125 (24.3281)\tTop 5-err 1.5625 (5.2031)\n",
            "Test (on val set): [100/157]\tTime 0.026 (0.032)\tLoss 0.8509 (0.8570)\tTop 1-err 25.0000 (24.3348)\tTop 5-err 1.5625 (5.1671)\n",
            "Test (on val set): [101/157]\tTime 0.031 (0.032)\tLoss 1.5719 (0.8640)\tTop 1-err 37.5000 (24.4638)\tTop 5-err 14.0625 (5.2543)\n",
            "Test (on val set): [102/157]\tTime 0.039 (0.032)\tLoss 1.2930 (0.8681)\tTop 1-err 35.9375 (24.5752)\tTop 5-err 10.9375 (5.3095)\n",
            "Test (on val set): [103/157]\tTime 0.035 (0.032)\tLoss 0.8145 (0.8676)\tTop 1-err 17.1875 (24.5042)\tTop 5-err 9.3750 (5.3486)\n",
            "Test (on val set): [104/157]\tTime 0.028 (0.032)\tLoss 0.5437 (0.8645)\tTop 1-err 17.1875 (24.4345)\tTop 5-err 0.0000 (5.2976)\n",
            "Test (on val set): [105/157]\tTime 0.026 (0.032)\tLoss 0.7373 (0.8633)\tTop 1-err 21.8750 (24.4104)\tTop 5-err 6.2500 (5.3066)\n",
            "Test (on val set): [106/157]\tTime 0.024 (0.032)\tLoss 0.9738 (0.8644)\tTop 1-err 17.1875 (24.3429)\tTop 5-err 3.1250 (5.2862)\n",
            "Test (on val set): [107/157]\tTime 0.030 (0.032)\tLoss 0.6732 (0.8626)\tTop 1-err 20.3125 (24.3056)\tTop 5-err 6.2500 (5.2951)\n",
            "Test (on val set): [108/157]\tTime 0.030 (0.032)\tLoss 0.7258 (0.8614)\tTop 1-err 17.1875 (24.2403)\tTop 5-err 4.6875 (5.2896)\n",
            "Test (on val set): [109/157]\tTime 0.039 (0.032)\tLoss 1.2877 (0.8652)\tTop 1-err 34.3750 (24.3324)\tTop 5-err 6.2500 (5.2983)\n",
            "Test (on val set): [110/157]\tTime 0.026 (0.032)\tLoss 0.9115 (0.8656)\tTop 1-err 28.1250 (24.3666)\tTop 5-err 7.8125 (5.3209)\n",
            "Test (on val set): [111/157]\tTime 0.024 (0.032)\tLoss 0.6797 (0.8640)\tTop 1-err 15.6250 (24.2885)\tTop 5-err 7.8125 (5.3432)\n",
            "Test (on val set): [112/157]\tTime 0.031 (0.032)\tLoss 0.8072 (0.8635)\tTop 1-err 29.6875 (24.3363)\tTop 5-err 7.8125 (5.3650)\n",
            "Test (on val set): [113/157]\tTime 0.027 (0.032)\tLoss 0.7210 (0.8622)\tTop 1-err 18.7500 (24.2873)\tTop 5-err 3.1250 (5.3454)\n",
            "Test (on val set): [114/157]\tTime 0.023 (0.032)\tLoss 0.8207 (0.8619)\tTop 1-err 25.0000 (24.2935)\tTop 5-err 3.1250 (5.3261)\n",
            "Test (on val set): [115/157]\tTime 0.029 (0.032)\tLoss 0.8882 (0.8621)\tTop 1-err 23.4375 (24.2861)\tTop 5-err 7.8125 (5.3475)\n",
            "Test (on val set): [116/157]\tTime 0.027 (0.032)\tLoss 1.1123 (0.8642)\tTop 1-err 26.5625 (24.3056)\tTop 5-err 10.9375 (5.3953)\n",
            "Test (on val set): [117/157]\tTime 0.025 (0.032)\tLoss 0.5404 (0.8615)\tTop 1-err 15.6250 (24.2320)\tTop 5-err 1.5625 (5.3628)\n",
            "Test (on val set): [118/157]\tTime 0.028 (0.032)\tLoss 0.9904 (0.8626)\tTop 1-err 26.5625 (24.2516)\tTop 5-err 6.2500 (5.3703)\n",
            "Test (on val set): [119/157]\tTime 0.023 (0.031)\tLoss 1.1771 (0.8652)\tTop 1-err 31.2500 (24.3099)\tTop 5-err 6.2500 (5.3776)\n",
            "Test (on val set): [120/157]\tTime 0.030 (0.031)\tLoss 0.8560 (0.8651)\tTop 1-err 25.0000 (24.3156)\tTop 5-err 6.2500 (5.3848)\n",
            "Test (on val set): [121/157]\tTime 0.026 (0.031)\tLoss 1.2892 (0.8686)\tTop 1-err 31.2500 (24.3724)\tTop 5-err 7.8125 (5.4047)\n",
            "Test (on val set): [122/157]\tTime 0.029 (0.031)\tLoss 0.9196 (0.8690)\tTop 1-err 28.1250 (24.4029)\tTop 5-err 4.6875 (5.3989)\n",
            "Test (on val set): [123/157]\tTime 0.023 (0.031)\tLoss 0.8768 (0.8691)\tTop 1-err 29.6875 (24.4456)\tTop 5-err 4.6875 (5.3931)\n",
            "Test (on val set): [124/157]\tTime 0.033 (0.031)\tLoss 0.9784 (0.8699)\tTop 1-err 25.0000 (24.4500)\tTop 5-err 4.6875 (5.3875)\n",
            "Test (on val set): [125/157]\tTime 0.025 (0.031)\tLoss 0.8202 (0.8696)\tTop 1-err 18.7500 (24.4048)\tTop 5-err 4.6875 (5.3819)\n",
            "Test (on val set): [126/157]\tTime 0.029 (0.031)\tLoss 1.0222 (0.8708)\tTop 1-err 25.0000 (24.4094)\tTop 5-err 6.2500 (5.3888)\n",
            "Test (on val set): [127/157]\tTime 0.027 (0.031)\tLoss 0.8833 (0.8709)\tTop 1-err 20.3125 (24.3774)\tTop 5-err 6.2500 (5.3955)\n",
            "Test (on val set): [128/157]\tTime 0.026 (0.031)\tLoss 0.6849 (0.8694)\tTop 1-err 17.1875 (24.3217)\tTop 5-err 3.1250 (5.3779)\n",
            "Test (on val set): [129/157]\tTime 0.025 (0.031)\tLoss 0.9297 (0.8699)\tTop 1-err 23.4375 (24.3149)\tTop 5-err 4.6875 (5.3726)\n",
            "Test (on val set): [130/157]\tTime 0.031 (0.031)\tLoss 0.7885 (0.8693)\tTop 1-err 25.0000 (24.3201)\tTop 5-err 6.2500 (5.3793)\n",
            "Test (on val set): [131/157]\tTime 0.027 (0.031)\tLoss 0.9308 (0.8697)\tTop 1-err 23.4375 (24.3134)\tTop 5-err 6.2500 (5.3859)\n",
            "Test (on val set): [132/157]\tTime 0.030 (0.031)\tLoss 0.5697 (0.8675)\tTop 1-err 17.1875 (24.2599)\tTop 5-err 3.1250 (5.3689)\n",
            "Test (on val set): [133/157]\tTime 0.027 (0.031)\tLoss 0.6243 (0.8657)\tTop 1-err 21.8750 (24.2421)\tTop 5-err 1.5625 (5.3405)\n",
            "Test (on val set): [134/157]\tTime 0.026 (0.031)\tLoss 0.6660 (0.8642)\tTop 1-err 26.5625 (24.2593)\tTop 5-err 3.1250 (5.3241)\n",
            "Test (on val set): [135/157]\tTime 0.025 (0.031)\tLoss 0.8535 (0.8641)\tTop 1-err 26.5625 (24.2762)\tTop 5-err 4.6875 (5.3194)\n",
            "Test (on val set): [136/157]\tTime 0.023 (0.031)\tLoss 0.4841 (0.8613)\tTop 1-err 14.0625 (24.2016)\tTop 5-err 3.1250 (5.3034)\n",
            "Test (on val set): [137/157]\tTime 0.024 (0.031)\tLoss 0.8965 (0.8616)\tTop 1-err 29.6875 (24.2414)\tTop 5-err 1.5625 (5.2763)\n",
            "Test (on val set): [138/157]\tTime 0.041 (0.031)\tLoss 0.7792 (0.8610)\tTop 1-err 21.8750 (24.2244)\tTop 5-err 9.3750 (5.3058)\n",
            "Test (on val set): [139/157]\tTime 0.024 (0.031)\tLoss 0.7986 (0.8605)\tTop 1-err 23.4375 (24.2188)\tTop 5-err 4.6875 (5.3013)\n",
            "Test (on val set): [140/157]\tTime 0.036 (0.031)\tLoss 0.9687 (0.8613)\tTop 1-err 25.0000 (24.2243)\tTop 5-err 7.8125 (5.3191)\n",
            "Test (on val set): [141/157]\tTime 0.028 (0.031)\tLoss 1.0165 (0.8624)\tTop 1-err 35.9375 (24.3068)\tTop 5-err 1.5625 (5.2927)\n",
            "Test (on val set): [142/157]\tTime 0.029 (0.031)\tLoss 1.2891 (0.8654)\tTop 1-err 35.9375 (24.3881)\tTop 5-err 14.0625 (5.3540)\n",
            "Test (on val set): [143/157]\tTime 0.027 (0.031)\tLoss 0.6523 (0.8639)\tTop 1-err 10.9375 (24.2947)\tTop 5-err 6.2500 (5.3602)\n",
            "Test (on val set): [144/157]\tTime 0.026 (0.031)\tLoss 0.8087 (0.8635)\tTop 1-err 18.7500 (24.2565)\tTop 5-err 6.2500 (5.3664)\n",
            "Test (on val set): [145/157]\tTime 0.030 (0.031)\tLoss 0.7191 (0.8625)\tTop 1-err 20.3125 (24.2295)\tTop 5-err 1.5625 (5.3403)\n",
            "Test (on val set): [146/157]\tTime 0.022 (0.031)\tLoss 0.8020 (0.8621)\tTop 1-err 26.5625 (24.2453)\tTop 5-err 7.8125 (5.3571)\n",
            "Test (on val set): [147/157]\tTime 0.033 (0.031)\tLoss 1.0838 (0.8636)\tTop 1-err 28.1250 (24.2715)\tTop 5-err 10.9375 (5.3948)\n",
            "Test (on val set): [148/157]\tTime 0.023 (0.031)\tLoss 0.6202 (0.8620)\tTop 1-err 15.6250 (24.2135)\tTop 5-err 4.6875 (5.3901)\n",
            "Test (on val set): [149/157]\tTime 0.020 (0.031)\tLoss 0.7213 (0.8610)\tTop 1-err 21.8750 (24.1979)\tTop 5-err 4.6875 (5.3854)\n",
            "Test (on val set): [150/157]\tTime 0.020 (0.031)\tLoss 0.8201 (0.8608)\tTop 1-err 23.4375 (24.1929)\tTop 5-err 7.8125 (5.4015)\n",
            "Test (on val set): [151/157]\tTime 0.020 (0.031)\tLoss 0.5427 (0.8587)\tTop 1-err 14.0625 (24.1262)\tTop 5-err 1.5625 (5.3762)\n",
            "Test (on val set): [152/157]\tTime 0.019 (0.030)\tLoss 0.9762 (0.8595)\tTop 1-err 26.5625 (24.1422)\tTop 5-err 9.3750 (5.4024)\n",
            "Test (on val set): [153/157]\tTime 0.020 (0.030)\tLoss 0.8825 (0.8596)\tTop 1-err 26.5625 (24.1579)\tTop 5-err 6.2500 (5.4079)\n",
            "Test (on val set): [154/157]\tTime 0.020 (0.030)\tLoss 0.6244 (0.8581)\tTop 1-err 21.8750 (24.1431)\tTop 5-err 4.6875 (5.4032)\n",
            "Test (on val set): [155/157]\tTime 0.020 (0.030)\tLoss 0.9227 (0.8585)\tTop 1-err 23.4375 (24.1386)\tTop 5-err 7.8125 (5.4187)\n",
            "Test (on val set): [156/157]\tTime 0.052 (0.030)\tLoss 0.9385 (0.8586)\tTop 1-err 18.7500 (24.1300)\tTop 5-err 12.5000 (5.4300)\n",
            "Accuracy (top-1 and 5 error): 24.13 5.43\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z4HbByBrAEff"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}